<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://oldcamel.run</id>
    <title>Old Camel</title>
    <updated>2021-04-15T01:20:28.970Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://oldcamel.run"/>
    <link rel="self" href="https://oldcamel.run/atom.xml"/>
    <subtitle>日常开发记录 
&lt;div&gt;&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=330 height=86 src=&quot;//music.163.com/outchain/player?type=2&amp;id=18611643&amp;auto=1&amp;height=66&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;</subtitle>
    <logo>https://oldcamel.run/images/avatar.png</logo>
    <icon>https://oldcamel.run/favicon.ico</icon>
    <rights>All rights reserved 2021, Old Camel</rights>
    <entry>
        <title type="html"><![CDATA[Flink Sql 实现CDC数据实时同步特殊处理]]></title>
        <id>https://oldcamel.run/post/flink-sql-shi-xian-cdc-shu-ju-shi-shi-tong-bu-te-shu-chu-li/</id>
        <link href="https://oldcamel.run/post/flink-sql-shi-xian-cdc-shu-ju-shi-shi-tong-bu-te-shu-chu-li/">
        </link>
        <updated>2021-04-15T01:06:21.000Z</updated>
        <content type="html"><![CDATA[<h1 id="1-decimal-类型在kafka中解析为乱码">1、decimal 类型在kafka中解析为乱码</h1>
<p>kafka connect 插件设置 &quot;decimal.handling.mode&quot;: &quot;string&quot; 属性。将decimal解析为字符串</p>
<p>sql语句中将字符串转换为数字 用CAST(字段名称) AS INT) 函数  INT可根据不同类型的数字替换为不同的目标类型（DECIMAL(10,4) BIGINT 等）</p>
<h1 id="2-日期处理">2、日期处理</h1>
<p>kafka中人日期类型会解析成时间戳，可用TO_TIMESTAMP + FROM_UNIXTIME做转换，注意处理时区问题</p>
<p>oracle日期类型处理精度+时区：</p>
<pre><code>CREATED_DATETS AS TO_TIMESTAMP(FROM_UNIXTIME(CREATED_DATE/1000/1000-8 * 60 * 60, 'yyyy-MM-dd HH:mm:ss')),
</code></pre>
<p>其他数据库日期类型只处理日期</p>
<pre><code>FDateTS AS TO_TIMESTAMP(FROM_UNIXTIME((FDate-8 * 60 * 60 * 1000) / 1000, 'yyyy-MM-dd HH:mm:ss'))
</code></pre>
<h1 id="3-cdc数据插入设置主键">3、cdc数据插入设置主键</h1>
<pre><code>PRIMARY KEY (AAA,BBB) NOT ENFORCED
</code></pre>
<h1 id="4-kafka-sink-sasl-connect插件设置认证">4、kafka sink sasl connect插件设置认证</h1>
<pre><code>&quot;database.history.producer.sasl.mechanism&quot;: &quot;PLAIN&quot;,
&quot;database.history.producer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;admin\&quot; password=\&quot;密码\&quot;;&quot;,
&quot;database.history.producer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;,
&quot;database.history.consumer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;admin\&quot; password=\&quot;密码\&quot;;&quot;,
&quot;database.history.consumer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;,
&quot;database.history.consumer.sasl.mechanism&quot;: &quot;PLAIN&quot;
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kafka实现oracle的CDC数据实时变更]]></title>
        <id>https://oldcamel.run/post/kafka-shi-xian-oracle-de-cdc-shu-ju-shi-shi-bian-geng/</id>
        <link href="https://oldcamel.run/post/kafka-shi-xian-oracle-de-cdc-shu-ju-shi-shi-bian-geng/">
        </link>
        <updated>2021-04-14T06:02:54.000Z</updated>
        <content type="html"><![CDATA[<h1 id="使用工具-debezium-oracle-connectororacle-logminer">使用工具  debezium-oracle-connector,oracle-LogMiner</h1>
<h2 id="1-oracle-设置">1、oracle 设置</h2>
<h3 id="开启归档模式">开启归档模式</h3>
<pre><code class="language-sqlplus">alter system set db_recovery_file_dest_size = 10G;
alter system set db_recovery_file_dest = '/home/oracle/oradta/recovery_area' scope=spfile;
shutdown immediate
startup mount
alter database archivelog;
alter database open;
archive log list
exit;

ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS;

</code></pre>
<h4 id="创建用户">创建用户</h4>
<pre><code>CREATE USER myuser IDENTIFIED BY dbz
   DEFAULT TABLESPACE 命名空间
   QUOTA UNLIMITED ON 命名空间
</code></pre>
<h4 id="授权">授权</h4>
<pre><code>GRANT CREATE SESSION TO myuser;
GRANT CREATE TABLE TO myuser;
GRANT CREATE SEQUENCE TO myuser;
GRANT CREATE TRIGGER TO myuser;
GRANT CREATE SESSION TO myuser;
GRANT SELECT ON V_$DATABASE to myuser;
GRANT FLASHBACK ANY TABLE TO myuser;
GRANT SELECT ANY TABLE TO myuser;
GRANT SELECT_CATALOG_ROLE TO myuser;
GRANT EXECUTE_CATALOG_ROLE TO myuser;
GRANT SELECT ANY TRANSACTION TO myuser;
GRANT CREATE TABLE TO myuser;
GRANT LOCK ANY TABLE TO myuser;
GRANT ALTER ANY TABLE TO myuser;
GRANT CREATE SEQUENCE TO myuser;
GRANT EXECUTE ON DBMS_LOGMNR TO myuser;
GRANT EXECUTE ON DBMS_LOGMNR_D TO myuser;
GRANT SELECT ON V_$LOG TO myuser;
GRANT SELECT ON V_$LOG_HISTORY TO myuser;
GRANT SELECT ON V_$LOGMNR_LOGS TO myuser;
GRANT SELECT ON V_$LOGMNR_CONTENTS TO myuser;
GRANT SELECT ON V_$LOGMNR_PARAMETERS TO myuser;
GRANT SELECT ON V_$LOGFILE TO myuser;
GRANT SELECT ON V_$ARCHIVED_LOG TO myuser;
GRANT SELECT ON V_$ARCHIVE_DEST_STATUS TO myuser;
GRANT SELECT ON SYSTEM.LOGMNR_COL$ TO myuser;
GRANT SELECT ON SYSTEM.LOGMNR_OBJ$ TO myuser;
GRANT SELECT ON SYSTEM.LOGMNR_USER$ TO myuser;
GRANT SELECT ON SYSTEM.LOGMNR_UID$ TO myuser;
</code></pre>
<h2 id="oracle客户端设置">oracle客户端设置</h2>
<h3 id="下载">下载</h3>
<pre><code>wget &quot;https://download.oracle.com/otn_software/linux/instantclient/19600/instantclient-basiclite-linux.x64-19.6.0.0.0dbru.zip&quot; -O /tmp/ic.zip；
unzip /tmp/ic.zip -d  自定义目录
</code></pre>
<h3 id="环境变量">环境变量</h3>
<p>设置LD_LIBRARY_PATH 指向 自定义目录</p>
<h2 id="kafaka-connect-插件设置">kafaka connect 插件设置</h2>
<h3 id="下载-2">下载</h3>
<pre><code>wget &quot;https://oss.sonatype.org/service/local/artifact/maven/redirect?r=snapshots&amp;g=io.debezium&amp;a=debezium-connector-oracle&amp;v=LATEST&amp;c=plugin&amp;e=tar.gz&quot; -O /tmp/dbz-ora.tgz；
tar -xvf /tmp/dbz-ora.tgz --directory  kafaka 插件目录
</code></pre>
<h3 id="添加-ojdbc-jar到插件目录">添加 ojdbc jar到插件目录</h3>
<pre><code>      curl https://maven.xwiki.org/externals/com/oracle/jdbc/ojdbc8/12.2.0.1/ojdbc8-12.2.0.1.jar -o ojdbc8-12.2.0.1.jar
</code></pre>
<h2 id="创建kafka-connect">创建kafka connect</h2>
<pre><code>curl --location --request POST 'http://localhost:8083/connectors' \
--header 'Content-Type: application/json' \
--data-raw '{
   &quot;name&quot;: &quot;name&quot;,
   &quot;config&quot;: {
       &quot;connector.class&quot; : &quot;io.debezium.connector.oracle.OracleConnector&quot;,
       &quot;tasks.max&quot; : &quot;1&quot;,
       &quot;database.server.name&quot; : &quot;topic&quot;,
       &quot;database.hostname&quot; : &quot;database_host&quot;,
       &quot;database.port&quot; : &quot;1521&quot;,
       &quot;database.user&quot; : &quot;myuser&quot;,
       &quot;database.password&quot; : &quot;dbz&quot;,
       &quot;database.dbname&quot; : &quot;orcl&quot;,
       &quot;database.tablename.case.insensitive&quot;: &quot;true&quot;,
       &quot;database.history.kafka.bootstrap.servers&quot; : &quot;localhost:9092&quot;,
       &quot;database.history.kafka.topic&quot;: &quot;name-h&quot;,
       &quot;table.include.list&quot;:&quot;AAA.BBB,CCC.DDD&quot;,
        &quot;database.history.producer.sasl.mechanism&quot;: &quot;PLAIN&quot;,
       &quot;database.history.producer.sasl.jaas.config&quot;:         &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;admin\&quot; password=\&quot;password\&quot;;&quot;,
   &quot;database.history.producer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;,
    &quot;database.history.consumer.sasl.jaas.config&quot;:          &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;admin\&quot;   password=\&quot;password\&quot;;&quot;,
     &quot;database.history.consumer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;,
       &quot;database.history.consumer.sasl.mechanism&quot;: &quot;PLAIN&quot;
   }
}'
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[linux 安装服务器资源监控 prometheus远程写入]]></title>
        <id>https://oldcamel.run/post/linux-an-zhuang-fu-wu-qi-zi-yuan-jian-kong-prometheus-yuan-cheng-xie-ru/</id>
        <link href="https://oldcamel.run/post/linux-an-zhuang-fu-wu-qi-zi-yuan-jian-kong-prometheus-yuan-cheng-xie-ru/">
        </link>
        <updated>2021-03-11T01:07:36.000Z</updated>
        <content type="html"><![CDATA[<h2 id="1安装prometheus">1.安装prometheus</h2>
<pre><code>wget -c https://github.com/prometheus/prometheus/releases/download/v2.23.0/prometheus-2.23.0.linux-amd64.tar.gz 
tar zxvf prometheus-2.23.0.linux-amd64.tar.gz  -C /opt/ 
cd /opt/ 
ln -s prometheus-2.23.0.linux-amd64 prometheus 
cat &gt; /etc/systemd/system/prometheus.service &lt;&lt;EOF 
[Unit] 
Description=prometheus 
After=network.target 
[Service] 
Type=simple 
WorkingDirectory=/opt/prometheus 
ExecStart=/opt/prometheus/prometheus --config.file=/opt/prometheus/prometheus.yml
LimitNOFILE=65536 
PrivateTmp=true 
RestartSec=2 
StartLimitInterval=0 
Restart=always 
[Install] 
WantedBy=multi-user.target 
EOF 
systemctl daemon-reload  
systemctl enable prometheus 
systemctl start prometheus 

</code></pre>
<h3 id="2配置prometheus">2.配置Prometheus</h3>
<pre><code>cat &gt; /opt/prometheus/prometheus.yml &lt;&lt;EOF 
# my global config 
global: 
  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. 
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. 
  # scrape_timeout is set to the global default (10s). 
  external_labels:
        tenant: &quot;140test&quot;
remote_write:
  - url: &quot;url&quot;
    basic_auth:
      username: 用户名
      password: 密码
# Alertmanager configuration 
alerting: 
  alertmanagers: 
  - static_configs: 
    - targets: 
      # - alertmanager:9093 
# Load rules once and periodically evaluate them according to the global 'evaluation_interval'. 
rule_files: 
  # - &quot;first_rules.yml&quot; 
  # - &quot;second_rules.yml&quot; 
# A scrape configuration containing exactly one endpoint to scrape: 
# Here it's Prometheus itself. 
scrape_configs: 
  # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. 
  - job_name: 'servers' 
    file_sd_configs: 
    - refresh_interval: 61s 
      files: 
        - /opt/prometheus/servers/*.json 
EOF 

</code></pre>
<h2 id="3创建node配置文件">3.创建node配置文件</h2>
<pre><code>mkdir -p /opt/prometheus/servers
cd /opt/prometheus/servers
vim json.json
[     
    { 
        &quot;targets&quot;: [ 
            “xxx.xxx.xxx.xxx:9100&quot; 
        ], 
        &quot;labels&quot;: { 
            &quot;instance&quot;: &quot;xxx.xxx.xxx.xxx&quot;, 
            &quot;job&quot;: &quot;node_exporter&quot; 
        } 
    }
   
] 
</code></pre>
<h2 id="安装node_exporter">安装node_exporter</h2>
<pre><code>Wget https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-amd64.tar.gz 
tar zxvf node_exporter-1.0.1.linux-amd64.tar.gz -C /opt/ 
cd /opt/ 
ln -s  node_exporter-1.0.1.linux-amd64 node_exporter 
cat &gt; /etc/systemd/system/node_exporter.service &lt;&lt;EOF 
[Unit] 
Description=node_exporter 
After=network.target 
[Service] 
Type=simple 
WorkingDirectory=/opt/node_exporter 
ExecStart=/opt/node_exporter/node_exporter 
LimitNOFILE=65536 
PrivateTmp=true 
RestartSec=2 
StartLimitInterval=0 
Restart=always 
[Install] 
WantedBy=multi-user.target 
EOF 
systemctl daemon-reload 
systemctl enable node_exporter 
systemctl start node_exporter 
systemctl restart prometheus 

</code></pre>
<h2 id="查看服务日志命令">查看服务日志命令</h2>
<pre><code>journalctl -u prometheus
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kafka实现sqlserver的CDC数据实时变更]]></title>
        <id>https://oldcamel.run/post/kafka-shi-xian-sqlserver-de-cdc-shu-ju-shi-shi-bian-geng/</id>
        <link href="https://oldcamel.run/post/kafka-shi-xian-sqlserver-de-cdc-shu-ju-shi-shi-bian-geng/">
        </link>
        <updated>2021-01-08T02:29:39.000Z</updated>
        <content type="html"><![CDATA[<h2 id="安装sqlserver">安装sqlserver</h2>
<pre><code>#!/bin/bash -e
# Password for the SA user (required)
# 设置密码
MSSQL_SA_PASSWORD='test@12345678'

# Product ID of the version of SQL server you're installing
# Must be evaluation, developer, express, web, standard, enterprise, or your 25 digit product key
# Defaults to developer
# 选择版本，有多种版本可供原则，具体版本标识符可自行百度
MSSQL_PID='evaluation'

# Install SQL Server Agent (recommended)
SQL_ENABLE_AGENT='y'

# Install SQL Server Full Text Search (optional)
# SQL_INSTALL_FULLTEXT='y'

# Create an additional user with sysadmin privileges (optional)
# 新建一个额外添加的用户，（可选）
# SQL_INSTALL_USER='&lt;Username&gt;'
# SQL_INSTALL_USER_PASSWORD='&lt;YourStrong!Passw0rd&gt;'

if [ -z $MSSQL_SA_PASSWORD ]
then
  echo Environment variable MSSQL_SA_PASSWORD must be set for unattended install
  exit 1
fi

# --------------------------- 远程拉包 并安装的过程
echo Adding Microsoft repositories...
sudo curl -o /etc/yum.repos.d/mssql-server.repo https://packages.microsoft.com/config/rhel/7/mssql-server-2017.repo
sudo curl -o /etc/yum.repos.d/msprod.repo https://packages.microsoft.com/config/rhel/7/prod.repo

echo Installing SQL Server...
sudo yum install -y mssql-server

# --------------------------- 下面给出 本地离线rpm包安装部分脚本
echo ：Local Installing SQL Server...
sudo yum localinstall -y ./sqlserver2017.rpm



# 执行sqlserver的配置
echo Running mssql-conf setup...
sudo MSSQL_SA_PASSWORD=$MSSQL_SA_PASSWORD \
     MSSQL_PID=$MSSQL_PID \
     /opt/mssql/bin/mssql-conf -n setup accept-eula

echo Installing mssql-tools and unixODBC developer...
sudo ACCEPT_EULA=Y yum install -y mssql-tools unixODBC-devel

# Add SQL Server tools to the path by default:
echo Adding SQL Server tools to your path...
echo PATH=&quot;$PATH:/opt/mssql-tools/bin&quot; &gt;&gt; ~/.bash_profile
echo 'export PATH=&quot;$PATH:/opt/mssql-tools/bin&quot;' &gt;&gt; ~/.bashrc
source ~/.bashrc

# Optional Enable SQL Server Agent :
if [ ! -z $SQL_ENABLE_AGENT ]
then
  echo Enable SQL Server Agent...
  sudo /opt/mssql/bin/mssql-conf set sqlagent.enabled true
  sudo systemctl restart mssql-server
fi

# Optional SQL Server Full Text Search installation:
if [ ! -z $SQL_INSTALL_FULLTEXT ]
then
    echo Installing SQL Server Full-Text Search...
    sudo yum install -y mssql-server-fts
fi

# Configure firewall to allow TCP port 1433:
# 配置防火墙放行1433端口，懒省事直接关掉防火墙亦可
echo Configuring firewall to allow traffic on port 1433...
sudo firewall-cmd --zone=public --add-port=1433/tcp --permanent
sudo firewall-cmd --reload

# Example of setting post-installation configuration options
# Set trace flags 1204 and 1222 for deadlock tracing:
#echo Setting trace flags...
#sudo /opt/mssql/bin/mssql-conf traceflag 1204 1222 on

# Restart SQL Server after making configuration changes:
# 重启
echo Restarting SQL Server...
sudo systemctl restart mssql-server

# Connect to server and get the version:
# 官方给出的测试链接脚本
counter=1
errstatus=1
while [ $counter -le 5 ] &amp;&amp; [ $errstatus = 1 ]
do
  echo Waiting for SQL Server to start...
  sleep 5s
  /opt/mssql-tools/bin/sqlcmd \
    -S localhost \
    -U SA \
    -P $MSSQL_SA_PASSWORD \
    -Q &quot;SELECT @@VERSION&quot; 2&gt;/dev/null
  errstatus=$?
  ((counter++))
done

# Display error if connection failed:
if [ $errstatus = 1 ]
then
  echo Cannot connect to SQL Server, installation aborted
  exit $errstatus
fi

# Optional new user creation:
if [ ! -z $SQL_INSTALL_USER ] &amp;&amp; [ ! -z $SQL_INSTALL_USER_PASSWORD ]
then
  echo Creating user $SQL_INSTALL_USER
  /opt/mssql-tools/bin/sqlcmd \
    -S localhost \
    -U SA \
    -P $MSSQL_SA_PASSWORD \
    -Q &quot;CREATE LOGIN [$SQL_INSTALL_USER] WITH PASSWORD=N'$SQL_INSTALL_USER_PASSWORD', DEFAULT_DATABASE=[master], CHECK_EXPIRATION=ON, CHECK_POLICY=ON; ALTER SERVER ROLE [sysadmin] ADD MEMBER [$SQL_INSTALL_USER]&quot;
fi

echo Done!
</code></pre>
<p>运行脚本等着安装完成</p>
<h2 id="开启表的cdc">开启表的cdc</h2>
<h3 id="创建shihu数据库">创建shihu数据库</h3>
<h3 id="建测试表">建测试表</h3>
<pre><code>IF EXISTS (SELECT * FROM sys.all_objects WHERE object_id = OBJECT_ID(N'[dbo].[dt_test]') AND type IN ('U'))
	DROP TABLE [dbo].[dt_test]
GO

CREATE TABLE [dbo].[dt_test] (
  [tid] int  NOT NULL,
  [tname] nvarchar(200) COLLATE SQL_Latin1_General_CP1_CI_AS  NULL,
  [tidcard] nvarchar(200) COLLATE SQL_Latin1_General_CP1_CI_AS  NULL,
  [tbirthday] date  NULL,
  [tmobile] nvarchar(200) COLLATE SQL_Latin1_General_CP1_CI_AS  NULL,
  [temail] nvarchar(200) COLLATE SQL_Latin1_General_CP1_CI_AS  NULL,
  [tgender] bigint  NULL,
  [tcreate_time] datetime  NULL
)
GO

ALTER TABLE [dbo].[dt_test] SET (LOCK_ESCALATION = TABLE)
GO


-- ----------------------------
-- Primary Key structure for table dt_test
-- ----------------------------
ALTER TABLE [dbo].[dt_test] ADD CONSTRAINT [PK__dt_test__DC105B0FA908205A] PRIMARY KEY CLUSTERED ([tid])
WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON)  
ON [PRIMARY]
GO

</code></pre>
<h3 id="开启cdc">开启cdc</h3>
<pre><code>USE shihu
GO
EXEC sys.sp_cdc_enable_db
GO

USE shihu
GO
EXEC sys.sp_cdc_enable_table
@source_schema = 'dbo',
@source_name   = ‘dt_test’,     
@role_name     = NULL,
@filegroup_name = 'PRIMARY',      
@supports_net_changes = 1
GO
</code></pre>
<h2 id="下载kafka插件">下载kafka插件</h2>
<h3 id="官方文档">官方文档</h3>
<p><a href="https://debezium.io/documentation/reference/1.3/connectors/sqlserver.html">https://debezium.io/documentation/reference/1.3/connectors/sqlserver.html</a></p>
<h3 id="下载">下载</h3>
<p><a href="https://repo1.maven.org/maven2/io/debezium/debezium-connector-sqlserver/1.2.5.Final/debezium-connector-sqlserver-1.2.5.Final-plugin.tar.gz">https://repo1.maven.org/maven2/io/debezium/debezium-connector-sqlserver/1.2.5.Final/debezium-connector-sqlserver-1.2.5.Final-plugin.tar.gz</a></p>
<h3 id="创建插件目录">创建插件目录</h3>
<p>在kafka跟目录创建 kafka_connect_plugins 目录<br>
将下载的包解压到此目录中<br>
<img src="https://oldcamel.run/post-images/1610074594464.png" alt="" loading="lazy"></p>
<h3 id="修改kafkaconfigconnect-distributeproperties">修改kafka/config/connect-distribute.properties</h3>
<p>bootstrap.servers=IP:9092<br>
plugin.path= /kafka路径/kafka_connect_plugins</p>
<h3 id="启动kafka-connect">启动kafka-connect</h3>
<pre><code>./bin/connect-distributed.sh  config/connect-distributed.properties &gt;logs/ksc.log &amp;
</code></pre>
<p>开启成功的标志,postman可以使用访问端口8083（默认的端口号,可以在connect-distributed.properties 更改）</p>
<h4 id="rest-api">rest api</h4>
<pre><code>GET /connectors – 返回所有正在运行的connector名
POST /connectors – 新建一个connector; 请求体必须是json格式并且需要包含name字段和config字段，name是connector的名字，config是json格式，必须包含你的connector的配置信息。
GET /connectors/{name} – 获取指定connetor的信息
GET /connectors/{name}/config – 获取指定connector的配置信息
PUT /connectors/{name}/config – 更新指定connector的配置信息
GET /connectors/{name}/status – 获取指定connector的状态，包括它是否在运行、停止、或者失败，如果发生错误，还会列出错误的具体信息。
GET /connectors/{name}/tasks – 获取指定connector正在运行的task。
GET /connectors/{name}/tasks/{taskid}/status – 获取指定connector的task的状态信息
PUT /connectors/{name}/pause – 暂停connector和它的task，停止数据处理知道它被恢复。
PUT /connectors/{name}/resume – 恢复一个被暂停的connector
POST /connectors/{name}/restart – 重启一个connector，尤其是在一个connector运行失败的情况下比较常用
POST /connectors/{name}/tasks/{taskId}/restart – 重启一个task，一般是因为它运行失败才这样做。
DELETE /connectors/{name} – 删除一个connector，停止它的所有task并删除配置。
</code></pre>
<h4 id="添加sqlserver-connect">添加sqlserver connect</h4>
<pre><code>curl --location --request POST 'http://localhost:8083/connectors' \
--header 'Content-Type: application/json' \
--data-raw '{
    &quot;name&quot;: &quot;sqlserver178-connector&quot;,
    &quot;config&quot;: {
        &quot;connector.class&quot;: &quot;io.debezium.connector.sqlserver.SqlServerConnector&quot;,
        &quot;database.hostname&quot;: &quot;sqlserver的ip&quot;,
        &quot;database.port&quot;: &quot;1433&quot;,
        &quot;database.user&quot;: &quot;sa&quot;,
        &quot;database.password&quot;: &quot;test@12345678&quot;,
        &quot;database.dbname&quot;: &quot;shihu&quot;,
        &quot;database.server.name&quot;: &quot;fullfillment&quot;,
        &quot;table.whitelist&quot;: &quot;dbo.dt_test&quot;,
        &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka的IP:9092&quot;,
        &quot;database.history.kafka.topic&quot;: &quot;dbhistory.fullfillment&quot;,
        &quot;database.history.producer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;,
        &quot;database.history.producer.sasl.mechanism&quot;: &quot;PLAIN&quot;,
        &quot;database.history.producer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;admin\&quot; password=\&quot;123\&quot;;&quot;,
        &quot;database.history.consumer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;,
        &quot;database.history.consumer.sasl.mechanism&quot;: &quot;PLAIN&quot;,
        &quot;database.history.consumer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;admin\&quot; password=\&quot;123\&quot;;&quot;
    }
}'
</code></pre>
<h3 id="消费信息">消费信息</h3>
<h4 id="consumerpreperties配置">consumer.preperties配置</h4>
<pre><code>bootstrap.servers=localhost:9092
group.id=test-consumer-group
security.protocol=SASL_PLAINTEXT
sasl.mechanism=PLAIN
</code></pre>
<h4 id="修改器kafka-console-consumersh为kafka-console-consumer-saalsh倒数第二行添加">修改器kafka-console-consumer.sh为kafka-console-consumer-saal.sh倒数第二行添加</h4>
<pre><code>export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/kafka_2.13-2.6.0/config/kafka_client_jaas.conf&quot;
</code></pre>
<h4 id="启动">启动</h4>
<pre><code>./bin/kafka-console-consumer-saal.sh --bootstrap-server localhost:9092 --topic fullfillment.dbo.dt_test --consumer.config config/consumer.properties
</code></pre>
<h4 id="消息示例">消息示例</h4>
<pre><code>{
    &quot;schema&quot;:{
        &quot;type&quot;:&quot;struct&quot;,
        &quot;fields&quot;:Array[6],
        &quot;optional&quot;:false,
        &quot;name&quot;:&quot;fullfillment.dbo.dt_test.Envelope&quot;
    },
    &quot;payload&quot;:{
        &quot;before&quot;:{
            &quot;tid&quot;:1,
            &quot;tname&quot;:&quot;222&quot;,
            &quot;tidcard&quot;:&quot;2&quot;,
            &quot;tbirthday&quot;:-25567,
            &quot;tmobile&quot;:&quot;1&quot;,
            &quot;temail&quot;:&quot;1&quot;,
            &quot;tgender&quot;:2,
            &quot;tcreate_time&quot;:-2208988800000
        },
        &quot;after&quot;:{
            &quot;tid&quot;:1,
            &quot;tname&quot;:&quot;test&quot;,
            &quot;tidcard&quot;:&quot;2&quot;,
            &quot;tbirthday&quot;:-25567,
            &quot;tmobile&quot;:&quot;1&quot;,
            &quot;temail&quot;:&quot;1&quot;,
            &quot;tgender&quot;:2,
            &quot;tcreate_time&quot;:-2208988800000
        },
        &quot;source&quot;:Object{...},
        &quot;op&quot;:&quot;u&quot;,
        &quot;ts_ms&quot;:1610019731750,
        &quot;transaction&quot;:null
    }
}
</code></pre>
<h4 id="oracle示例">oracle示例</h4>
<pre><code>
curl --location --request POST 'http://localhost:8083/connectors/' \
--header 'Content-Type: application/json' \
--data-raw '
{
    &quot;name&quot;: &quot;oracle-61-connector&quot;,
    &quot;config&quot;: {
        &quot;connector.class&quot;: &quot;com.ecer.kafka.connect.oracle.OracleSourceConnector&quot;,
        &quot;db.name.alias&quot;: &quot;61test&quot;,
        &quot;tasks.max&quot;: 1,
        &quot;topic&quot;: &quot;kol&quot;,
        &quot;db.name&quot;: &quot;portaldb&quot;,
        &quot;db.hostname&quot;: &quot;oralce连接host地址&quot;,
        &quot;db.port&quot;: 1521,
        &quot;db.user&quot;: &quot;info&quot;,
        &quot;db.user.password&quot;: &quot;111111&quot;,
        &quot;db.fetch.size&quot;: 1,
        &quot;table.whitelist&quot;: &quot;INFO.*,SPAUTH.*,WORKFLOW.*,FLOWABLE.*,OA.*&quot;,
        &quot;table.blacklist&quot;: &quot;&quot;,
        &quot;parse.dml.data&quot;: true,
        &quot;reset.offset&quot;: false,
        &quot;multitenant&quot;: false
    }
}
</code></pre>
<h3 id="mysql示例">mysql示例</h3>
<pre><code>curl --location --request POST 'http://localhost:8083/connectors/' \
--header 'Content-Type: application/json' \
--data-raw '
{
  &quot;name&quot;: &quot;140mysql-connector&quot;, 
  &quot;config&quot;: {
    &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;, 
    &quot;database.hostname&quot;: &quot;localhost&quot;, 
    &quot;database.port&quot;: &quot;3306&quot;, 
    &quot;database.user&quot;: &quot;root&quot;, 
    &quot;database.password&quot;: &quot;111111&quot;, 
    &quot;database.server.id&quot;: &quot;184054&quot;, 
    &quot;database.server.name&quot;: &quot;fullfillment140ms&quot;, 
    &quot;database.whitelist&quot;:&quot;test&quot;,
    &quot;database.history.kafka.bootstrap.servers&quot;: &quot;localhost:9092&quot;, 
    &quot;database.history.kafka.topic&quot;: &quot;dbhistory.fullfillment140ms&quot;, 
    &quot;include.schema.changes&quot;: &quot;false&quot;,
        &quot;database.history.producer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;,
        &quot;database.history.producer.sasl.mechanism&quot;: &quot;PLAIN&quot;,
        &quot;database.history.producer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;admin\&quot; password=\&quot;123\&quot;;&quot;,
        &quot;database.history.consumer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;,
        &quot;database.history.consumer.sasl.mechanism&quot;: &quot;PLAIN&quot;,
        &quot;database.history.consumer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;admin\&quot; password=\&quot;123\&quot;;&quot;
    }
}
</code></pre>
<h3 id="查看topic-list">查看topic list</h3>
<pre><code>bin/kafka-topics.sh --zookeeper localhost --list
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kafka开启SASL用户名密码认证]]></title>
        <id>https://oldcamel.run/post/kafka-kai-qi-sasl-yong-hu-ming-mi-ma-ren-zheng/</id>
        <link href="https://oldcamel.run/post/kafka-kai-qi-sasl-yong-hu-ming-mi-ma-ren-zheng/">
        </link>
        <updated>2021-01-08T01:51:25.000Z</updated>
        <content type="html"><![CDATA[<h2 id="创建kafka_server_jaasconf文件">创建kafka_server_jaas.conf文件</h2>
<p>config目录下创建kafka_server_jaas.conf文件</p>
<pre><code>KafkaServer {
  org.apache.kafka.common.security.plain.PlainLoginModule required
  username=&quot;admin&quot;
  password=&quot;123&quot;
  user_admin=&quot;123&quot;
  user_yunzai=&quot;123&quot;;
};
</code></pre>
<h2 id="创建kafka_client_jaasconf文件">创建kafka_client_jaas.conf文件</h2>
<p>config目录下创建kafka_client_jaas.conf文件</p>
<pre><code>KafkaClient {
        org.apache.kafka.common.security.plain.PlainLoginModule required
        username=&quot;admin&quot;
        password=&quot;123&quot;;
};
</code></pre>
<h2 id="修改serverproperties">修改server.properties</h2>
<pre><code>listeners=SASL_PLAINTEXT://IP:9092
advertised.listeners=SASL_PLAINTEXT://IP:9092
security.inter.broker.protocol=SASL_PLAINTEXT
sasl.enabled.mechanisms=PLAIN
sasl.mechanism.inter.broker.protocol=PLAIN
authorizer.class.name = kafka.security.auth.SimpleAclAuthorizer
super.users=User:admin;
</code></pre>
<h2 id="修改kafaka启动脚本">修改kafaka启动脚本</h2>
<p>修改bin目录下kafka_start.sh<br>
在倒数第二行添加kafka_server_jaas.conf的全路径</p>
<pre><code>export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/kafka_2.13-2.6.0/config/kafka_server_jaas.conf&quot;
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://oldcamel.run/post-images/1610071605941.png" alt="" loading="lazy"></figure>
<h2 id="可选为特定用户添加特定topic的acl授权">（可选）为特定用户添加特定topic的acl授权</h2>
<p>如下表示为用户yunzai添加topic nginx-log的读写权限。</p>
<pre><code>./bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 -add --allow-principal User:yunzai  --operation Read --operation Write --topic testyunzai
</code></pre>
<p>验证授权</p>
<pre><code>./bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --list --topic testyunzai
</code></pre>
<h3 id="生产者消费者验证">生产者消费者验证</h3>
<h4 id="生产者">生产者</h4>
<p>1.以用户yunzai为例，创建JAAS认证文件yunzai_jaas.conf放在config目录下，内容如下:</p>
<pre><code>KafkaClient {
org.apache.kafka.common.security.plain.PlainLoginModule required
username=&quot;yunzai&quot;
password=&quot;123&quot;;
};
</code></pre>
<p>2.拷贝bin/kafka-console-producer.sh为bin/yunzai-kafka-console-producer.sh，并将JAAS文件作为一个JVM参数传给console producer</p>
<p>倒数第二行添加</p>
<pre><code>export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/kafka_2.13-2.6.0/config/yunzai_jaas.conf&quot;
</code></pre>
<p>3.创建文件producer.config指定如下属性：</p>
<pre><code>security.protocol=SASL_PLAINTEXT
sasl.mechanism=PLAIN
</code></pre>
<p>4.启动producer</p>
<pre><code>./bin/yunzai-kafka-console-producer.sh --broker-list IP:9092 --topic testyunzai --producer.config producer.config
</code></pre>
<h4 id="消费者">消费者</h4>
<p>1.以用户yunzai为例，创建JAAS认证文件yunzai_jaas.conf放在config目录下，如果用户跟生产者是同一个，可以复用上面生产者的JAAS文件，内容如下:</p>
<pre><code>KafkaClient {
org.apache.kafka.common.security.plain.PlainLoginModule required
username=&quot;yunzai&quot;
password=&quot;123&quot;;
};
</code></pre>
<p>2.拷贝bin/kafka-console-consumer.sh为bin/yunzai-kafka-console-consumer.sh，并将JAAS文件作为一个JVM参数传给console consumer<br>
倒数第二行添加</p>
<pre><code>export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/kafka_2.13-2.6.0/config/yunzai_jaas.conf&quot;
</code></pre>
<p>3.创建文件consumer.config指定如下属性：</p>
<pre><code>security.protocol=SASL_PLAINTEXT
sasl.mechanism=PLAIN
group.id=test
</code></pre>
<p>4.启动consumer</p>
<pre><code>./bin/yunzai-kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic testyunzai --from-beginning --consumer.config consumer.config
</code></pre>
<h3 id="java-beam客户端认证">java beam客户端认证</h3>
<p>beam设置</p>
<pre><code class="language-java">   final Map&lt;String, Object&gt; immutableMap = new ImmutableMap.Builder&lt;String, Object&gt;()
                .put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;)
                .put(ConsumerConfig.GROUP_ID_CONFIG, options.getGroupid())
                .put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true)
                .put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, &quot;SASL_PLAINTEXT&quot;)
                .put(SaslConfigs.SASL_MECHANISM, &quot;PLAIN&quot;)
                .put(SaslConfigs.SASL_JAAS_CONFIG, &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;&quot; + options.getKafkaUsername() + &quot;\&quot; password=\&quot;&quot; + options.getKafkaPassword() + &quot;\&quot;;&quot;)
                .build();
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[VictoriaMetrics 汇总多个Prometheus指标（四）读取Istio指标]]></title>
        <id>https://oldcamel.run/post/victoriametrics-hui-zong-duo-ge-prometheus-zhi-biao-san-du-qu-istio-zhi-biao/</id>
        <link href="https://oldcamel.run/post/victoriametrics-hui-zong-duo-ge-prometheus-zhi-biao-san-du-qu-istio-zhi-biao/">
        </link>
        <updated>2020-12-25T07:41:53.000Z</updated>
        <content type="html"><![CDATA[<p>istio中默认带了prometheus,里面包含istio代理的指标，现需要将istio中的指标配置在kube-prometheus中。</p>
<h2 id="修改prometheus-k8s-角色权限">修改prometheus-k8s 角色权限</h2>
<p>修改prometheus-clusterRole.yaml</p>
<pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus-k8s
rules:
- apiGroups:
  - &quot;&quot;
  resources:
  - nodes
  - services
  - endpoints
  - pods
  - nodes/proxy
  verbs:
  - get
  - watch
  - list
- apiGroups:
  - &quot;&quot;
  resources:
  - configmaps
  verbs:
  - get
- nonResourceURLs:
  - /metrics
  verbs:
  - get
</code></pre>
<h2 id="配置数据平面的服务监控">配置数据平面的服务监控</h2>
<pre><code>apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: prometheus-oper-istio-controlplane
spec:
  jobLabel: istio
  selector:
    matchExpressions:
      - {key: istio, operator: In, values: [mixer,pilot,galley,citadel,sidecar-injector]}
  namespaceSelector:
    any: true
  endpoints:
  - port: http-monitoring
    interval: 15s
  - port: http-policy-monitoring
    interval: 15s

</code></pre>
<h2 id="配置数据层的服务监控">配置数据层的服务监控</h2>
<pre><code>apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: prometheus-oper-istio-dataplane
  labels:
    monitoring: istio-dataplane
spec:
  selector:
    matchExpressions:
      - {key: istio-prometheus-ignore, operator: DoesNotExist}
  namespaceSelector:
    any: true
  jobLabel: envoy-stats
  endpoints:
  - path: /stats/prometheus
    targetPort: http-envoy-prom
    interval: 15s


</code></pre>
<h2 id="查看是否生效">查看是否生效</h2>
<figure data-type="image" tabindex="1"><img src="https://oldcamel.run/post-images/1608882536864.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="2"><img src="https://oldcamel.run/post-images/1608882571184.png" alt="" loading="lazy"></figure>
<h2 id="victoriametrics-中不同集群中的指标获取">VictoriaMetrics 中不同集群中的指标获取</h2>
<figure data-type="image" tabindex="3"><img src="https://oldcamel.run/post-images/1608882968056.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[VictoriaMetrics 汇总多个Prometheus指标（三）kube-prometheus安装]]></title>
        <id>https://oldcamel.run/post/victoriametrics-hui-zong-duo-ge-prometheus-zhi-biao-san-kube-prometheus-an-zhuang/</id>
        <link href="https://oldcamel.run/post/victoriametrics-hui-zong-duo-ge-prometheus-zhi-biao-san-kube-prometheus-an-zhuang/">
        </link>
        <updated>2020-12-25T07:32:20.000Z</updated>
        <content type="html"><![CDATA[<p>kube-prometheus是Prometheus Operator 针对kubernetes的监控组件。<br>
github地址<br>
<a href="https://github.com/prometheus-operator/kube-prometheus">https://github.com/prometheus-operator/kube-prometheus</a></p>
<h2 id="配置远程写入">配置远程写入</h2>
<h3 id="basic用户名密码配置-secret">basic用户名密码配置 secret</h3>
<pre><code>apiVersion: v1
kind: Secret
metadata:
  name: vmuser
  namespace: monitoring
type: kubernetes.io/basic-auth
stringData:
  username: username
  password: password
</code></pre>
<h3 id="添加远程写入-和外部标签">添加远程写入 和外部标签</h3>
<p>修改prometheus-prometheus.yaml文件，添加</p>
<pre><code>prometheusExternalLabelName: &quot;&quot;
  replicaExternalLabelName: &quot;&quot;
  externalLabels:
    tenant: &quot;租户名称&quot;
  remoteWrite:
    - url: http://VictoriaMetrics连接地址/api/v1/write
      basicAuth:
       username:
          key: username
          name: vmuser
       password:
          key: password
          name: vmuser
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[VictoriaMetrics 汇总多个Prometheus指标（二）Grafana安装]]></title>
        <id>https://oldcamel.run/post/grafana-an-zhuang/</id>
        <link href="https://oldcamel.run/post/grafana-an-zhuang/">
        </link>
        <updated>2020-12-25T06:50:09.000Z</updated>
        <content type="html"><![CDATA[<p>安装grafana 从victoriametrics中读取汇总的指标</p>
<h2 id="安装mysql">安装mysql</h2>
<h3 id="下载离线包">下载离线包</h3>
<p><a href="https://dev.mysql.com/downloads/mysql/5.7.html#downloads">下载地址</a><br>
<img src="https://oldcamel.run/post-images/1608879483467.png" alt="" loading="lazy"></p>
<h3 id="解压安装">解压安装</h3>
<figure data-type="image" tabindex="1"><img src="https://oldcamel.run/post-images/1608879544128.png" alt="" loading="lazy"></figure>
<pre><code>rpm -Uvh *.rpm --nodeps --force
</code></pre>
<h3 id="修改配置文件">修改配置文件</h3>
<p>修改 /etc/my.cnf 添加</p>
<pre><code>lower_case_table_names=1
default-storage-engine=INNODB
</code></pre>
<h3 id="生成用户不带密码">生成用户不带密码</h3>
<pre><code>mysqld --initialize-insecure
</code></pre>
<p>如果要重新初始化，必须先清空data文件夹</p>
<h3 id="启动mysql">启动mysql</h3>
<pre><code>systemctl start mysqld
</code></pre>
<h3 id="进入mysql">进入mysql</h3>
<pre><code>mysql -u root
</code></pre>
<h3 id="修改密码开启远程访问">修改密码+开启远程访问</h3>
<pre><code>set password for root@localhost = password('123456');
GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456' WITH GRANT OPTION;
FLUSH PRIVILEGES;
</code></pre>
<h3 id="创建grafana数据库">创建grafana数据库</h3>
<pre><code>CREATE DATABASE  `grafana` DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
</code></pre>
<h2 id="安装grafana">安装grafana</h2>
<h3 id="下载安装">下载安装</h3>
<p><a href="https://grafana.com/grafana/download">参考文档</a></p>
<pre><code>wget https://dl.grafana.com/oss/release/grafana-7.3.6-1.x86_64.rpm
sudo yum install grafana-7.3.6-1.x86_64.rpm
</code></pre>
<h3 id="修改配置文件-2">修改配置文件</h3>
<pre><code class="language-shell">vim /etc/grafana/grafana.ini
</code></pre>
<h4 id="domain修改">domain修改</h4>
<pre><code>domain = 修改成主机IP
</code></pre>
<h4 id="默认用户名密码修改">默认用户名密码修改</h4>
<pre><code>admin_user = admin
admin_password = 123456
</code></pre>
<h4 id="数据库修改">数据库修改</h4>
<pre><code>type = mysql
host = 127.0.0.1:3306
name = grafana
user = root
password =123456

</code></pre>
<h3 id="启动grafana服务">启动grafana服务</h3>
<pre><code>service grafana-server start
</code></pre>
<h3 id="配置-victoriametrics连接">配置 victoriametrics连接</h3>
<p>访问 ip:3000<br>
添加prometheus数据源 设置为 victoriametrics主机ip  端口 8428<br>
<img src="https://oldcamel.run/post-images/1608881141442.png" alt="" loading="lazy"></p>
<p>配置victoriametrics官方的监控图表<br>
<a href="https://grafana.com/grafana/dashboards/10229">grafana lab地址</a><br>
<img src="https://oldcamel.run/post-images/1608881288591.png" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[VictoriaMetrics 汇总多个Prometheus指标（一）安装VictoriaMetrics]]></title>
        <id>https://oldcamel.run/post/victoriametrics-an-zhuang/</id>
        <link href="https://oldcamel.run/post/victoriametrics-an-zhuang/">
        </link>
        <updated>2020-12-25T06:33:44.000Z</updated>
        <content type="html"><![CDATA[<h2 id="victoriametrics-用来收集k8s集群中的prometheus指标信息">victoriametrics 用来收集k8s集群中的prometheus指标信息 。</h2>
<p><a href="https://github.com/VictoriaMetrics/VictoriaMetrics">github地址</a><br>
<a href="https://victoriametrics.github.io/#retention">官方文档地址</a><br>
<a href="https://github.com/VictoriaMetrics/VictoriaMetrics/releases">下载二级制安装包</a></p>
<figure data-type="image" tabindex="1"><img src="https://oldcamel.run/post-images/1608878459634.png" alt="" loading="lazy"></figure>
<h2 id="启动主进程">启动主进程</h2>
<p>运行端口8428</p>
<pre><code class="language-shell">./victoria-metrics-prod -retentionPeriod=3 -selfScrapeInterval=10s &amp;
</code></pre>
<h2 id="启动认证代理">启动认证代理</h2>
<p>运行端口8427</p>
<pre><code class="language-shell">./vmauth-prod -auth.config=/opt/vm/config/config.yml &amp;
</code></pre>
<h2 id="basic认证配置文件">basic认证配置文件</h2>
<pre><code>users:
- username: &quot;username&quot;
  password: &quot;password&quot;
  url_prefix: &quot;http://localhost:8428&quot;
</code></pre>
<h2 id="运行端口">运行端口</h2>
<figure data-type="image" tabindex="2"><img src="https://oldcamel.run/post-images/1608878759387.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[flowable修改流程缓存实现自由流程]]></title>
        <id>https://oldcamel.run/post/flowable-xiu-gai-liu-cheng-huan-cun-shi-xian-zi-you-liu-cheng/</id>
        <link href="https://oldcamel.run/post/flowable-xiu-gai-liu-cheng-huan-cun-shi-xian-zi-you-liu-cheng/">
        </link>
        <updated>2020-12-11T08:32:34.000Z</updated>
        <content type="html"><![CDATA[<p><br>
flowable中会对流程定义做缓存处理，在实现自由流程的时候需要动态给流程定义添加节点，默认的是对流程定义做的缓存，要想在每个流程实例中动态添加节点，可以通过修改流程缓存，添加每个流程实例的缓存</p>
<h2 id="涉及到的类">涉及到的类</h2>
<figure data-type="image" tabindex="1"><img src="https://img-blog.csdnimg.cn/20200410174158592.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5MTM4NjE0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<h2 id="主要步骤">主要步骤</h2>
<ul>
<li>步骤1：修改BpmnJsonConverter,添加自定义属性，区分流程是否为自由流程</li>
<li>步骤2：设置全局的FlowableEventListener( FlowableEngineEventType. ACTIVITY_COMPLETED ),在流程启动的时候判断流程是否为自由流程，如果是的话，把流程的bpmnModel转成xml设置到自己定义的缓存对象中，然后保存到redis里。<br>
Redis采用了Hash方式存储，key为流程实例ID,value为自定义缓存类。</li>
<li>步骤3：自由流程添加节点的时候提前把流程实例id放在request作用域中</li>
<li>步骤4：定义 DefaultDeploymentCache 子类重写get方法，如果是自定义流程并且request作用域中有流程实例id就中redis中获取流程定义json转换成 ProcessDefinitionCacheEntry</li>
<li>步骤5：定义自由流程命令类，获取流程定义缓存，给里面添加连线和节点</li>
<li>步骤6：往新加的节点上跳转</li>
</ul>
<h2 id="放入缓存主要代码">放入缓存主要代码</h2>
<p>if (FlowUtils.isFreeProcess(entity.getProcessDefinitionId())) {</p>
<pre><code class="language-java"> //放入缓存
    ProcessDefinitionCacheEntry processDefinitionCacheEntry = managementService.executeCommand(new GetProcessDefinitionCacheEntryCmd(entity.getProcessDefinitionId()));
    CustomProcessDefinitionCacheEntry customProcessDefinitionCacheEntry = FlowUtils.parseCustomProcessDefinitionCacheEntry(processDefinitionCacheEntry);
    FreeProcessCaChe cache = SpringUtil.getBean(FreeProcessCaChe.class);
    cache.add(entity.getProcessInstanceId(), customProcessDefinitionCacheEntry);
}
</code></pre>
<h2 id="下方为以上用到的类以及方法">下方为以上用到的类以及方法</h2>
<ul>
<li>获取缓存命令类</li>
</ul>
<pre><code class="language-java">public class GetProcessDefinitionCacheEntryCmd implements Command&lt;ProcessDefinitionCacheEntry&gt; {
	protected String processDefinitionId;

	public GetProcessDefinitionCacheEntryCmd(String processDefinitionId) {
		this.processDefinitionId = processDefinitionId;
	}

	@Override
	public ProcessDefinitionCacheEntry execute(CommandContext commandContext) {
		DeploymentManager deploymentManager = CommandContextUtil.getProcessEngineConfiguration().getDeploymentManager();
		ProcessDefinitionCacheEntry processDefinitionCacheEntry = deploymentManager.getProcessDefinitionCache()
				.get(processDefinitionId);
		return processDefinitionCacheEntry;
	}

}
</code></pre>
<ul>
<li>序列化缓存对象方法</li>
</ul>
<pre><code class="language-java"> public static CustomProcessDefinitionCacheEntry parseCustomProcessDefinitionCacheEntry(ProcessDefinitionCacheEntry processDefinitionCacheEntry) {
    CustomProcessDefinitionCacheEntry customProcessDefinitionCacheEntry = new CustomProcessDefinitionCacheEntry();
    ProcessDefinition processDefinition = processDefinitionCacheEntry.getProcessDefinition();
    String resourceName = processDefinition.getResourceName();
    String deploymentId = processDefinition.getDeploymentId();
    BpmnModel bpmnModel = processDefinitionCacheEntry.getBpmnModel();
    BpmnXMLConverter bpmnXMLConverter = new BpmnXMLConverter();
    byte[] bytes = bpmnXMLConverter.convertToXML(bpmnModel);
    customProcessDefinitionCacheEntry.setBpmnModel(bytes);
    customProcessDefinitionCacheEntry.setDeploymentId(deploymentId);
    customProcessDefinitionCacheEntry.setResourceName(resourceName);
    return customProcessDefinitionCacheEntry;
}
</code></pre>
<ul>
<li>Redis存储的对象</li>
</ul>
<pre><code class="language-java">@Data
@AllArgsConstructor
@NoArgsConstructor
public class CustomProcessDefinitionCacheEntry implements Serializable {
    private static final long serialVersionUID = 6833801933658529071L;
    protected  String deploymentId;
    protected  String resourceName;
    protected byte[] bpmnModel;
}
</code></pre>
<ul>
<li>添加到redis接口</li>
</ul>
<pre><code class="language-java">public interface FreeProcessCaChe {
    CustomProcessDefinitionCacheEntry get(String key);
    void set(String key,CustomProcessDefinitionCacheEntry process);

    boolean contains(String key);

    void add(String key, CustomProcessDefinitionCacheEntry process);

    void remove(String key);

    void clear();
}
</code></pre>
<ul>
<li>添加到redis接口实现</li>
</ul>
<pre><code class="language-java">@Service
public class FreeProcessCaCheImpl implements FreeProcessCaChe {
    private String hashkey = &quot;freeprocesscache&quot;;

    @PostConstruct
    public void init() {
        boolean b = redisUtil.hasKey(hashkey);
        if (!b) {
            redisUtil.hset(hashkey, &quot;init&quot;, &quot;init&quot;);
        }
    }


    @Autowired
    private RedisUtil redisUtil;

    @Override
    public CustomProcessDefinitionCacheEntry get(String key) {
        if (StringUtils.isBlank(key)) {
            return null;
        }
        Object value = redisUtil.hget(hashkey, key);
        if (value != null) {
            JSONObject jb = (JSONObject) value;
            CustomProcessDefinitionCacheEntry customProcessDefinitionCacheEntry = JSON.toJavaObject(jb, CustomProcessDefinitionCacheEntry.class);
            return customProcessDefinitionCacheEntry;
        }
        return null;
    }

    @Override
    public boolean contains(String key) {
        return redisUtil.hHasKey(hashkey, key);
    }

    @Override
    public void add(String key, CustomProcessDefinitionCacheEntry process) {
        boolean hset = redisUtil.hset(hashkey, key, process);
        if (!hset) {
            throw new CheckErrorException(&quot;缓存自由流程信息失败&quot;);
        }
    }

    @Override
    public void set(String key, CustomProcessDefinitionCacheEntry process) {
        boolean hset = redisUtil.hset(hashkey, key, process);
        if (!hset) {
            throw new CheckErrorException(&quot;更新自由流程信息失败&quot;);
        }
    }

    @Override
    public void remove(String key) {
        redisUtil.hdel(hashkey, key);
    }

    @Override
    public void clear() {
        redisUtil.hmset(hashkey, Maps.newHashMap());
    }
}
</code></pre>
<h2 id="修改流程配置类添加自定义缓存获取">修改流程配置类添加自定义缓存获取</h2>
<pre><code class="language-java">springProcessEngineConfiguration.setProcessDefinitionCache(new CustomDeploymentCache&lt;&gt;());
</code></pre>
<p>设置的自定义缓存获取类（主要是重写获取缓存的get方法，以下仅为参考）</p>
<pre><code class="language-java">public class CustomDeploymentCache&lt;T&gt; extends DefaultDeploymentCache&lt;T&gt;  {
    @Override
    public T get(String id) {
        T t = super.get(id);
        if(t==null){
            return  t;
        }
        String processInstanceId=null;
        try {
             processInstanceId = (String) FlowUtils.getRequest().getAttribute(&quot;processInstanceId&quot;);
            if (StringUtils.isBlank(processInstanceId)) {
                return t;
            }
        }catch (Exception e){
            return t;
        }
        if(t instanceof ProcessDefinitionCacheEntry) {
            JSONObject jsonObject = new JSONObject();
            Process mainProcess = ((ProcessDefinitionCacheEntry) t).getBpmnModel().getMainProcess();
            String processGlobelSettings = &quot;processGlobelSettings&quot;;
            List&lt;ExtensionElement&gt; extensionElements = mainProcess.getExtensionElements().get(processGlobelSettings);
            if (extensionElements != null &amp;&amp; (!extensionElements.isEmpty())) {
                ExtensionElement extensionElement = extensionElements.get(0);
                String elementText = extensionElement.getElementText();
                jsonObject = JSON.parseObject(elementText, Feature.OrderedField);
            }
           boolean freeProcess=jsonObject.getBooleanValue(&quot;freeProcess&quot;);
            if (!freeProcess) {
                return super.get(id);
            } else {
                //从缓存中取值
                FreeProcessCaChe cache = SpringUtil.getBean(FreeProcessCaChe.class);
                CustomProcessDefinitionCacheEntry redisProcessDefinitionCacheEntry = cache.get(processInstanceId);
                BpmnModel bpmnModel=null;
                if(redisProcessDefinitionCacheEntry==null){
                    return  t;
                }else {
                    try {
                        bpmnModel = FlowUtils.parseBpmnModelFromCustomProcessDefinitionCacheEntry(redisProcessDefinitionCacheEntry);
                    } catch (Exception e) {
                        e.printStackTrace();
                    }
                }
                Process process = bpmnModel.getMainProcess();
                RuntimeService  runtimeService = SpringUtil.getBean(RuntimeService.class);
                RepositoryService repositoryService = SpringUtil.getBean(RepositoryService.class);
                ProcessInstance processInstance = runtimeService.createProcessInstanceQuery().processInstanceId(processInstanceId).singleResult();
                String processDefinitionId = processInstance.getProcessDefinitionId();
                ProcessDefinition dataProcessDefinition = null;
                if(t!=null){
                    ProcessDefinitionCacheEntry pdc=(ProcessDefinitionCacheEntry)t;
                    dataProcessDefinition=pdc.getProcessDefinition();
                }else{
                    dataProcessDefinition=repositoryService.createProcessDefinitionQuery().processDefinitionId(processDefinitionId).singleResult();
                }
                ProcessDefinitionCacheEntry customProcessDefinitionCacheEntry = new ProcessDefinitionCacheEntry(dataProcessDefinition,bpmnModel,process);

                return (T) customProcessDefinitionCacheEntry;
            }
        }else{
            return super.get(id);
        }
    }
}
</code></pre>
]]></content>
    </entry>
</feed>