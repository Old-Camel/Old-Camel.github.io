<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://oldcamel.run</id>
    <title>Old Camel</title>
    <updated>2020-12-11T01:18:05.722Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://oldcamel.run"/>
    <link rel="self" href="https://oldcamel.run/atom.xml"/>
    <subtitle>日常开发记录</subtitle>
    <logo>https://oldcamel.run/images/avatar.png</logo>
    <icon>https://oldcamel.run/favicon.ico</icon>
    <rights>All rights reserved 2020, Old Camel</rights>
    <entry>
        <title type="html"><![CDATA[ Beam+Kafka同步Oracle日志到ClickHouse]]></title>
        <id>https://oldcamel.run/post/apache-beam-tong-bu-oracle-ri-zhi-dao-clickhouse/</id>
        <link href="https://oldcamel.run/post/apache-beam-tong-bu-oracle-ri-zhi-dao-clickhouse/">
        </link>
        <updated>2020-12-10T10:19:02.000Z</updated>
        <content type="html"><![CDATA[<p>用kafka的oracle connect插件存储oracle日志，用beam读取kafka数据插入到clickhouse中</p>
<h2 id="oracle配置">oracle配置</h2>
<h3 id="archivelog模式">archivelog模式</h3>
<p>数据库必须处于archivelog模式，并且必须启用补充日志记录<br>
在数据库服务器上执行</p>
<pre><code>sqlplus / as sysdba    
SQL&gt;shutdown immediate
SQL&gt;startup mount
SQL&gt;alter database archivelog;
SQL&gt;alter database open;
</code></pre>
<h3 id="启用补充日志记录">启用补充日志记录</h3>
<pre><code>sqlplus / as sysdba    
SQL&gt;alter database add supplemental log data (all) columns;
</code></pre>
<h2 id="kafka配置">kafka配置</h2>
<h3 id="插件配置文件">插件配置文件</h3>
<p>在$KAFKA_HOME/config 目录新建OracleSourceConnector.properties配置文件，<br>
文件内示例配置</p>
<pre><code>name=oracle-logminer-connector
connector.class=com.ecer.kafka.connect.oracle.OracleSourceConnector
db.name.alias=test
tasks.max=1
topic=cdctest
db.name=testdb
db.hostname=10.1.X.X
db.port=1521
db.user=kminer
db.user.password=kminerpass
db.fetch.size=1
table.whitelist=TEST.*,TEST2.TABLE2
table.blacklist=TEST2.TABLE3
parse.dml.data=true
reset.offset=false
multitenant=false
</code></pre>
<h3 id="配置说明">配置说明</h3>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>连接器名称</td>
</tr>
<tr>
<td>connector.class</td>
<td>String</td>
<td>此连接器的Java类的名称</td>
</tr>
<tr>
<td>db.name.alias</td>
<td>String</td>
<td>数据库的标识符名称（例如Test，Dev，Prod）或用于标识数据库的特定名称该名称将用作主题和架构名称的标头</td>
</tr>
<tr>
<td>tasks.max</td>
<td>Integer</td>
<td>创建的最大任务数此连接器使用单个任务.</td>
</tr>
<tr>
<td>topic</td>
<td>String</td>
<td>消息将写入的主题的名称如果设置了值，则所有消息都将写入此声明的topic，如果未设置，则将为每个数据库表动态创建一个主题</td>
</tr>
<tr>
<td>db.name</td>
<td>String</td>
<td>要连接的数据库的服务名称或sid通常使用数据库服务名称</td>
</tr>
<tr>
<td>db.hostname</td>
<td>String</td>
<td>Oracle数据库服务器的IP地址或主机名</td>
</tr>
<tr>
<td>db.port</td>
<td>Integer</td>
<td>Oracle数据库服务器的端口号</td>
</tr>
<tr>
<td>db.user</td>
<td>String</td>
<td>数据库的用户名</td>
</tr>
<tr>
<td>db.user.password</td>
<td>String</td>
<td>数据库用户密码</td>
</tr>
<tr>
<td>db.fetch.size</td>
<td>Integer</td>
<td>此配置属性设置Oracle行提取大小值</td>
</tr>
<tr>
<td>table.whitelist</td>
<td>String</td>
<td>白名单格式为用户名.表名用逗号分隔，如果要收集用户下的所有的表，用用户名.*</td>
</tr>
<tr>
<td>parse.dml.data</td>
<td>Boolean</td>
<td>如果为true，则将捕获的sql DML语句解析为字段和值;如果为false，则仅发布sql DML语句</td>
</tr>
<tr>
<td>reset.offset</td>
<td>Boolean</td>
<td>如果为true，则在连接器启动时将偏移值设置为数据库的当前SCN如果为false，则连接器将从上一个偏移值开始</td>
</tr>
<tr>
<td>start.scn</td>
<td>Long</td>
<td>如果设置此属性，则将偏移值设置为该指定值，并且logminer将从此SCN启动如果连接器希望从所需的SCN启动，则可以使用此属性</td>
</tr>
<tr>
<td>multitenant</td>
<td>Boolean</td>
<td>如果为true，则启用多租户支持如果为false，将使用单实例配置</td>
</tr>
<tr>
<td>table.blacklist</td>
<td>String</td>
<td>黑名单格式为用户名.表名用逗号分隔，如果要收集用户下的所有的表，用用户名.*.</td>
</tr>
<tr>
<td>dml.types</td>
<td>String</td>
<td>以逗号分隔的DML操作列表（INSERT，UPDATE，DELETE）如果未指定，则将执行复制所有DML操作的默认行为，如果指定，则仅捕获指定的操作</td>
</tr>
</tbody>
</table>
<p>以上为机翻，原说明文档地址见<br>
<a href="https://github.com/erdemcer/kafka-connect-oracle/blob/master/README.md">https://github.com/erdemcer/kafka-connect-oracle/blob/master/README.md</a></p>
<h3 id="添加kafka插件包">添加kafka插件包</h3>
<p><a href="https://www.jianguoyun.com/p/DSQTC_AQz5aGCRiHodMD"><mark>kafka-connect-oracle-1.0.68.jar</mark></a>和相应版本的oracle的驱动包放入到 $KAFKA_HOME/lib文件夹中</p>
<h3 id="启动插件">启动插件</h3>
<pre><code>cd $KAFKA_HOME
./bin/connect-standalone.sh ./config/connect-standalone.properties ./config/OracleSourceConnector.properties
</code></pre>
<h3 id="kafka消息说明">kafka消息说明</h3>
<table>
<thead>
<tr>
<th>字段</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>SCN</td>
<td>数据库日志时间</td>
</tr>
<tr>
<td>SEG_OWNER</td>
<td>数据库用户名</td>
</tr>
<tr>
<td>TABLE_NAME</td>
<td>数据库表名</td>
</tr>
<tr>
<td>TIMESTAMP</td>
<td>时间戳</td>
</tr>
<tr>
<td>SQL_REDO</td>
<td>执行的sql语句</td>
</tr>
<tr>
<td>OPERATION</td>
<td>操作</td>
</tr>
<tr>
<td>DATA</td>
<td>更新后的数据</td>
</tr>
<tr>
<td>BEFORE</td>
<td>更新前的数据</td>
</tr>
</tbody>
</table>
<h2 id="clickhouse-建表">ClickHouse 建表</h2>
<pre><code class="language-sql">CREATE TABLE default.ODB_LOG
(
SCN        Int64,
SEG_OWNER  String,
TABLE_NAME String,
`TIMESTAMP` DateTime,
SQL_REDO String,
OPERATION String,
 `DATA` String,
`BEFORE` String
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(TIMESTAMP)
ORDER BY  (SEG_OWNER,TABLE_NAME,OPERATION,TIMESTAMP )
SETTINGS index_granularity=8192;
</code></pre>
<h2 id="beam-程序">Beam 程序</h2>
<h3 id="maven添加-依赖包">maven添加 依赖包</h3>
<pre><code class="language-xml">        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.beam&lt;/groupId&gt;
            &lt;artifactId&gt;beam-sdks-java-io-clickhouse&lt;/artifactId&gt;
            &lt;version&gt;${beam.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.beam&lt;/groupId&gt;
            &lt;artifactId&gt;beam-sdks-java-io-kafka&lt;/artifactId&gt;
            &lt;version&gt;${beam.version}&lt;/version&gt;
        &lt;/dependency&gt;
         &lt;dependency&gt;
            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
            &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
            &lt;version&gt;2.4.1&lt;/version&gt;
            &lt;!--        &lt;scope&gt;runtime&lt;/scope&gt;--&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.beam&lt;/groupId&gt;
            &lt;artifactId&gt;beam-runners-flink-1.11&lt;/artifactId&gt;
            &lt;version&gt;${beam.version}&lt;/version&gt;
        &lt;/dependency&gt;

</code></pre>
<h3 id="创建配置参数类">创建配置参数类</h3>
<pre><code class="language-java">package com.yunzainfo.kol.config;

import org.apache.beam.runners.flink.FlinkPipelineOptions;
import org.apache.beam.sdk.options.Default;
import org.apache.beam.sdk.options.Description;
import org.apache.beam.sdk.options.PipelineOptions;
import org.apache.beam.sdk.options.Validation.Required;

/**
 * Created by IntelliJ IDEA
 * TODO: TODO
 *
 * @author: 徐成
 * Date: 2020/12/2
 * Time: 10:08 上午
 * Email: old_camel@163.com
 */
public interface KolOptions extends FlinkPipelineOptions {
//public interface OdcOptions extends PipelineOptions {
    @Description(&quot;kafka链接地址&quot;)
    @Required
    @Default.String(&quot;xxx.xxx.xxx.xxx:9092&quot;)
    String getBootstrapServers();
    void setBootstrapServers(String value);
    @Description(&quot;kafka 消息主题&quot;)
    @Required
    @Default.String(&quot;kol&quot;)
    String getTopic();
    void setTopic(String value);
    @Description(&quot;分组id&quot;)
    @Required
    @Default.String(&quot;bdf_kol&quot;)
    String getGroupid();
    void setGroupid(String value);
    @Description(&quot;kafka用户&quot;)
    @Required
    @Default.String(&quot;xxx&quot;)
    String getKafkaUsername();
    void setKafkaUsername(String kafkaUsername);
    @Description(&quot;kafka密码&quot;)
    @Required
    @Default.String(&quot;xxx&quot;)
    String getKafkaPassword();
    void setKafkaPassword(String kafkaPassword);
    @Description(&quot;ClickHouse连接地址&quot;)
    @Required
    @Default.String(&quot;xxx.xxx.xxx.xxx:xxxx/default&quot;)
    String getClickHouseUrl();
    void setClickHouseUrl(String value);
    @Description(&quot;ClickHouse用户名&quot;)
    @Required
    @Default.String(&quot;root&quot;)
    String getClickHouseUserName();
    void setClickHouseUserName(String value);
    @Description(&quot;ClickHouse密码&quot;)
    @Required
    @Default.String(&quot;xxx&quot;)
    String getClickHousePassword();
    void setClickHousePassword(String value);
    @Description(&quot;ClickHouse表&quot;)
    @Required
    @Default.String(&quot;ODB_LOG&quot;)
    String getClickHouseTableName();
    void setClickHouseTableName(String value);
}


</code></pre>
<h3 id="kafka消息映射类">kafka消息映射类</h3>
<pre><code class="language-java">package com.yunzainfo.kol.model;

import org.codehaus.jackson.annotate.JsonProperty;

import java.io.Serializable;
import java.util.Date;

/**
 * Created by IntelliJ IDEA
 * TODO: TODO
 *
 * @author: 徐成
 * Date: 2020/11/30
 * Time: 2:32 下午
 * Email: old_camel@163.com
 */
public class Payload implements Serializable {
    private static final long serialVersionUID = 1L;
    @JsonProperty(value = &quot;SCN&quot;)
    private Long SCN;
    @JsonProperty(value = &quot;SEG_OWNER&quot;)
    private String SEG_OWNER;
    @JsonProperty(value = &quot;TABLE_NAME&quot;)
    private String TABLE_NAME;
    @JsonProperty(value = &quot;TIMESTAMP&quot;)
    private Long TIMESTAMP;
    @JsonProperty(value = &quot;SQL_REDO&quot;)
    private String SQL_REDO;
    @JsonProperty(value = &quot;OPERATION&quot;)
    private String OPERATION;
    @JsonProperty(value = &quot;data&quot;)
    private Object DATA;
    @JsonProperty(value = &quot;before&quot;)
    private Object BEFORE;

    public static long getSerialVersionUID() {
        return serialVersionUID;
    }

    public Long getSCN() {
        return SCN;
    }

    public void setSCN(Long SCN) {
        this.SCN = SCN;
    }

    public String getSEG_OWNER() {
        return SEG_OWNER;
    }

    public void setSEG_OWNER(String SEG_OWNER) {
        this.SEG_OWNER = SEG_OWNER;
    }

    public String getTABLE_NAME() {
        return TABLE_NAME;
    }

    public void setTABLE_NAME(String TABLE_NAME) {
        this.TABLE_NAME = TABLE_NAME;
    }

    public Long getTIMESTAMP() {
        return TIMESTAMP;
    }

    public void setTIMESTAMP(Long TIMESTAMP) {
        this.TIMESTAMP = TIMESTAMP;
    }

    public String getSQL_REDO() {
        return SQL_REDO;
    }

    public void setSQL_REDO(String SQL_REDO) {
        this.SQL_REDO = SQL_REDO;
    }

    public String getOPERATION() {
        return OPERATION;
    }

    public void setOPERATION(String OPERATION) {
        this.OPERATION = OPERATION;
    }

    public Object getDATA() {
        return DATA;
    }

    public void setDATA(Object DATA) {
        this.DATA = DATA;
    }

    public Object getBEFORE() {
        return BEFORE;
    }

    public void setBEFORE(Object BEFORE) {
        this.BEFORE = BEFORE;
    }

    @Override
    public String toString() {
        return &quot;Payload{&quot; +
                &quot;SCN=&quot; + SCN +
                &quot;, SEG_OWNER='&quot; + SEG_OWNER + '\'' +
                &quot;, TABLE_NAME='&quot; + TABLE_NAME + '\'' +
                &quot;, TIMESTAMP=&quot; + TIMESTAMP +
                &quot;, SQL_REDO='&quot; + SQL_REDO + '\'' +
                &quot;, OPERATION='&quot; + OPERATION + '\'' +
                &quot;, DATA='&quot; + DATA + '\'' +
                &quot;, BEFORE='&quot; + BEFORE + '\'' +
                '}';
    }
}


</code></pre>
<h3 id="主程序">主程序</h3>
<pre><code class="language-java">package com.yunzainfo.kol;

import com.yunzainfo.kol.config.KolOptions;
import com.yunzainfo.kol.model.Payload;
import org.apache.beam.runners.flink.FlinkRunner;
import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.coders.SerializableCoder;
import org.apache.beam.sdk.io.clickhouse.ClickHouseIO;
import org.apache.beam.sdk.io.kafka.KafkaIO;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.schemas.Schema;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.transforms.Values;
import org.apache.beam.sdk.values.Row;
import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;
import org.apache.kafka.clients.CommonClientConfigs;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.config.SaslConfigs;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.codehaus.jackson.map.ObjectMapper;
import org.joda.time.DateTime;
import org.joda.time.Duration;

import java.io.IOException;
import java.util.LinkedHashMap;
import java.util.Map;

/**
 * Created by IntelliJ IDEA
 * TODO: TODO
 *
 * @author: 徐成
 * Date: 2020/11/27
 * Time: 5:49 下午
 * Email: old_camel@163.com
 */
public class KolApp {


    public static void main(String[] args) {
        KolOptions options = PipelineOptionsFactory.fromArgs(args).as(KolOptions.class);
        options.setRunner(FlinkRunner.class);
        runOdc(options);
    }

    public static Object isnull(Object o) {
        if (o == null) {
            return &quot;&quot;;
        } else {
            return o;
        }
    }

    public static void runOdc(KolOptions options) {
        Pipeline p = Pipeline.create(options);
        final Map&lt;String, Object&gt; immutableMap = new ImmutableMap.Builder&lt;String, Object&gt;()
                .put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;)
                .put(ConsumerConfig.GROUP_ID_CONFIG, options.getGroupid())
                .put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true)
                .put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, &quot;SASL_PLAINTEXT&quot;)
                .put(SaslConfigs.SASL_MECHANISM, &quot;PLAIN&quot;)
                .put(SaslConfigs.SASL_JAAS_CONFIG, &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;&quot; + options.getKafkaUsername() + &quot;\&quot; password=\&quot;&quot; + options.getKafkaPassword() + &quot;\&quot;;&quot;)
                .build();
        final Schema ckType = Schema.of(
                Schema.Field.of(&quot;SCN&quot;, Schema.FieldType.INT64.withNullable(true)),
                Schema.Field.of(&quot;SEG_OWNER&quot;, Schema.FieldType.STRING.withNullable(true)),
                Schema.Field.of(&quot;TABLE_NAME&quot;, Schema.FieldType.STRING.withNullable(true)),
                Schema.Field.of(&quot;TIMESTAMP&quot;, Schema.FieldType.DATETIME.withNullable(true)),
                Schema.Field.of(&quot;SQL_REDO&quot;, Schema.FieldType.STRING.withNullable(true)),
                Schema.Field.of(&quot;OPERATION&quot;, Schema.FieldType.STRING.withNullable(true)),
                Schema.Field.of(&quot;DATA&quot;, Schema.FieldType.STRING.withNullable(true)),
                Schema.Field.of(&quot;BEFORE&quot;, Schema.FieldType.STRING.withNullable(true))
        );
        p.apply(&quot;读取消息&quot;, KafkaIO.&lt;String, String&gt;read()
                .withBootstrapServers(options.getBootstrapServers())
                .withTopic(options.getTopic())
                .withKeyDeserializer(StringDeserializer.class)
                .withValueDeserializer(StringDeserializer.class)
                .withOffsetConsumerConfigOverrides(ImmutableMap.&lt;String, Object&gt;of(ConsumerConfig.GROUP_ID_CONFIG, options.getGroupid()))
                .withConsumerConfigUpdates(immutableMap)
                .withReadCommitted()
                .withoutMetadata())
                .apply(Values.create())
                .apply(&quot;转义Payload&quot;, ParDo.of(new DoFn&lt;String, Payload&gt;() {
                    private static final long serialVersionUID = 1L;

                    @ProcessElement
                    public void processElement(ProcessContext ctx) {
                        String element = ctx.element();
                        ObjectMapper mapper = new ObjectMapper();
                        try {
                            LinkedHashMap map = mapper.readValue(element, LinkedHashMap.class);                //Object payload = data.get(&quot;payload&quot;);
                            LinkedHashMap payloadMap = (LinkedHashMap) map.get(&quot;payload&quot;);
                            Payload payload = mapper.convertValue(payloadMap, Payload.class);
                            payload.setDATA(mapper.writeValueAsString(payload.getDATA()));
                            payload.setBEFORE(mapper.writeValueAsString(payload.getBEFORE()));
                            ctx.output(payload);
                        } catch (IOException e) {
                            e.printStackTrace();
                        }
                    }
                }))
                .setCoder(SerializableCoder.of(Payload.class))
                .apply(&quot;转换Row&quot;, ParDo.of(new DoFn&lt;Payload, Row&gt;() {
                    @ProcessElement
                    public void processElement(ProcessContext cxt) {
                        Payload payload = cxt.element();
                        Row alarmRow = Row.withSchema(ckType).addValues(
                                isnull(payload.getSCN()),
                                isnull(payload.getSEG_OWNER()),
                                isnull(payload.getTABLE_NAME()),
                                isnull(new DateTime(payload.getTIMESTAMP())),
                                isnull(payload.getSQL_REDO()),
                                isnull(payload.getOPERATION()),
                                isnull(payload.getDATA()),
                                isnull(payload.getBEFORE())
                        ).build();
                        cxt.output(alarmRow);
                    }
                })).setRowSchema(ckType)
                .apply(&quot;写入ClickHouse数据库&quot;,
                        ClickHouseIO.&lt;Row&gt;write(&quot;jdbc:clickhouse://&quot; + options.getClickHouseUrl() + &quot;?username=&quot; + options.getClickHouseUserName() + &quot;&amp;password=&quot; + options.getClickHousePassword(), options.getClickHouseTableName())
                                .withMaxRetries(3)//重试次数
                                .withInsertDeduplicate(true)//重复数据是否删除
                                .withMaxInsertBlockSize(1)//添加最大块的大小
                                .withInitialBackoff(Duration.standardSeconds(5))//初始退回时间
                                .withInsertDistributedSync(false)
                );
        p.run().waitUntilFinish();

    }
}

</code></pre>
<h2 id="把beam包放在flink上运行">把beam包放在flink上运行</h2>
<figure data-type="image" tabindex="1"><img src="https://oldcamel.run/post-images/1607602332596.png" alt="" loading="lazy"></figure>
<h2 id="clickhouse中的数据">clickhouse中的数据</h2>
<figure data-type="image" tabindex="2"><img src="https://oldcamel.run/post-images/1607602692592.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
</feed>