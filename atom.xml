<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://oldcamel.run</id>
    <title>Old Camel</title>
    <updated>2021-05-31T09:14:10.832Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://oldcamel.run"/>
    <link rel="self" href="https://oldcamel.run/atom.xml"/>
    <subtitle>日常开发记录 
&lt;div&gt;&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=330 height=86 src=&quot;//music.163.com/outchain/player?type=2&amp;id=18611643&amp;auto=1&amp;height=66&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;</subtitle>
    <logo>https://oldcamel.run/images/avatar.png</logo>
    <icon>https://oldcamel.run/favicon.ico</icon>
    <rights>All rights reserved 2021, Old Camel</rights>
    <entry>
        <title type="html"><![CDATA[4.0 新的开端]]></title>
        <id>https://oldcamel.run/post/40-xin-de-kai-duan/</id>
        <link href="https://oldcamel.run/post/40-xin-de-kai-duan/">
        </link>
        <updated>2021-05-31T08:22:46.000Z</updated>
        <content type="html"><![CDATA[<p>又又又又开始搭建新框架了,这次开搞多租户云服务。</p>
<table>
<thead>
<tr>
<th>初步计划列表</th>
</tr>
</thead>
<tbody>
<tr>
<td>stars-spring-boot-starter  多数据源  jpa+mybatis</td>
</tr>
<tr>
<td>stars-spring-boot-starter  mybatis 换成mybatisplus</td>
</tr>
<tr>
<td>stars-spring-boot-starter  liquibase初步集成（多数据源在启动时刷新库）</td>
</tr>
<tr>
<td>认证中心 升级 2.5.0  spring-cloud-oauth2换成 spring-security  oauth2</td>
</tr>
<tr>
<td>认证中心 jpa在pgsql里运行, 调试功能</td>
</tr>
<tr>
<td>stars-spring-boot-starter  添加通用设置+工具类+pom依赖（依赖项目pom精简）</td>
</tr>
<tr>
<td>stars-archetype maven模板项目重新修改</td>
</tr>
<tr>
<td>k8s rook安装cephfs，测试pvc动态动态挂载，s3储存</td>
</tr>
<tr>
<td>pgsql helm版本</td>
</tr>
<tr>
<td>基于master数据源做租户管理功能。租户数据调整-&gt;动态增减数据源-&gt;liquibase初始化库（发布订阅）</td>
</tr>
<tr>
<td>文件中心 mongo存储修改为 cephfs存储 （minio）</td>
</tr>
<tr>
<td>消息中心等其他基础组件修改</td>
</tr>
<tr>
<td>cephfs 备份脚本</td>
</tr>
</tbody>
</table>
<h2 id="stars-spring-boot-starter-多数据源-jpamybatis">stars-spring-boot-starter  多数据源  jpa+mybatis</h2>
<p>多租户系统,数据库按租户区分数据源。计划每个租户单独数据源部署，所以先搞个启动器配置多数据源。<br>
多数据源实现:继承AbstractRoutingDataSource，重写determineCurrentLookupKey方法 返回自定义的key。这个key 用拦截器在请求头里获取用户的租户值放到一个线程变量里。</p>
<ul>
<li>拦截器</li>
</ul>
<pre><code class="language-java">public class TenantInterceptor implements HandlerInterceptor {
    final String X_TENANT_ID = &quot;X_TenantID&quot;;
    @Override
    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {
        final String tenantId = request.getHeader(X_TENANT_ID);
        if (tenantId != null) {
            DynamicDataSourceContextHolder.setDataSourceKey(tenantId);
        }
        return true;
    }

    @Override
    public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception {
        DynamicDataSourceContextHolder.clearDataSourceKey();
    }
}

@Configuration
public class TenantConfig implements WebMvcConfigurer {
    @Override
    public void addInterceptors(InterceptorRegistry registry) {
        //指定拦截器，指定拦截路径
        registry.addInterceptor(new TenantInterceptor()).addPathPatterns(&quot;/**&quot;);
    }
}

</code></pre>
<ul>
<li>线程变量</li>
</ul>
<pre><code class="language-java">public class DynamicDataSourceContextHolder {
  private static final ThreadLocal&lt;String&gt; contextHolder = new ThreadLocal&lt;String&gt;() {
      /**
       * 将 master 数据源的 key作为默认数据源的 key
       */
      @Override
      protected String initialValue() {
          return &quot;master&quot;;
      }
  };

  /**
   * 数据源的 key集合，用于切换时判断数据源是否存在
   */
  public static Set&lt;Object&gt; dataSourceKeys = new HashSet&lt;&gt;();

  /**
   * 切换数据源
   * @param key  数据源
   */
  public static void setDataSourceKey(String key) {
      if (StringUtils.isNotBlank(key)) {
          contextHolder.set(key);
      }
  }

  /**
   * 获取数据源
   * @return
   */
  public static String getDataSourceKey() {
      return contextHolder.get();
  }

  /**
   * 重置数据源
   */
  public static void clearDataSourceKey() {
      contextHolder.remove();
  }

  /**
   * 判断是否包含数据源
   * @param key   数据源
   * @return
   */
  public static boolean containDataSourceKey(String key) {
      return dataSourceKeys.contains(key);
  }

  /**
   * 添加数据源Keys
   * @param keys
   * @return
   */
  public static boolean addDataSourceKeys(Collection&lt;? extends Object&gt; keys) {
      return dataSourceKeys.addAll(keys);
  }
}
</code></pre>
<ul>
<li>配置数据源</li>
</ul>
<pre><code class="language-java">
 @Bean(&quot;master&quot;)
   @ConfigurationProperties(prefix = &quot;spring.datasource.hikari&quot;)
   public DataSource master(){
       HikariDataSource build = (HikariDataSource)DataSourceBuilder.create().build();
       build.setTransactionIsolation(&quot;TRANSACTION_READ_COMMITTED&quot;);
       return build;
   }
   @Bean
   @Primary
   public DataSource dataSource(){
       DynamicDataSource dynamicDataSource = new DynamicDataSource();
       HashMap&lt;Object, Object&gt; dataSourceMap = new HashMap&lt;&gt;();
       dataSourceMap.put(&quot;master&quot;,master());
       dynamicDataSource.setDefaultDataSource(master());
       dynamicDataSource.setDataSources(dataSourceMap);
       dynamicDataSource.afterPropertiesSet();
       return  dynamicDataSource;
   }

</code></pre>
<ul>
<li>多数据源读取</li>
</ul>
<pre><code class="language-java">@Data
public class DynamicDataSource  extends AbstractRoutingDataSource {
  private final Map&lt;Object, Object&gt; tenantDataSources = new ConcurrentHashMap&lt;&gt;();

  @Override
  protected Object determineCurrentLookupKey() {
      return DynamicDataSourceContextHolder.getDataSourceKey();
  }


  public void setDefaultDataSource(Object defaultDataSource) {
      super.setDefaultTargetDataSource(defaultDataSource);
  }

  public void setDataSources(Map&lt;Object, Object&gt; dataSources) {
      tenantDataSources.putAll(dataSources);
      super.setTargetDataSources(tenantDataSources);
      super.afterPropertiesSet();
      DynamicDataSourceContextHolder.addDataSourceKeys(dataSources.keySet());
  }

}
</code></pre>
<ul>
<li>创建租户表</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://oldcamel.run/post-images/1622451428396.png" alt="" loading="lazy"></figure>
<ul>
<li>写ApplicationRunner在项目启动完成后添加多租户数据源</li>
</ul>
<pre><code class="language-java">  public class DynamicDataSourceInit implements ApplicationRunner {
    @Autowired
    DynamicDataSource dataSource;
    @Override
    public void run(ApplicationArguments args) throws Exception {
        HikariDataSource hikariDataSource = (HikariDataSource) SpringContextUtils.getBean(&quot;master&quot;);
        Map&lt;Object, Object&gt; resolvedDataSources =new HashMap&lt;&gt;();
        Connection connection = hikariDataSource.getConnection();
        Statement statement = connection.createStatement();
        ResultSet resultSet = statement.executeQuery(&quot;select tenant_id   \&quot;tenantId\&quot;,tenant_name \&quot;tenantName\&quot;,datasource_url \&quot;datasourceUrl\&quot;,datasource_username \&quot;datasourceUsername\&quot;,datasource_password \&quot;datasourcePassword\&quot;,datasource_driver \&quot;datasourceDriver\&quot; from tenant where status=1&quot;);
        List&lt;Tenant&gt; tenants = (ArrayList&lt;Tenant&gt;) this.populate(resultSet, Tenant.class);
        for (Tenant tenant : tenants) {
            HikariDataSource dataSource = new HikariDataSource();
            dataSource.setDriverClassName(tenant.getDatasourceDriver());
            dataSource.setJdbcUrl(tenant.getDatasourceUrl());
            dataSource.setUsername(tenant.getDatasourceUsername());
            dataSource.setPassword(tenant.getDatasourcePassword());
            dataSource.setConnectionTestQuery(&quot;SELECT 1&quot;);
            dataSource.setTransactionIsolation(&quot;TRANSACTION_READ_COMMITTED&quot;);
            dataSource.setDataSourceProperties(hikariDataSource.getDataSourceProperties());
            resolvedDataSources.put(tenant.getTenantId(), dataSource);
        }
        dataSource.setDataSources(resolvedDataSources);
        dataSource.afterPropertiesSet();
        connection.close();
    }
    private    List populate(ResultSet rs , Class clazz) throws SQLException, InstantiationException, IllegalAccessException{
        //结果集的元素对象
        ResultSetMetaData rsmd = rs.getMetaData();
        //获取结果集的元素个数
        int colCount = rsmd.getColumnCount();
        //返回结果的列表集合
        List list = new ArrayList();
        //业务对象的属性数组
        Field[] fields = clazz.getDeclaredFields();
        while(rs.next()){//对每一条记录进行操作
            Object obj = clazz.newInstance();//构造业务对象实体
            //将每一个字段取出进行赋值
            for(int i = 1;i&lt;=colCount;i++){
                Object value = rs.getObject(i);
                //寻找该列对应的对象属性
                for(int j=0;j&lt;fields.length;j++){
                    Field f = fields[j];
                    //如果匹配进行赋值
                    if(f.getName().equalsIgnoreCase(rsmd.getColumnName(i))){
                        boolean flag = f.isAccessible();
                        f.setAccessible(true);
                        f.set(obj, value);
                        f.setAccessible(flag);
                    }
                }
            }
            list.add(obj);
        }
        return list;
    }
}
</code></pre>
<h1 id="stars-spring-boot-starter-mybatis-换成mybatisplus">stars-spring-boot-starter  mybatis 换成mybatisplus</h1>
<p>因为部分项目组用了mybatisplus,在启动器里把mybatis升级成mybatisplus，里面分页插件 采用 pagehelper+mybatisplus自带分页 并存的方式。</p>
<ul>
<li>添加依赖</li>
</ul>
<pre><code class="language-xml"> &lt;dependency&gt;
            &lt;groupId&gt;com.baomidou&lt;/groupId&gt;
            &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt;
            &lt;version&gt;3.4.3&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt;
            &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt;
            &lt;version&gt;1.3.0&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt;
                    &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency
</code></pre>
<ul>
<li>mybatisplus+ 分页 配置</li>
</ul>
<pre><code class="language-java">    @Bean
    @ConfigurationProperties(prefix = &quot;pagehelper&quot;)
    public Properties pageHelperProperties() {
        return new Properties();
    }
   @Bean
    public MybatisSqlSessionFactoryBean sqlSessionFactoryBean() throws Exception {
        MybatisSqlSessionFactoryBean sessionFactory = new MybatisSqlSessionFactoryBean();
        Interceptor[] plugins = new Interceptor[2];
        plugins[0] = mybatisPlusInterceptor();
        plugins[1] = pageHelperInterceptor();
        sessionFactory.setPlugins(plugins);
        sessionFactory.setDataSource(dataSource());
        PathMatchingResourcePatternResolver resolver = new PathMatchingResourcePatternResolver();
        sessionFactory.setMapperLocations(resolver.getResources(&quot;classpath*:**/*Mapper.xml&quot;));
        return sessionFactory;
    }
    @Bean
    public PlatformTransactionManager transactionManager() {
        return new DataSourceTransactionManager(dataSource());
    }
    @Bean
    public MybatisPlusInterceptor mybatisPlusInterceptor() {
        MybatisPlusInterceptor interceptor = new MybatisPlusInterceptor();
        PaginationInnerInterceptor paginationInnerInterceptor = new PaginationInnerInterceptor(DbType.POSTGRE_SQL);
        BlockAttackInnerInterceptor blockAttackInnerInterceptor = new BlockAttackInnerInterceptor();
        //分页插件
        interceptor.addInnerInterceptor(paginationInnerInterceptor);
        //防止全表更新与删除
        interceptor.addInnerInterceptor(blockAttackInnerInterceptor);
        return interceptor;
    }
    @Bean
    public PageInterceptor pageHelperInterceptor() {
        PageInterceptor interceptor = new PageInterceptor();
        interceptor.setProperties(this.pageHelperProperties());
        return interceptor;
    }
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes 问题处理记录]]></title>
        <id>https://oldcamel.run/post/kubernetes-wen-ti-chu-li-ji-lu/</id>
        <link href="https://oldcamel.run/post/kubernetes-wen-ti-chu-li-ji-lu/">
        </link>
        <updated>2021-05-26T01:28:44.000Z</updated>
        <content type="html"><![CDATA[<h1 id="1-强制删除namespace">1、强制删除namespace</h1>
<p>kubectl get namespace rook-ceph -o json &gt; rook-ceph.json<br>
vim rook-ceph.json<br>
删除spec中的内容<br>
<code>&quot;spec&quot;: { }</code><br>
开启kube-proxy   kubectl proxy</p>
<p>另开一个ssh登录master，执行<br>
<code>curl -k -H &quot;Content-Type: application/json&quot; -X PUT --data-binary @rook-ceph.json http://127.0.0.1:8001/api/v1/namespaces/rook-ceph/finalize</code><br>
操作完成👌</p>
<p>#2、kubectl get pod 获取不到资源</p>
<p>大概率是节点网络插件出现了问题。重启istiod 服务尝试将其运行在不同的节点上试下</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flink Sql 实现CDC数据实时同步特殊处理]]></title>
        <id>https://oldcamel.run/post/flink-sql-shi-xian-cdc-shu-ju-shi-shi-tong-bu-te-shu-chu-li/</id>
        <link href="https://oldcamel.run/post/flink-sql-shi-xian-cdc-shu-ju-shi-shi-tong-bu-te-shu-chu-li/">
        </link>
        <updated>2021-04-15T01:06:21.000Z</updated>
        <content type="html"><![CDATA[<h1 id="1-decimal-类型在kafka中解析为乱码">1、decimal 类型在kafka中解析为乱码</h1>
<p>kafka connect 插件设置 &quot;decimal.handling.mode&quot;: &quot;string&quot; 属性。将decimal解析为字符串</p>
<p>sql语句中将字符串转换为数字 用CAST(字段名称) AS INT) 函数  INT可根据不同类型的数字替换为不同的目标类型（DECIMAL(10,4) BIGINT 等）</p>
<h1 id="2-日期处理">2、日期处理</h1>
<p>kafka中人日期类型会解析成时间戳，可用TO_TIMESTAMP + FROM_UNIXTIME做转换，注意处理时区问题</p>
<p>oracle日期类型处理精度+时区：</p>
<pre><code>CREATED_DATETS AS TO_TIMESTAMP(FROM_UNIXTIME(CREATED_DATE/1000/1000-8 * 60 * 60, 'yyyy-MM-dd HH:mm:ss')),
</code></pre>
<p>其他数据库日期类型只处理日期</p>
<pre><code>FDateTS AS TO_TIMESTAMP(FROM_UNIXTIME((FDate-8 * 60 * 60 * 1000) / 1000, 'yyyy-MM-dd HH:mm:ss'))
</code></pre>
<h1 id="3-cdc数据插入设置主键">3、cdc数据插入设置主键</h1>
<pre><code>PRIMARY KEY (AAA,BBB) NOT ENFORCED
</code></pre>
<h1 id="4-kafka-sink-sasl-connect插件设置认证">4、kafka sink sasl connect插件设置认证</h1>
<pre><code>&quot;database.history.producer.sasl.mechanism&quot;: &quot;PLAIN&quot;,
&quot;database.history.producer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;admin\&quot; password=\&quot;密码\&quot;;&quot;,
&quot;database.history.producer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;,
&quot;database.history.consumer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;admin\&quot; password=\&quot;密码\&quot;;&quot;,
&quot;database.history.consumer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;,
&quot;database.history.consumer.sasl.mechanism&quot;: &quot;PLAIN&quot;
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kafka实现oracle的CDC数据实时变更]]></title>
        <id>https://oldcamel.run/post/kafka-shi-xian-oracle-de-cdc-shu-ju-shi-shi-bian-geng/</id>
        <link href="https://oldcamel.run/post/kafka-shi-xian-oracle-de-cdc-shu-ju-shi-shi-bian-geng/">
        </link>
        <updated>2021-04-14T06:02:54.000Z</updated>
        <content type="html"><![CDATA[<h1 id="使用工具-debezium-oracle-connectororacle-logminer">使用工具  debezium-oracle-connector,oracle-LogMiner</h1>
<h2 id="1-oracle-设置">1、oracle 设置</h2>
<h3 id="开启归档模式">开启归档模式</h3>
<pre><code class="language-sqlplus">alter system set db_recovery_file_dest_size = 10G;
alter system set db_recovery_file_dest = '/home/oracle/oradta/recovery_area' scope=spfile;
shutdown immediate
startup mount
alter database archivelog;
alter database open;
archive log list
exit;

ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS;

</code></pre>
<h4 id="创建用户">创建用户</h4>
<pre><code>CREATE USER myuser IDENTIFIED BY dbz
   DEFAULT TABLESPACE 命名空间
   QUOTA UNLIMITED ON 命名空间
</code></pre>
<h4 id="授权">授权</h4>
<pre><code>GRANT CREATE SESSION TO myuser;
GRANT CREATE TABLE TO myuser;
GRANT CREATE SEQUENCE TO myuser;
GRANT CREATE TRIGGER TO myuser;
GRANT CREATE SESSION TO myuser;
GRANT SELECT ON V_$DATABASE to myuser;
GRANT FLASHBACK ANY TABLE TO myuser;
GRANT SELECT ANY TABLE TO myuser;
GRANT SELECT_CATALOG_ROLE TO myuser;
GRANT EXECUTE_CATALOG_ROLE TO myuser;
GRANT SELECT ANY TRANSACTION TO myuser;
GRANT CREATE TABLE TO myuser;
GRANT LOCK ANY TABLE TO myuser;
GRANT ALTER ANY TABLE TO myuser;
GRANT CREATE SEQUENCE TO myuser;
GRANT EXECUTE ON DBMS_LOGMNR TO myuser;
GRANT EXECUTE ON DBMS_LOGMNR_D TO myuser;
GRANT SELECT ON V_$LOG TO myuser;
GRANT SELECT ON V_$LOG_HISTORY TO myuser;
GRANT SELECT ON V_$LOGMNR_LOGS TO myuser;
GRANT SELECT ON V_$LOGMNR_CONTENTS TO myuser;
GRANT SELECT ON V_$LOGMNR_PARAMETERS TO myuser;
GRANT SELECT ON V_$LOGFILE TO myuser;
GRANT SELECT ON V_$ARCHIVED_LOG TO myuser;
GRANT SELECT ON V_$ARCHIVE_DEST_STATUS TO myuser;
GRANT SELECT ON SYSTEM.LOGMNR_COL$ TO myuser;
GRANT SELECT ON SYSTEM.LOGMNR_OBJ$ TO myuser;
GRANT SELECT ON SYSTEM.LOGMNR_USER$ TO myuser;
GRANT SELECT ON SYSTEM.LOGMNR_UID$ TO myuser;
</code></pre>
<h2 id="oracle客户端设置">oracle客户端设置</h2>
<h3 id="下载">下载</h3>
<pre><code>wget &quot;https://download.oracle.com/otn_software/linux/instantclient/19600/instantclient-basiclite-linux.x64-19.6.0.0.0dbru.zip&quot; -O /tmp/ic.zip；
unzip /tmp/ic.zip -d  自定义目录
</code></pre>
<h3 id="环境变量">环境变量</h3>
<p>设置LD_LIBRARY_PATH 指向 自定义目录</p>
<h2 id="kafaka-connect-插件设置">kafaka connect 插件设置</h2>
<h3 id="下载-2">下载</h3>
<pre><code>wget &quot;https://oss.sonatype.org/service/local/artifact/maven/redirect?r=snapshots&amp;g=io.debezium&amp;a=debezium-connector-oracle&amp;v=LATEST&amp;c=plugin&amp;e=tar.gz&quot; -O /tmp/dbz-ora.tgz；
tar -xvf /tmp/dbz-ora.tgz --directory  kafaka 插件目录
</code></pre>
<h3 id="添加-ojdbc-jar到插件目录">添加 ojdbc jar到插件目录</h3>
<pre><code>      curl https://maven.xwiki.org/externals/com/oracle/jdbc/ojdbc8/12.2.0.1/ojdbc8-12.2.0.1.jar -o ojdbc8-12.2.0.1.jar
</code></pre>
<h2 id="创建kafka-connect">创建kafka connect</h2>
<pre><code>curl --location --request POST 'http://localhost:8083/connectors' \
--header 'Content-Type: application/json' \
--data-raw '{
   &quot;name&quot;: &quot;name&quot;,
   &quot;config&quot;: {
       &quot;connector.class&quot; : &quot;io.debezium.connector.oracle.OracleConnector&quot;,
       &quot;tasks.max&quot; : &quot;1&quot;,
       &quot;database.server.name&quot; : &quot;topic&quot;,
       &quot;database.hostname&quot; : &quot;database_host&quot;,
       &quot;database.port&quot; : &quot;1521&quot;,
       &quot;database.user&quot; : &quot;myuser&quot;,
       &quot;database.password&quot; : &quot;dbz&quot;,
       &quot;database.dbname&quot; : &quot;orcl&quot;,
       &quot;database.tablename.case.insensitive&quot;: &quot;true&quot;,
       &quot;database.history.kafka.bootstrap.servers&quot; : &quot;localhost:9092&quot;,
       &quot;database.history.kafka.topic&quot;: &quot;name-h&quot;,
       &quot;table.include.list&quot;:&quot;AAA.BBB,CCC.DDD&quot;,
        &quot;database.history.producer.sasl.mechanism&quot;: &quot;PLAIN&quot;,
       &quot;database.history.producer.sasl.jaas.config&quot;:         &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;admin\&quot; password=\&quot;password\&quot;;&quot;,
   &quot;database.history.producer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;,
    &quot;database.history.consumer.sasl.jaas.config&quot;:          &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;admin\&quot;   password=\&quot;password\&quot;;&quot;,
     &quot;database.history.consumer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;,
       &quot;database.history.consumer.sasl.mechanism&quot;: &quot;PLAIN&quot;
   }
}'
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[linux 安装服务器资源监控 prometheus远程写入]]></title>
        <id>https://oldcamel.run/post/linux-an-zhuang-fu-wu-qi-zi-yuan-jian-kong-prometheus-yuan-cheng-xie-ru/</id>
        <link href="https://oldcamel.run/post/linux-an-zhuang-fu-wu-qi-zi-yuan-jian-kong-prometheus-yuan-cheng-xie-ru/">
        </link>
        <updated>2021-03-11T01:07:36.000Z</updated>
        <content type="html"><![CDATA[<h2 id="1安装prometheus">1.安装prometheus</h2>
<pre><code>wget -c https://github.com/prometheus/prometheus/releases/download/v2.23.0/prometheus-2.23.0.linux-amd64.tar.gz 
tar zxvf prometheus-2.23.0.linux-amd64.tar.gz  -C /opt/ 
cd /opt/ 
ln -s prometheus-2.23.0.linux-amd64 prometheus 
cat &gt; /etc/systemd/system/prometheus.service &lt;&lt;EOF 
[Unit] 
Description=prometheus 
After=network.target 
[Service] 
Type=simple 
WorkingDirectory=/opt/prometheus 
ExecStart=/opt/prometheus/prometheus --config.file=/opt/prometheus/prometheus.yml
LimitNOFILE=65536 
PrivateTmp=true 
RestartSec=2 
StartLimitInterval=0 
Restart=always 
[Install] 
WantedBy=multi-user.target 
EOF 
systemctl daemon-reload  
systemctl enable prometheus 
systemctl start prometheus 

</code></pre>
<h3 id="2配置prometheus">2.配置Prometheus</h3>
<pre><code>cat &gt; /opt/prometheus/prometheus.yml &lt;&lt;EOF 
# my global config 
global: 
  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. 
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. 
  # scrape_timeout is set to the global default (10s). 
  external_labels:
        tenant: &quot;140test&quot;
remote_write:
  - url: &quot;url&quot;
    basic_auth:
      username: 用户名
      password: 密码
# Alertmanager configuration 
alerting: 
  alertmanagers: 
  - static_configs: 
    - targets: 
      # - alertmanager:9093 
# Load rules once and periodically evaluate them according to the global 'evaluation_interval'. 
rule_files: 
  # - &quot;first_rules.yml&quot; 
  # - &quot;second_rules.yml&quot; 
# A scrape configuration containing exactly one endpoint to scrape: 
# Here it's Prometheus itself. 
scrape_configs: 
  # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. 
  - job_name: 'servers' 
    file_sd_configs: 
    - refresh_interval: 61s 
      files: 
        - /opt/prometheus/servers/*.json 
EOF 

</code></pre>
<h2 id="3创建node配置文件">3.创建node配置文件</h2>
<pre><code>mkdir -p /opt/prometheus/servers
cd /opt/prometheus/servers
vim json.json
[     
    { 
        &quot;targets&quot;: [ 
            “xxx.xxx.xxx.xxx:9100&quot; 
        ], 
        &quot;labels&quot;: { 
            &quot;instance&quot;: &quot;xxx.xxx.xxx.xxx&quot;, 
            &quot;job&quot;: &quot;node_exporter&quot; 
        } 
    }
   
] 
</code></pre>
<h2 id="安装node_exporter">安装node_exporter</h2>
<pre><code>Wget https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-amd64.tar.gz 
tar zxvf node_exporter-1.0.1.linux-amd64.tar.gz -C /opt/ 
cd /opt/ 
ln -s  node_exporter-1.0.1.linux-amd64 node_exporter 
cat &gt; /etc/systemd/system/node_exporter.service &lt;&lt;EOF 
[Unit] 
Description=node_exporter 
After=network.target 
[Service] 
Type=simple 
WorkingDirectory=/opt/node_exporter 
ExecStart=/opt/node_exporter/node_exporter 
LimitNOFILE=65536 
PrivateTmp=true 
RestartSec=2 
StartLimitInterval=0 
Restart=always 
[Install] 
WantedBy=multi-user.target 
EOF 
systemctl daemon-reload 
systemctl enable node_exporter 
systemctl start node_exporter 
systemctl restart prometheus 

</code></pre>
<h2 id="查看服务日志命令">查看服务日志命令</h2>
<pre><code>journalctl -u prometheus
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kafka实现sqlserver的CDC数据实时变更]]></title>
        <id>https://oldcamel.run/post/kafka-shi-xian-sqlserver-de-cdc-shu-ju-shi-shi-bian-geng/</id>
        <link href="https://oldcamel.run/post/kafka-shi-xian-sqlserver-de-cdc-shu-ju-shi-shi-bian-geng/">
        </link>
        <updated>2021-01-08T02:29:39.000Z</updated>
        <content type="html"><![CDATA[<h2 id="安装sqlserver">安装sqlserver</h2>
<pre><code>#!/bin/bash -e
# Password for the SA user (required)
# 设置密码
MSSQL_SA_PASSWORD='test@12345678'

# Product ID of the version of SQL server you're installing
# Must be evaluation, developer, express, web, standard, enterprise, or your 25 digit product key
# Defaults to developer
# 选择版本，有多种版本可供原则，具体版本标识符可自行百度
MSSQL_PID='evaluation'

# Install SQL Server Agent (recommended)
SQL_ENABLE_AGENT='y'

# Install SQL Server Full Text Search (optional)
# SQL_INSTALL_FULLTEXT='y'

# Create an additional user with sysadmin privileges (optional)
# 新建一个额外添加的用户，（可选）
# SQL_INSTALL_USER='&lt;Username&gt;'
# SQL_INSTALL_USER_PASSWORD='&lt;YourStrong!Passw0rd&gt;'

if [ -z $MSSQL_SA_PASSWORD ]
then
  echo Environment variable MSSQL_SA_PASSWORD must be set for unattended install
  exit 1
fi

# --------------------------- 远程拉包 并安装的过程
echo Adding Microsoft repositories...
sudo curl -o /etc/yum.repos.d/mssql-server.repo https://packages.microsoft.com/config/rhel/7/mssql-server-2017.repo
sudo curl -o /etc/yum.repos.d/msprod.repo https://packages.microsoft.com/config/rhel/7/prod.repo

echo Installing SQL Server...
sudo yum install -y mssql-server

# --------------------------- 下面给出 本地离线rpm包安装部分脚本
echo ：Local Installing SQL Server...
sudo yum localinstall -y ./sqlserver2017.rpm



# 执行sqlserver的配置
echo Running mssql-conf setup...
sudo MSSQL_SA_PASSWORD=$MSSQL_SA_PASSWORD \
     MSSQL_PID=$MSSQL_PID \
     /opt/mssql/bin/mssql-conf -n setup accept-eula

echo Installing mssql-tools and unixODBC developer...
sudo ACCEPT_EULA=Y yum install -y mssql-tools unixODBC-devel

# Add SQL Server tools to the path by default:
echo Adding SQL Server tools to your path...
echo PATH=&quot;$PATH:/opt/mssql-tools/bin&quot; &gt;&gt; ~/.bash_profile
echo 'export PATH=&quot;$PATH:/opt/mssql-tools/bin&quot;' &gt;&gt; ~/.bashrc
source ~/.bashrc

# Optional Enable SQL Server Agent :
if [ ! -z $SQL_ENABLE_AGENT ]
then
  echo Enable SQL Server Agent...
  sudo /opt/mssql/bin/mssql-conf set sqlagent.enabled true
  sudo systemctl restart mssql-server
fi

# Optional SQL Server Full Text Search installation:
if [ ! -z $SQL_INSTALL_FULLTEXT ]
then
    echo Installing SQL Server Full-Text Search...
    sudo yum install -y mssql-server-fts
fi

# Configure firewall to allow TCP port 1433:
# 配置防火墙放行1433端口，懒省事直接关掉防火墙亦可
echo Configuring firewall to allow traffic on port 1433...
sudo firewall-cmd --zone=public --add-port=1433/tcp --permanent
sudo firewall-cmd --reload

# Example of setting post-installation configuration options
# Set trace flags 1204 and 1222 for deadlock tracing:
#echo Setting trace flags...
#sudo /opt/mssql/bin/mssql-conf traceflag 1204 1222 on

# Restart SQL Server after making configuration changes:
# 重启
echo Restarting SQL Server...
sudo systemctl restart mssql-server

# Connect to server and get the version:
# 官方给出的测试链接脚本
counter=1
errstatus=1
while [ $counter -le 5 ] &amp;&amp; [ $errstatus = 1 ]
do
  echo Waiting for SQL Server to start...
  sleep 5s
  /opt/mssql-tools/bin/sqlcmd \
    -S localhost \
    -U SA \
    -P $MSSQL_SA_PASSWORD \
    -Q &quot;SELECT @@VERSION&quot; 2&gt;/dev/null
  errstatus=$?
  ((counter++))
done

# Display error if connection failed:
if [ $errstatus = 1 ]
then
  echo Cannot connect to SQL Server, installation aborted
  exit $errstatus
fi

# Optional new user creation:
if [ ! -z $SQL_INSTALL_USER ] &amp;&amp; [ ! -z $SQL_INSTALL_USER_PASSWORD ]
then
  echo Creating user $SQL_INSTALL_USER
  /opt/mssql-tools/bin/sqlcmd \
    -S localhost \
    -U SA \
    -P $MSSQL_SA_PASSWORD \
    -Q &quot;CREATE LOGIN [$SQL_INSTALL_USER] WITH PASSWORD=N'$SQL_INSTALL_USER_PASSWORD', DEFAULT_DATABASE=[master], CHECK_EXPIRATION=ON, CHECK_POLICY=ON; ALTER SERVER ROLE [sysadmin] ADD MEMBER [$SQL_INSTALL_USER]&quot;
fi

echo Done!
</code></pre>
<p>运行脚本等着安装完成</p>
<h2 id="开启表的cdc">开启表的cdc</h2>
<h3 id="创建shihu数据库">创建shihu数据库</h3>
<h3 id="建测试表">建测试表</h3>
<pre><code>IF EXISTS (SELECT * FROM sys.all_objects WHERE object_id = OBJECT_ID(N'[dbo].[dt_test]') AND type IN ('U'))
	DROP TABLE [dbo].[dt_test]
GO

CREATE TABLE [dbo].[dt_test] (
  [tid] int  NOT NULL,
  [tname] nvarchar(200) COLLATE SQL_Latin1_General_CP1_CI_AS  NULL,
  [tidcard] nvarchar(200) COLLATE SQL_Latin1_General_CP1_CI_AS  NULL,
  [tbirthday] date  NULL,
  [tmobile] nvarchar(200) COLLATE SQL_Latin1_General_CP1_CI_AS  NULL,
  [temail] nvarchar(200) COLLATE SQL_Latin1_General_CP1_CI_AS  NULL,
  [tgender] bigint  NULL,
  [tcreate_time] datetime  NULL
)
GO

ALTER TABLE [dbo].[dt_test] SET (LOCK_ESCALATION = TABLE)
GO


-- ----------------------------
-- Primary Key structure for table dt_test
-- ----------------------------
ALTER TABLE [dbo].[dt_test] ADD CONSTRAINT [PK__dt_test__DC105B0FA908205A] PRIMARY KEY CLUSTERED ([tid])
WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON)  
ON [PRIMARY]
GO

</code></pre>
<h3 id="开启cdc">开启cdc</h3>
<pre><code>USE shihu
GO
EXEC sys.sp_cdc_enable_db
GO

USE shihu
GO
EXEC sys.sp_cdc_enable_table
@source_schema = 'dbo',
@source_name   = ‘dt_test’,     
@role_name     = NULL,
@filegroup_name = 'PRIMARY',      
@supports_net_changes = 1
GO
</code></pre>
<h2 id="下载kafka插件">下载kafka插件</h2>
<h3 id="官方文档">官方文档</h3>
<p><a href="https://debezium.io/documentation/reference/1.3/connectors/sqlserver.html">https://debezium.io/documentation/reference/1.3/connectors/sqlserver.html</a></p>
<h3 id="下载">下载</h3>
<p><a href="https://repo1.maven.org/maven2/io/debezium/debezium-connector-sqlserver/1.2.5.Final/debezium-connector-sqlserver-1.2.5.Final-plugin.tar.gz">https://repo1.maven.org/maven2/io/debezium/debezium-connector-sqlserver/1.2.5.Final/debezium-connector-sqlserver-1.2.5.Final-plugin.tar.gz</a></p>
<h3 id="创建插件目录">创建插件目录</h3>
<p>在kafka跟目录创建 kafka_connect_plugins 目录<br>
将下载的包解压到此目录中<br>
<img src="https://oldcamel.run/post-images/1610074594464.png" alt="" loading="lazy"></p>
<h3 id="修改kafkaconfigconnect-distributeproperties">修改kafka/config/connect-distribute.properties</h3>
<p>bootstrap.servers=IP:9092<br>
plugin.path= /kafka路径/kafka_connect_plugins</p>
<h3 id="启动kafka-connect">启动kafka-connect</h3>
<pre><code>./bin/connect-distributed.sh  config/connect-distributed.properties &gt;logs/ksc.log &amp;
</code></pre>
<p>开启成功的标志,postman可以使用访问端口8083（默认的端口号,可以在connect-distributed.properties 更改）</p>
<h4 id="rest-api">rest api</h4>
<pre><code>GET /connectors – 返回所有正在运行的connector名
POST /connectors – 新建一个connector; 请求体必须是json格式并且需要包含name字段和config字段，name是connector的名字，config是json格式，必须包含你的connector的配置信息。
GET /connectors/{name} – 获取指定connetor的信息
GET /connectors/{name}/config – 获取指定connector的配置信息
PUT /connectors/{name}/config – 更新指定connector的配置信息
GET /connectors/{name}/status – 获取指定connector的状态，包括它是否在运行、停止、或者失败，如果发生错误，还会列出错误的具体信息。
GET /connectors/{name}/tasks – 获取指定connector正在运行的task。
GET /connectors/{name}/tasks/{taskid}/status – 获取指定connector的task的状态信息
PUT /connectors/{name}/pause – 暂停connector和它的task，停止数据处理知道它被恢复。
PUT /connectors/{name}/resume – 恢复一个被暂停的connector
POST /connectors/{name}/restart – 重启一个connector，尤其是在一个connector运行失败的情况下比较常用
POST /connectors/{name}/tasks/{taskId}/restart – 重启一个task，一般是因为它运行失败才这样做。
DELETE /connectors/{name} – 删除一个connector，停止它的所有task并删除配置。
</code></pre>
<h4 id="添加sqlserver-connect">添加sqlserver connect</h4>
<pre><code>curl --location --request POST 'http://localhost:8083/connectors' \
--header 'Content-Type: application/json' \
--data-raw '{
    &quot;name&quot;: &quot;sqlserver178-connector&quot;,
    &quot;config&quot;: {
        &quot;connector.class&quot;: &quot;io.debezium.connector.sqlserver.SqlServerConnector&quot;,
        &quot;database.hostname&quot;: &quot;sqlserver的ip&quot;,
        &quot;database.port&quot;: &quot;1433&quot;,
        &quot;database.user&quot;: &quot;sa&quot;,
        &quot;database.password&quot;: &quot;test@12345678&quot;,
        &quot;database.dbname&quot;: &quot;shihu&quot;,
        &quot;database.server.name&quot;: &quot;fullfillment&quot;,
        &quot;table.whitelist&quot;: &quot;dbo.dt_test&quot;,
        &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka的IP:9092&quot;,
        &quot;database.history.kafka.topic&quot;: &quot;dbhistory.fullfillment&quot;,
        &quot;database.history.producer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;,
        &quot;database.history.producer.sasl.mechanism&quot;: &quot;PLAIN&quot;,
        &quot;database.history.producer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;admin\&quot; password=\&quot;123\&quot;;&quot;,
        &quot;database.history.consumer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;,
        &quot;database.history.consumer.sasl.mechanism&quot;: &quot;PLAIN&quot;,
        &quot;database.history.consumer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;admin\&quot; password=\&quot;123\&quot;;&quot;
    }
}'
</code></pre>
<h3 id="消费信息">消费信息</h3>
<h4 id="consumerpreperties配置">consumer.preperties配置</h4>
<pre><code>bootstrap.servers=localhost:9092
group.id=test-consumer-group
security.protocol=SASL_PLAINTEXT
sasl.mechanism=PLAIN
</code></pre>
<h4 id="修改器kafka-console-consumersh为kafka-console-consumer-saalsh倒数第二行添加">修改器kafka-console-consumer.sh为kafka-console-consumer-saal.sh倒数第二行添加</h4>
<pre><code>export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/kafka_2.13-2.6.0/config/kafka_client_jaas.conf&quot;
</code></pre>
<h4 id="启动">启动</h4>
<pre><code>./bin/kafka-console-consumer-saal.sh --bootstrap-server localhost:9092 --topic fullfillment.dbo.dt_test --consumer.config config/consumer.properties
</code></pre>
<h4 id="消息示例">消息示例</h4>
<pre><code>{
    &quot;schema&quot;:{
        &quot;type&quot;:&quot;struct&quot;,
        &quot;fields&quot;:Array[6],
        &quot;optional&quot;:false,
        &quot;name&quot;:&quot;fullfillment.dbo.dt_test.Envelope&quot;
    },
    &quot;payload&quot;:{
        &quot;before&quot;:{
            &quot;tid&quot;:1,
            &quot;tname&quot;:&quot;222&quot;,
            &quot;tidcard&quot;:&quot;2&quot;,
            &quot;tbirthday&quot;:-25567,
            &quot;tmobile&quot;:&quot;1&quot;,
            &quot;temail&quot;:&quot;1&quot;,
            &quot;tgender&quot;:2,
            &quot;tcreate_time&quot;:-2208988800000
        },
        &quot;after&quot;:{
            &quot;tid&quot;:1,
            &quot;tname&quot;:&quot;test&quot;,
            &quot;tidcard&quot;:&quot;2&quot;,
            &quot;tbirthday&quot;:-25567,
            &quot;tmobile&quot;:&quot;1&quot;,
            &quot;temail&quot;:&quot;1&quot;,
            &quot;tgender&quot;:2,
            &quot;tcreate_time&quot;:-2208988800000
        },
        &quot;source&quot;:Object{...},
        &quot;op&quot;:&quot;u&quot;,
        &quot;ts_ms&quot;:1610019731750,
        &quot;transaction&quot;:null
    }
}
</code></pre>
<h4 id="oracle示例">oracle示例</h4>
<pre><code>
curl --location --request POST 'http://localhost:8083/connectors/' \
--header 'Content-Type: application/json' \
--data-raw '
{
    &quot;name&quot;: &quot;oracle-61-connector&quot;,
    &quot;config&quot;: {
        &quot;connector.class&quot;: &quot;com.ecer.kafka.connect.oracle.OracleSourceConnector&quot;,
        &quot;db.name.alias&quot;: &quot;61test&quot;,
        &quot;tasks.max&quot;: 1,
        &quot;topic&quot;: &quot;kol&quot;,
        &quot;db.name&quot;: &quot;portaldb&quot;,
        &quot;db.hostname&quot;: &quot;oralce连接host地址&quot;,
        &quot;db.port&quot;: 1521,
        &quot;db.user&quot;: &quot;info&quot;,
        &quot;db.user.password&quot;: &quot;111111&quot;,
        &quot;db.fetch.size&quot;: 1,
        &quot;table.whitelist&quot;: &quot;INFO.*,SPAUTH.*,WORKFLOW.*,FLOWABLE.*,OA.*&quot;,
        &quot;table.blacklist&quot;: &quot;&quot;,
        &quot;parse.dml.data&quot;: true,
        &quot;reset.offset&quot;: false,
        &quot;multitenant&quot;: false
    }
}
</code></pre>
<h3 id="mysql示例">mysql示例</h3>
<pre><code>curl --location --request POST 'http://localhost:8083/connectors/' \
--header 'Content-Type: application/json' \
--data-raw '
{
  &quot;name&quot;: &quot;140mysql-connector&quot;, 
  &quot;config&quot;: {
    &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;, 
    &quot;database.hostname&quot;: &quot;localhost&quot;, 
    &quot;database.port&quot;: &quot;3306&quot;, 
    &quot;database.user&quot;: &quot;root&quot;, 
    &quot;database.password&quot;: &quot;111111&quot;, 
    &quot;database.server.id&quot;: &quot;184054&quot;, 
    &quot;database.server.name&quot;: &quot;fullfillment140ms&quot;, 
    &quot;database.whitelist&quot;:&quot;test&quot;,
    &quot;database.history.kafka.bootstrap.servers&quot;: &quot;localhost:9092&quot;, 
    &quot;database.history.kafka.topic&quot;: &quot;dbhistory.fullfillment140ms&quot;, 
    &quot;include.schema.changes&quot;: &quot;false&quot;,
        &quot;database.history.producer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;,
        &quot;database.history.producer.sasl.mechanism&quot;: &quot;PLAIN&quot;,
        &quot;database.history.producer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;admin\&quot; password=\&quot;123\&quot;;&quot;,
        &quot;database.history.consumer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;,
        &quot;database.history.consumer.sasl.mechanism&quot;: &quot;PLAIN&quot;,
        &quot;database.history.consumer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;admin\&quot; password=\&quot;123\&quot;;&quot;
    }
}
</code></pre>
<h3 id="查看topic-list">查看topic list</h3>
<pre><code>bin/kafka-topics.sh --zookeeper localhost --list
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kafka开启SASL用户名密码认证]]></title>
        <id>https://oldcamel.run/post/kafka-kai-qi-sasl-yong-hu-ming-mi-ma-ren-zheng/</id>
        <link href="https://oldcamel.run/post/kafka-kai-qi-sasl-yong-hu-ming-mi-ma-ren-zheng/">
        </link>
        <updated>2021-01-08T01:51:25.000Z</updated>
        <content type="html"><![CDATA[<h2 id="创建kafka_server_jaasconf文件">创建kafka_server_jaas.conf文件</h2>
<p>config目录下创建kafka_server_jaas.conf文件</p>
<pre><code>KafkaServer {
  org.apache.kafka.common.security.plain.PlainLoginModule required
  username=&quot;admin&quot;
  password=&quot;123&quot;
  user_admin=&quot;123&quot;
  user_yunzai=&quot;123&quot;;
};
</code></pre>
<h2 id="创建kafka_client_jaasconf文件">创建kafka_client_jaas.conf文件</h2>
<p>config目录下创建kafka_client_jaas.conf文件</p>
<pre><code>KafkaClient {
        org.apache.kafka.common.security.plain.PlainLoginModule required
        username=&quot;admin&quot;
        password=&quot;123&quot;;
};
</code></pre>
<h2 id="修改serverproperties">修改server.properties</h2>
<pre><code>listeners=SASL_PLAINTEXT://IP:9092
advertised.listeners=SASL_PLAINTEXT://IP:9092
security.inter.broker.protocol=SASL_PLAINTEXT
sasl.enabled.mechanisms=PLAIN
sasl.mechanism.inter.broker.protocol=PLAIN
authorizer.class.name = kafka.security.auth.SimpleAclAuthorizer
super.users=User:admin;
</code></pre>
<h2 id="修改kafaka启动脚本">修改kafaka启动脚本</h2>
<p>修改bin目录下kafka_start.sh<br>
在倒数第二行添加kafka_server_jaas.conf的全路径</p>
<pre><code>export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/kafka_2.13-2.6.0/config/kafka_server_jaas.conf&quot;
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://oldcamel.run/post-images/1610071605941.png" alt="" loading="lazy"></figure>
<h2 id="可选为特定用户添加特定topic的acl授权">（可选）为特定用户添加特定topic的acl授权</h2>
<p>如下表示为用户yunzai添加topic nginx-log的读写权限。</p>
<pre><code>./bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 -add --allow-principal User:yunzai  --operation Read --operation Write --topic testyunzai
</code></pre>
<p>验证授权</p>
<pre><code>./bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --list --topic testyunzai
</code></pre>
<h3 id="生产者消费者验证">生产者消费者验证</h3>
<h4 id="生产者">生产者</h4>
<p>1.以用户yunzai为例，创建JAAS认证文件yunzai_jaas.conf放在config目录下，内容如下:</p>
<pre><code>KafkaClient {
org.apache.kafka.common.security.plain.PlainLoginModule required
username=&quot;yunzai&quot;
password=&quot;123&quot;;
};
</code></pre>
<p>2.拷贝bin/kafka-console-producer.sh为bin/yunzai-kafka-console-producer.sh，并将JAAS文件作为一个JVM参数传给console producer</p>
<p>倒数第二行添加</p>
<pre><code>export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/kafka_2.13-2.6.0/config/yunzai_jaas.conf&quot;
</code></pre>
<p>3.创建文件producer.config指定如下属性：</p>
<pre><code>security.protocol=SASL_PLAINTEXT
sasl.mechanism=PLAIN
</code></pre>
<p>4.启动producer</p>
<pre><code>./bin/yunzai-kafka-console-producer.sh --broker-list IP:9092 --topic testyunzai --producer.config producer.config
</code></pre>
<h4 id="消费者">消费者</h4>
<p>1.以用户yunzai为例，创建JAAS认证文件yunzai_jaas.conf放在config目录下，如果用户跟生产者是同一个，可以复用上面生产者的JAAS文件，内容如下:</p>
<pre><code>KafkaClient {
org.apache.kafka.common.security.plain.PlainLoginModule required
username=&quot;yunzai&quot;
password=&quot;123&quot;;
};
</code></pre>
<p>2.拷贝bin/kafka-console-consumer.sh为bin/yunzai-kafka-console-consumer.sh，并将JAAS文件作为一个JVM参数传给console consumer<br>
倒数第二行添加</p>
<pre><code>export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/kafka_2.13-2.6.0/config/yunzai_jaas.conf&quot;
</code></pre>
<p>3.创建文件consumer.config指定如下属性：</p>
<pre><code>security.protocol=SASL_PLAINTEXT
sasl.mechanism=PLAIN
group.id=test
</code></pre>
<p>4.启动consumer</p>
<pre><code>./bin/yunzai-kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic testyunzai --from-beginning --consumer.config consumer.config
</code></pre>
<h3 id="java-beam客户端认证">java beam客户端认证</h3>
<p>beam设置</p>
<pre><code class="language-java">   final Map&lt;String, Object&gt; immutableMap = new ImmutableMap.Builder&lt;String, Object&gt;()
                .put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;)
                .put(ConsumerConfig.GROUP_ID_CONFIG, options.getGroupid())
                .put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true)
                .put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, &quot;SASL_PLAINTEXT&quot;)
                .put(SaslConfigs.SASL_MECHANISM, &quot;PLAIN&quot;)
                .put(SaslConfigs.SASL_JAAS_CONFIG, &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;&quot; + options.getKafkaUsername() + &quot;\&quot; password=\&quot;&quot; + options.getKafkaPassword() + &quot;\&quot;;&quot;)
                .build();
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[VictoriaMetrics 汇总多个Prometheus指标（四）读取Istio指标]]></title>
        <id>https://oldcamel.run/post/victoriametrics-hui-zong-duo-ge-prometheus-zhi-biao-san-du-qu-istio-zhi-biao/</id>
        <link href="https://oldcamel.run/post/victoriametrics-hui-zong-duo-ge-prometheus-zhi-biao-san-du-qu-istio-zhi-biao/">
        </link>
        <updated>2020-12-25T07:41:53.000Z</updated>
        <content type="html"><![CDATA[<p>istio中默认带了prometheus,里面包含istio代理的指标，现需要将istio中的指标配置在kube-prometheus中。</p>
<h2 id="修改prometheus-k8s-角色权限">修改prometheus-k8s 角色权限</h2>
<p>修改prometheus-clusterRole.yaml</p>
<pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus-k8s
rules:
- apiGroups:
  - &quot;&quot;
  resources:
  - nodes
  - services
  - endpoints
  - pods
  - nodes/proxy
  verbs:
  - get
  - watch
  - list
- apiGroups:
  - &quot;&quot;
  resources:
  - configmaps
  verbs:
  - get
- nonResourceURLs:
  - /metrics
  verbs:
  - get
</code></pre>
<h2 id="配置数据平面的服务监控">配置数据平面的服务监控</h2>
<pre><code>apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: prometheus-oper-istio-controlplane
spec:
  jobLabel: istio
  selector:
    matchExpressions:
      - {key: istio, operator: In, values: [mixer,pilot,galley,citadel,sidecar-injector]}
  namespaceSelector:
    any: true
  endpoints:
  - port: http-monitoring
    interval: 15s
  - port: http-policy-monitoring
    interval: 15s

</code></pre>
<h2 id="配置数据层的服务监控">配置数据层的服务监控</h2>
<pre><code>apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: prometheus-oper-istio-dataplane
  labels:
    monitoring: istio-dataplane
spec:
  selector:
    matchExpressions:
      - {key: istio-prometheus-ignore, operator: DoesNotExist}
  namespaceSelector:
    any: true
  jobLabel: envoy-stats
  endpoints:
  - path: /stats/prometheus
    targetPort: http-envoy-prom
    interval: 15s


</code></pre>
<h2 id="查看是否生效">查看是否生效</h2>
<figure data-type="image" tabindex="1"><img src="https://oldcamel.run/post-images/1608882536864.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="2"><img src="https://oldcamel.run/post-images/1608882571184.png" alt="" loading="lazy"></figure>
<h2 id="victoriametrics-中不同集群中的指标获取">VictoriaMetrics 中不同集群中的指标获取</h2>
<figure data-type="image" tabindex="3"><img src="https://oldcamel.run/post-images/1608882968056.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[VictoriaMetrics 汇总多个Prometheus指标（三）kube-prometheus安装]]></title>
        <id>https://oldcamel.run/post/victoriametrics-hui-zong-duo-ge-prometheus-zhi-biao-san-kube-prometheus-an-zhuang/</id>
        <link href="https://oldcamel.run/post/victoriametrics-hui-zong-duo-ge-prometheus-zhi-biao-san-kube-prometheus-an-zhuang/">
        </link>
        <updated>2020-12-25T07:32:20.000Z</updated>
        <content type="html"><![CDATA[<p>kube-prometheus是Prometheus Operator 针对kubernetes的监控组件。<br>
github地址<br>
<a href="https://github.com/prometheus-operator/kube-prometheus">https://github.com/prometheus-operator/kube-prometheus</a></p>
<h2 id="配置远程写入">配置远程写入</h2>
<h3 id="basic用户名密码配置-secret">basic用户名密码配置 secret</h3>
<pre><code>apiVersion: v1
kind: Secret
metadata:
  name: vmuser
  namespace: monitoring
type: kubernetes.io/basic-auth
stringData:
  username: username
  password: password
</code></pre>
<h3 id="添加远程写入-和外部标签">添加远程写入 和外部标签</h3>
<p>修改prometheus-prometheus.yaml文件，添加</p>
<pre><code>prometheusExternalLabelName: &quot;&quot;
  replicaExternalLabelName: &quot;&quot;
  externalLabels:
    tenant: &quot;租户名称&quot;
  remoteWrite:
    - url: http://VictoriaMetrics连接地址/api/v1/write
      basicAuth:
       username:
          key: username
          name: vmuser
       password:
          key: password
          name: vmuser
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[VictoriaMetrics 汇总多个Prometheus指标（二）Grafana安装]]></title>
        <id>https://oldcamel.run/post/grafana-an-zhuang/</id>
        <link href="https://oldcamel.run/post/grafana-an-zhuang/">
        </link>
        <updated>2020-12-25T06:50:09.000Z</updated>
        <content type="html"><![CDATA[<p>安装grafana 从victoriametrics中读取汇总的指标</p>
<h2 id="安装mysql">安装mysql</h2>
<h3 id="下载离线包">下载离线包</h3>
<p><a href="https://dev.mysql.com/downloads/mysql/5.7.html#downloads">下载地址</a><br>
<img src="https://oldcamel.run/post-images/1608879483467.png" alt="" loading="lazy"></p>
<h3 id="解压安装">解压安装</h3>
<figure data-type="image" tabindex="1"><img src="https://oldcamel.run/post-images/1608879544128.png" alt="" loading="lazy"></figure>
<pre><code>rpm -Uvh *.rpm --nodeps --force
</code></pre>
<h3 id="修改配置文件">修改配置文件</h3>
<p>修改 /etc/my.cnf 添加</p>
<pre><code>lower_case_table_names=1
default-storage-engine=INNODB
</code></pre>
<h3 id="生成用户不带密码">生成用户不带密码</h3>
<pre><code>mysqld --initialize-insecure
</code></pre>
<p>如果要重新初始化，必须先清空data文件夹</p>
<h3 id="启动mysql">启动mysql</h3>
<pre><code>systemctl start mysqld
</code></pre>
<h3 id="进入mysql">进入mysql</h3>
<pre><code>mysql -u root
</code></pre>
<h3 id="修改密码开启远程访问">修改密码+开启远程访问</h3>
<pre><code>set password for root@localhost = password('123456');
GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456' WITH GRANT OPTION;
FLUSH PRIVILEGES;
</code></pre>
<h3 id="创建grafana数据库">创建grafana数据库</h3>
<pre><code>CREATE DATABASE  `grafana` DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
</code></pre>
<h2 id="安装grafana">安装grafana</h2>
<h3 id="下载安装">下载安装</h3>
<p><a href="https://grafana.com/grafana/download">参考文档</a></p>
<pre><code>wget https://dl.grafana.com/oss/release/grafana-7.3.6-1.x86_64.rpm
sudo yum install grafana-7.3.6-1.x86_64.rpm
</code></pre>
<h3 id="修改配置文件-2">修改配置文件</h3>
<pre><code class="language-shell">vim /etc/grafana/grafana.ini
</code></pre>
<h4 id="domain修改">domain修改</h4>
<pre><code>domain = 修改成主机IP
</code></pre>
<h4 id="默认用户名密码修改">默认用户名密码修改</h4>
<pre><code>admin_user = admin
admin_password = 123456
</code></pre>
<h4 id="数据库修改">数据库修改</h4>
<pre><code>type = mysql
host = 127.0.0.1:3306
name = grafana
user = root
password =123456

</code></pre>
<h3 id="启动grafana服务">启动grafana服务</h3>
<pre><code>service grafana-server start
</code></pre>
<h3 id="配置-victoriametrics连接">配置 victoriametrics连接</h3>
<p>访问 ip:3000<br>
添加prometheus数据源 设置为 victoriametrics主机ip  端口 8428<br>
<img src="https://oldcamel.run/post-images/1608881141442.png" alt="" loading="lazy"></p>
<p>配置victoriametrics官方的监控图表<br>
<a href="https://grafana.com/grafana/dashboards/10229">grafana lab地址</a><br>
<img src="https://oldcamel.run/post-images/1608881288591.png" alt="" loading="lazy"></p>
]]></content>
    </entry>
</feed>