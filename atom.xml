<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://oldcamel.run</id>
    <title>Old Camel</title>
    <updated>2021-03-11T01:15:10.548Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://oldcamel.run"/>
    <link rel="self" href="https://oldcamel.run/atom.xml"/>
    <subtitle>日常开发记录 
&lt;div&gt;&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=330 height=86 src=&quot;//music.163.com/outchain/player?type=2&amp;id=18611643&amp;auto=1&amp;height=66&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;</subtitle>
    <logo>https://oldcamel.run/images/avatar.png</logo>
    <icon>https://oldcamel.run/favicon.ico</icon>
    <rights>All rights reserved 2021, Old Camel</rights>
    <entry>
        <title type="html"><![CDATA[linux 安装服务器资源监控 prometheus远程写入]]></title>
        <id>https://oldcamel.run/post/linux-an-zhuang-fu-wu-qi-zi-yuan-jian-kong-prometheus-yuan-cheng-xie-ru/</id>
        <link href="https://oldcamel.run/post/linux-an-zhuang-fu-wu-qi-zi-yuan-jian-kong-prometheus-yuan-cheng-xie-ru/">
        </link>
        <updated>2021-03-11T01:07:36.000Z</updated>
        <content type="html"><![CDATA[<h2 id="1安装prometheus">1.安装prometheus</h2>
<pre><code>wget -c https://github.com/prometheus/prometheus/releases/download/v2.23.0/prometheus-2.23.0.linux-amd64.tar.gz 
tar zxvf prometheus-2.23.0.linux-amd64.tar.gz  -C /opt/ 
cd /opt/ 
ln -s prometheus-2.23.0.linux-amd64 prometheus 
cat &gt; /etc/systemd/system/prometheus.service &lt;&lt;EOF 
[Unit] 
Description=prometheus 
After=network.target 
[Service] 
Type=simple 
WorkingDirectory=/opt/prometheus 
ExecStart=/opt/prometheus/prometheus --config.file=/opt/prometheus/prometheus.yml
LimitNOFILE=65536 
PrivateTmp=true 
RestartSec=2 
StartLimitInterval=0 
Restart=always 
[Install] 
WantedBy=multi-user.target 
EOF 
systemctl daemon-reload  
systemctl enable prometheus 
systemctl start prometheus 

</code></pre>
<h3 id="2配置prometheus">2.配置Prometheus</h3>
<pre><code>cat &gt; /opt/prometheus/prometheus.yml &lt;&lt;EOF 
# my global config 
global: 
  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. 
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. 
  # scrape_timeout is set to the global default (10s). 
  external_labels:
        tenant: &quot;140test&quot;
remote_write:
  - url: &quot;url&quot;
    basic_auth:
      username: 用户名
      password: 密码
# Alertmanager configuration 
alerting: 
  alertmanagers: 
  - static_configs: 
    - targets: 
      # - alertmanager:9093 
# Load rules once and periodically evaluate them according to the global 'evaluation_interval'. 
rule_files: 
  # - &quot;first_rules.yml&quot; 
  # - &quot;second_rules.yml&quot; 
# A scrape configuration containing exactly one endpoint to scrape: 
# Here it's Prometheus itself. 
scrape_configs: 
  # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. 
  - job_name: 'servers' 
    file_sd_configs: 
    - refresh_interval: 61s 
      files: 
        - /opt/prometheus/servers/*.json 
EOF 

</code></pre>
<h2 id="3创建node配置文件">3.创建node配置文件</h2>
<pre><code>mkdir -p /opt/prometheus/servers
cd /opt/prometheus/servers
vim json.json
[     
    { 
        &quot;targets&quot;: [ 
            “xxx.xxx.xxx.xxx:9100&quot; 
        ], 
        &quot;labels&quot;: { 
            &quot;instance&quot;: &quot;xxx.xxx.xxx.xxx&quot;, 
            &quot;job&quot;: &quot;node_exporter&quot; 
        } 
    }
   
] 
</code></pre>
<h2 id="安装node_exporter">安装node_exporter</h2>
<pre><code>Wget https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-amd64.tar.gz 
tar zxvf node_exporter-1.0.1.linux-amd64.tar.gz -C /opt/ 
cd /opt/ 
ln -s  node_exporter-1.0.1.linux-amd64 node_exporter 
cat &gt; /etc/systemd/system/node_exporter.service &lt;&lt;EOF 
[Unit] 
Description=node_exporter 
After=network.target 
[Service] 
Type=simple 
WorkingDirectory=/opt/node_exporter 
ExecStart=/opt/node_exporter/node_exporter 
LimitNOFILE=65536 
PrivateTmp=true 
RestartSec=2 
StartLimitInterval=0 
Restart=always 
[Install] 
WantedBy=multi-user.target 
EOF 
systemctl daemon-reload 
systemctl enable node_exporter 
systemctl start node_exporter 
systemctl restart prometheus 

</code></pre>
<h2 id="查看服务日志命令">查看服务日志命令</h2>
<pre><code>journalctl -u prometheus
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kafka实现sqlserver的CDC数据实时变更]]></title>
        <id>https://oldcamel.run/post/kafka-shi-xian-sqlserver-de-cdc-shu-ju-shi-shi-bian-geng/</id>
        <link href="https://oldcamel.run/post/kafka-shi-xian-sqlserver-de-cdc-shu-ju-shi-shi-bian-geng/">
        </link>
        <updated>2021-01-08T02:29:39.000Z</updated>
        <content type="html"><![CDATA[<h2 id="安装sqlserver">安装sqlserver</h2>
<pre><code>#!/bin/bash -e
# Password for the SA user (required)
# 设置密码
MSSQL_SA_PASSWORD='test@12345678'

# Product ID of the version of SQL server you're installing
# Must be evaluation, developer, express, web, standard, enterprise, or your 25 digit product key
# Defaults to developer
# 选择版本，有多种版本可供原则，具体版本标识符可自行百度
MSSQL_PID='evaluation'

# Install SQL Server Agent (recommended)
SQL_ENABLE_AGENT='y'

# Install SQL Server Full Text Search (optional)
# SQL_INSTALL_FULLTEXT='y'

# Create an additional user with sysadmin privileges (optional)
# 新建一个额外添加的用户，（可选）
# SQL_INSTALL_USER='&lt;Username&gt;'
# SQL_INSTALL_USER_PASSWORD='&lt;YourStrong!Passw0rd&gt;'

if [ -z $MSSQL_SA_PASSWORD ]
then
  echo Environment variable MSSQL_SA_PASSWORD must be set for unattended install
  exit 1
fi

# --------------------------- 远程拉包 并安装的过程
echo Adding Microsoft repositories...
sudo curl -o /etc/yum.repos.d/mssql-server.repo https://packages.microsoft.com/config/rhel/7/mssql-server-2017.repo
sudo curl -o /etc/yum.repos.d/msprod.repo https://packages.microsoft.com/config/rhel/7/prod.repo

echo Installing SQL Server...
sudo yum install -y mssql-server

# --------------------------- 下面给出 本地离线rpm包安装部分脚本
echo ：Local Installing SQL Server...
sudo yum localinstall -y ./sqlserver2017.rpm



# 执行sqlserver的配置
echo Running mssql-conf setup...
sudo MSSQL_SA_PASSWORD=$MSSQL_SA_PASSWORD \
     MSSQL_PID=$MSSQL_PID \
     /opt/mssql/bin/mssql-conf -n setup accept-eula

echo Installing mssql-tools and unixODBC developer...
sudo ACCEPT_EULA=Y yum install -y mssql-tools unixODBC-devel

# Add SQL Server tools to the path by default:
echo Adding SQL Server tools to your path...
echo PATH=&quot;$PATH:/opt/mssql-tools/bin&quot; &gt;&gt; ~/.bash_profile
echo 'export PATH=&quot;$PATH:/opt/mssql-tools/bin&quot;' &gt;&gt; ~/.bashrc
source ~/.bashrc

# Optional Enable SQL Server Agent :
if [ ! -z $SQL_ENABLE_AGENT ]
then
  echo Enable SQL Server Agent...
  sudo /opt/mssql/bin/mssql-conf set sqlagent.enabled true
  sudo systemctl restart mssql-server
fi

# Optional SQL Server Full Text Search installation:
if [ ! -z $SQL_INSTALL_FULLTEXT ]
then
    echo Installing SQL Server Full-Text Search...
    sudo yum install -y mssql-server-fts
fi

# Configure firewall to allow TCP port 1433:
# 配置防火墙放行1433端口，懒省事直接关掉防火墙亦可
echo Configuring firewall to allow traffic on port 1433...
sudo firewall-cmd --zone=public --add-port=1433/tcp --permanent
sudo firewall-cmd --reload

# Example of setting post-installation configuration options
# Set trace flags 1204 and 1222 for deadlock tracing:
#echo Setting trace flags...
#sudo /opt/mssql/bin/mssql-conf traceflag 1204 1222 on

# Restart SQL Server after making configuration changes:
# 重启
echo Restarting SQL Server...
sudo systemctl restart mssql-server

# Connect to server and get the version:
# 官方给出的测试链接脚本
counter=1
errstatus=1
while [ $counter -le 5 ] &amp;&amp; [ $errstatus = 1 ]
do
  echo Waiting for SQL Server to start...
  sleep 5s
  /opt/mssql-tools/bin/sqlcmd \
    -S localhost \
    -U SA \
    -P $MSSQL_SA_PASSWORD \
    -Q &quot;SELECT @@VERSION&quot; 2&gt;/dev/null
  errstatus=$?
  ((counter++))
done

# Display error if connection failed:
if [ $errstatus = 1 ]
then
  echo Cannot connect to SQL Server, installation aborted
  exit $errstatus
fi

# Optional new user creation:
if [ ! -z $SQL_INSTALL_USER ] &amp;&amp; [ ! -z $SQL_INSTALL_USER_PASSWORD ]
then
  echo Creating user $SQL_INSTALL_USER
  /opt/mssql-tools/bin/sqlcmd \
    -S localhost \
    -U SA \
    -P $MSSQL_SA_PASSWORD \
    -Q &quot;CREATE LOGIN [$SQL_INSTALL_USER] WITH PASSWORD=N'$SQL_INSTALL_USER_PASSWORD', DEFAULT_DATABASE=[master], CHECK_EXPIRATION=ON, CHECK_POLICY=ON; ALTER SERVER ROLE [sysadmin] ADD MEMBER [$SQL_INSTALL_USER]&quot;
fi

echo Done!
</code></pre>
<p>运行脚本等着安装完成</p>
<h2 id="开启表的cdc">开启表的cdc</h2>
<h3 id="创建shihu数据库">创建shihu数据库</h3>
<h3 id="建测试表">建测试表</h3>
<pre><code>IF EXISTS (SELECT * FROM sys.all_objects WHERE object_id = OBJECT_ID(N'[dbo].[dt_test]') AND type IN ('U'))
	DROP TABLE [dbo].[dt_test]
GO

CREATE TABLE [dbo].[dt_test] (
  [tid] int  NOT NULL,
  [tname] nvarchar(200) COLLATE SQL_Latin1_General_CP1_CI_AS  NULL,
  [tidcard] nvarchar(200) COLLATE SQL_Latin1_General_CP1_CI_AS  NULL,
  [tbirthday] date  NULL,
  [tmobile] nvarchar(200) COLLATE SQL_Latin1_General_CP1_CI_AS  NULL,
  [temail] nvarchar(200) COLLATE SQL_Latin1_General_CP1_CI_AS  NULL,
  [tgender] bigint  NULL,
  [tcreate_time] datetime  NULL
)
GO

ALTER TABLE [dbo].[dt_test] SET (LOCK_ESCALATION = TABLE)
GO


-- ----------------------------
-- Primary Key structure for table dt_test
-- ----------------------------
ALTER TABLE [dbo].[dt_test] ADD CONSTRAINT [PK__dt_test__DC105B0FA908205A] PRIMARY KEY CLUSTERED ([tid])
WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON)  
ON [PRIMARY]
GO

</code></pre>
<h3 id="开启cdc">开启cdc</h3>
<pre><code>USE shihu
GO
EXEC sys.sp_cdc_enable_db
GO

USE shihu
GO
EXEC sys.sp_cdc_enable_table
@source_schema = 'dbo',
@source_name   = ‘dt_test’,     
@role_name     = NULL,
@filegroup_name = 'PRIMARY',      
@supports_net_changes = 1
GO
</code></pre>
<h2 id="下载kafka插件">下载kafka插件</h2>
<h3 id="官方文档">官方文档</h3>
<p><a href="https://debezium.io/documentation/reference/1.3/connectors/sqlserver.html">https://debezium.io/documentation/reference/1.3/connectors/sqlserver.html</a></p>
<h3 id="下载">下载</h3>
<p><a href="https://repo1.maven.org/maven2/io/debezium/debezium-connector-sqlserver/1.2.5.Final/debezium-connector-sqlserver-1.2.5.Final-plugin.tar.gz">https://repo1.maven.org/maven2/io/debezium/debezium-connector-sqlserver/1.2.5.Final/debezium-connector-sqlserver-1.2.5.Final-plugin.tar.gz</a></p>
<h3 id="创建插件目录">创建插件目录</h3>
<p>在kafka跟目录创建 kafka_connect_plugins 目录<br>
将下载的包解压到此目录中<br>
<img src="https://oldcamel.run/post-images/1610074594464.png" alt="" loading="lazy"></p>
<h3 id="修改kafkaconfigconnect-distributeproperties">修改kafka/config/connect-distribute.properties</h3>
<p>bootstrap.servers=IP:9092<br>
plugin.path= /kafka路径/kafka_connect_plugins</p>
<h3 id="启动kafka-connect">启动kafka-connect</h3>
<pre><code>./bin/connect-distributed.sh  config/connect-distributed.properties &gt;logs/ksc.log &amp;
</code></pre>
<p>开启成功的标志,postman可以使用访问端口8083（默认的端口号,可以在connect-distributed.properties 更改）</p>
<h4 id="rest-api">rest api</h4>
<pre><code>GET /connectors – 返回所有正在运行的connector名
POST /connectors – 新建一个connector; 请求体必须是json格式并且需要包含name字段和config字段，name是connector的名字，config是json格式，必须包含你的connector的配置信息。
GET /connectors/{name} – 获取指定connetor的信息
GET /connectors/{name}/config – 获取指定connector的配置信息
PUT /connectors/{name}/config – 更新指定connector的配置信息
GET /connectors/{name}/status – 获取指定connector的状态，包括它是否在运行、停止、或者失败，如果发生错误，还会列出错误的具体信息。
GET /connectors/{name}/tasks – 获取指定connector正在运行的task。
GET /connectors/{name}/tasks/{taskid}/status – 获取指定connector的task的状态信息
PUT /connectors/{name}/pause – 暂停connector和它的task，停止数据处理知道它被恢复。
PUT /connectors/{name}/resume – 恢复一个被暂停的connector
POST /connectors/{name}/restart – 重启一个connector，尤其是在一个connector运行失败的情况下比较常用
POST /connectors/{name}/tasks/{taskId}/restart – 重启一个task，一般是因为它运行失败才这样做。
DELETE /connectors/{name} – 删除一个connector，停止它的所有task并删除配置。
</code></pre>
<h4 id="添加sqlserver-connect">添加sqlserver connect</h4>
<pre><code>curl --location --request POST 'http://localhost:8083/connectors' \
--header 'Content-Type: application/json' \
--data-raw '{
    &quot;name&quot;: &quot;sqlserver178-connector&quot;,
    &quot;config&quot;: {
        &quot;connector.class&quot;: &quot;io.debezium.connector.sqlserver.SqlServerConnector&quot;,
        &quot;database.hostname&quot;: &quot;sqlserver的ip&quot;,
        &quot;database.port&quot;: &quot;1433&quot;,
        &quot;database.user&quot;: &quot;sa&quot;,
        &quot;database.password&quot;: &quot;test@12345678&quot;,
        &quot;database.dbname&quot;: &quot;shihu&quot;,
        &quot;database.server.name&quot;: &quot;fullfillment&quot;,
        &quot;table.whitelist&quot;: &quot;dbo.dt_test&quot;,
        &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka的IP:9092&quot;,
        &quot;database.history.kafka.topic&quot;: &quot;dbhistory.fullfillment&quot;,
        &quot;database.history.producer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;,
        &quot;database.history.producer.sasl.mechanism&quot;: &quot;PLAIN&quot;,
        &quot;database.history.producer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;admin\&quot; password=\&quot;123\&quot;;&quot;,
        &quot;database.history.consumer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;,
        &quot;database.history.consumer.sasl.mechanism&quot;: &quot;PLAIN&quot;,
        &quot;database.history.consumer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;admin\&quot; password=\&quot;123\&quot;;&quot;
    }
}'
</code></pre>
<h3 id="消费信息">消费信息</h3>
<h4 id="consumerpreperties配置">consumer.preperties配置</h4>
<pre><code>bootstrap.servers=localhost:9092
group.id=test-consumer-group
security.protocol=SASL_PLAINTEXT
sasl.mechanism=PLAIN
</code></pre>
<h4 id="修改器kafka-console-consumersh为kafka-console-consumer-saalsh倒数第二行添加">修改器kafka-console-consumer.sh为kafka-console-consumer-saal.sh倒数第二行添加</h4>
<pre><code>export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/kafka_2.13-2.6.0/config/kafka_client_jaas.conf&quot;
</code></pre>
<h4 id="启动">启动</h4>
<pre><code>./bin/kafka-console-consumer-saal.sh --bootstrap-server localhost:9092 --topic fullfillment.dbo.dt_test --consumer.config config/consumer.properties
</code></pre>
<h4 id="消息示例">消息示例</h4>
<pre><code>{
    &quot;schema&quot;:{
        &quot;type&quot;:&quot;struct&quot;,
        &quot;fields&quot;:Array[6],
        &quot;optional&quot;:false,
        &quot;name&quot;:&quot;fullfillment.dbo.dt_test.Envelope&quot;
    },
    &quot;payload&quot;:{
        &quot;before&quot;:{
            &quot;tid&quot;:1,
            &quot;tname&quot;:&quot;222&quot;,
            &quot;tidcard&quot;:&quot;2&quot;,
            &quot;tbirthday&quot;:-25567,
            &quot;tmobile&quot;:&quot;1&quot;,
            &quot;temail&quot;:&quot;1&quot;,
            &quot;tgender&quot;:2,
            &quot;tcreate_time&quot;:-2208988800000
        },
        &quot;after&quot;:{
            &quot;tid&quot;:1,
            &quot;tname&quot;:&quot;test&quot;,
            &quot;tidcard&quot;:&quot;2&quot;,
            &quot;tbirthday&quot;:-25567,
            &quot;tmobile&quot;:&quot;1&quot;,
            &quot;temail&quot;:&quot;1&quot;,
            &quot;tgender&quot;:2,
            &quot;tcreate_time&quot;:-2208988800000
        },
        &quot;source&quot;:Object{...},
        &quot;op&quot;:&quot;u&quot;,
        &quot;ts_ms&quot;:1610019731750,
        &quot;transaction&quot;:null
    }
}
</code></pre>
<h4 id="oracle示例">oracle示例</h4>
<pre><code>
curl --location --request POST 'http://localhost:8083/connectors/' \
--header 'Content-Type: application/json' \
--data-raw '
{
    &quot;name&quot;: &quot;oracle-61-connector&quot;,
    &quot;config&quot;: {
        &quot;connector.class&quot;: &quot;com.ecer.kafka.connect.oracle.OracleSourceConnector&quot;,
        &quot;db.name.alias&quot;: &quot;61test&quot;,
        &quot;tasks.max&quot;: 1,
        &quot;topic&quot;: &quot;kol&quot;,
        &quot;db.name&quot;: &quot;portaldb&quot;,
        &quot;db.hostname&quot;: &quot;oralce连接host地址&quot;,
        &quot;db.port&quot;: 1521,
        &quot;db.user&quot;: &quot;info&quot;,
        &quot;db.user.password&quot;: &quot;111111&quot;,
        &quot;db.fetch.size&quot;: 1,
        &quot;table.whitelist&quot;: &quot;INFO.*,SPAUTH.*,WORKFLOW.*,FLOWABLE.*,OA.*&quot;,
        &quot;table.blacklist&quot;: &quot;&quot;,
        &quot;parse.dml.data&quot;: true,
        &quot;reset.offset&quot;: false,
        &quot;multitenant&quot;: false
    }
}
</code></pre>
<h3 id="mysql示例">mysql示例</h3>
<pre><code>curl --location --request POST 'http://localhost:8083/connectors/' \
--header 'Content-Type: application/json' \
--data-raw '
{
  &quot;name&quot;: &quot;140mysql-connector&quot;, 
  &quot;config&quot;: {
    &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;, 
    &quot;database.hostname&quot;: &quot;localhost&quot;, 
    &quot;database.port&quot;: &quot;3306&quot;, 
    &quot;database.user&quot;: &quot;root&quot;, 
    &quot;database.password&quot;: &quot;111111&quot;, 
    &quot;database.server.id&quot;: &quot;184054&quot;, 
    &quot;database.server.name&quot;: &quot;fullfillment140ms&quot;, 
    &quot;database.whitelist&quot;:&quot;test&quot;,
    &quot;database.history.kafka.bootstrap.servers&quot;: &quot;localhost:9092&quot;, 
    &quot;database.history.kafka.topic&quot;: &quot;dbhistory.fullfillment140ms&quot;, 
    &quot;include.schema.changes&quot;: &quot;false&quot;,
        &quot;database.history.producer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;,
        &quot;database.history.producer.sasl.mechanism&quot;: &quot;PLAIN&quot;,
        &quot;database.history.producer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;admin\&quot; password=\&quot;123\&quot;;&quot;,
        &quot;database.history.consumer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;,
        &quot;database.history.consumer.sasl.mechanism&quot;: &quot;PLAIN&quot;,
        &quot;database.history.consumer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;admin\&quot; password=\&quot;123\&quot;;&quot;
    }
}
</code></pre>
<h3 id="查看topic-list">查看topic list</h3>
<pre><code>bin/kafka-topics.sh --zookeeper localhost --list
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kafka开启SASL用户名密码认证]]></title>
        <id>https://oldcamel.run/post/kafka-kai-qi-sasl-yong-hu-ming-mi-ma-ren-zheng/</id>
        <link href="https://oldcamel.run/post/kafka-kai-qi-sasl-yong-hu-ming-mi-ma-ren-zheng/">
        </link>
        <updated>2021-01-08T01:51:25.000Z</updated>
        <content type="html"><![CDATA[<h2 id="创建kafka_server_jaasconf文件">创建kafka_server_jaas.conf文件</h2>
<p>config目录下创建kafka_server_jaas.conf文件</p>
<pre><code>KafkaServer {
  org.apache.kafka.common.security.plain.PlainLoginModule required
  username=&quot;admin&quot;
  password=&quot;123&quot;
  user_admin=&quot;123&quot;
  user_yunzai=&quot;123&quot;;
};
</code></pre>
<h2 id="创建kafka_client_jaasconf文件">创建kafka_client_jaas.conf文件</h2>
<p>config目录下创建kafka_client_jaas.conf文件</p>
<pre><code>KafkaClient {
        org.apache.kafka.common.security.plain.PlainLoginModule required
        username=&quot;admin&quot;
        password=&quot;123&quot;;
};
</code></pre>
<h2 id="修改serverproperties">修改server.properties</h2>
<pre><code>listeners=SASL_PLAINTEXT://IP:9092
advertised.listeners=SASL_PLAINTEXT://IP:9092
security.inter.broker.protocol=SASL_PLAINTEXT
sasl.enabled.mechanisms=PLAIN
sasl.mechanism.inter.broker.protocol=PLAIN
authorizer.class.name = kafka.security.auth.SimpleAclAuthorizer
super.users=User:admin;
</code></pre>
<h2 id="修改kafaka启动脚本">修改kafaka启动脚本</h2>
<p>修改bin目录下kafka_start.sh<br>
在倒数第二行添加kafka_server_jaas.conf的全路径</p>
<pre><code>export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/kafka_2.13-2.6.0/config/kafka_server_jaas.conf&quot;
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://oldcamel.run/post-images/1610071605941.png" alt="" loading="lazy"></figure>
<h2 id="可选为特定用户添加特定topic的acl授权">（可选）为特定用户添加特定topic的acl授权</h2>
<p>如下表示为用户yunzai添加topic nginx-log的读写权限。</p>
<pre><code>./bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 -add --allow-principal User:yunzai  --operation Read --operation Write --topic testyunzai
</code></pre>
<p>验证授权</p>
<pre><code>./bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --list --topic testyunzai
</code></pre>
<h3 id="生产者消费者验证">生产者消费者验证</h3>
<h4 id="生产者">生产者</h4>
<p>1.以用户yunzai为例，创建JAAS认证文件yunzai_jaas.conf放在config目录下，内容如下:</p>
<pre><code>KafkaClient {
org.apache.kafka.common.security.plain.PlainLoginModule required
username=&quot;yunzai&quot;
password=&quot;123&quot;;
};
</code></pre>
<p>2.拷贝bin/kafka-console-producer.sh为bin/yunzai-kafka-console-producer.sh，并将JAAS文件作为一个JVM参数传给console producer</p>
<p>倒数第二行添加</p>
<pre><code>export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/kafka_2.13-2.6.0/config/yunzai_jaas.conf&quot;
</code></pre>
<p>3.创建文件producer.config指定如下属性：</p>
<pre><code>security.protocol=SASL_PLAINTEXT
sasl.mechanism=PLAIN
</code></pre>
<p>4.启动producer</p>
<pre><code>./bin/yunzai-kafka-console-producer.sh --broker-list IP:9092 --topic testyunzai --producer.config producer.config
</code></pre>
<h4 id="消费者">消费者</h4>
<p>1.以用户yunzai为例，创建JAAS认证文件yunzai_jaas.conf放在config目录下，如果用户跟生产者是同一个，可以复用上面生产者的JAAS文件，内容如下:</p>
<pre><code>KafkaClient {
org.apache.kafka.common.security.plain.PlainLoginModule required
username=&quot;yunzai&quot;
password=&quot;123&quot;;
};
</code></pre>
<p>2.拷贝bin/kafka-console-consumer.sh为bin/yunzai-kafka-console-consumer.sh，并将JAAS文件作为一个JVM参数传给console consumer<br>
倒数第二行添加</p>
<pre><code>export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/kafka_2.13-2.6.0/config/yunzai_jaas.conf&quot;
</code></pre>
<p>3.创建文件consumer.config指定如下属性：</p>
<pre><code>security.protocol=SASL_PLAINTEXT
sasl.mechanism=PLAIN
group.id=test
</code></pre>
<p>4.启动consumer</p>
<pre><code>./bin/yunzai-kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic testyunzai --from-beginning --consumer.config consumer.config
</code></pre>
<h3 id="java-beam客户端认证">java beam客户端认证</h3>
<p>beam设置</p>
<pre><code class="language-java">   final Map&lt;String, Object&gt; immutableMap = new ImmutableMap.Builder&lt;String, Object&gt;()
                .put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;)
                .put(ConsumerConfig.GROUP_ID_CONFIG, options.getGroupid())
                .put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true)
                .put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, &quot;SASL_PLAINTEXT&quot;)
                .put(SaslConfigs.SASL_MECHANISM, &quot;PLAIN&quot;)
                .put(SaslConfigs.SASL_JAAS_CONFIG, &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;&quot; + options.getKafkaUsername() + &quot;\&quot; password=\&quot;&quot; + options.getKafkaPassword() + &quot;\&quot;;&quot;)
                .build();
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[VictoriaMetrics 汇总多个Prometheus指标（四）读取Istio指标]]></title>
        <id>https://oldcamel.run/post/victoriametrics-hui-zong-duo-ge-prometheus-zhi-biao-san-du-qu-istio-zhi-biao/</id>
        <link href="https://oldcamel.run/post/victoriametrics-hui-zong-duo-ge-prometheus-zhi-biao-san-du-qu-istio-zhi-biao/">
        </link>
        <updated>2020-12-25T07:41:53.000Z</updated>
        <content type="html"><![CDATA[<p>istio中默认带了prometheus,里面包含istio代理的指标，现需要将istio中的指标配置在kube-prometheus中。</p>
<h2 id="修改prometheus-k8s-角色权限">修改prometheus-k8s 角色权限</h2>
<p>修改prometheus-clusterRole.yaml</p>
<pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus-k8s
rules:
- apiGroups:
  - &quot;&quot;
  resources:
  - nodes
  - services
  - endpoints
  - pods
  - nodes/proxy
  verbs:
  - get
  - watch
  - list
- apiGroups:
  - &quot;&quot;
  resources:
  - configmaps
  verbs:
  - get
- nonResourceURLs:
  - /metrics
  verbs:
  - get
</code></pre>
<h2 id="配置数据平面的服务监控">配置数据平面的服务监控</h2>
<pre><code>apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: prometheus-oper-istio-controlplane
spec:
  jobLabel: istio
  selector:
    matchExpressions:
      - {key: istio, operator: In, values: [mixer,pilot,galley,citadel,sidecar-injector]}
  namespaceSelector:
    any: true
  endpoints:
  - port: http-monitoring
    interval: 15s
  - port: http-policy-monitoring
    interval: 15s

</code></pre>
<h2 id="配置数据层的服务监控">配置数据层的服务监控</h2>
<pre><code>apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: prometheus-oper-istio-dataplane
  labels:
    monitoring: istio-dataplane
spec:
  selector:
    matchExpressions:
      - {key: istio-prometheus-ignore, operator: DoesNotExist}
  namespaceSelector:
    any: true
  jobLabel: envoy-stats
  endpoints:
  - path: /stats/prometheus
    targetPort: http-envoy-prom
    interval: 15s


</code></pre>
<h2 id="查看是否生效">查看是否生效</h2>
<figure data-type="image" tabindex="1"><img src="https://oldcamel.run/post-images/1608882536864.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="2"><img src="https://oldcamel.run/post-images/1608882571184.png" alt="" loading="lazy"></figure>
<h2 id="victoriametrics-中不同集群中的指标获取">VictoriaMetrics 中不同集群中的指标获取</h2>
<figure data-type="image" tabindex="3"><img src="https://oldcamel.run/post-images/1608882968056.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[VictoriaMetrics 汇总多个Prometheus指标（三）kube-prometheus安装]]></title>
        <id>https://oldcamel.run/post/victoriametrics-hui-zong-duo-ge-prometheus-zhi-biao-san-kube-prometheus-an-zhuang/</id>
        <link href="https://oldcamel.run/post/victoriametrics-hui-zong-duo-ge-prometheus-zhi-biao-san-kube-prometheus-an-zhuang/">
        </link>
        <updated>2020-12-25T07:32:20.000Z</updated>
        <content type="html"><![CDATA[<p>kube-prometheus是Prometheus Operator 针对kubernetes的监控组件。<br>
github地址<br>
<a href="https://github.com/prometheus-operator/kube-prometheus">https://github.com/prometheus-operator/kube-prometheus</a></p>
<h2 id="配置远程写入">配置远程写入</h2>
<h3 id="basic用户名密码配置-secret">basic用户名密码配置 secret</h3>
<pre><code>apiVersion: v1
kind: Secret
metadata:
  name: vmuser
  namespace: monitoring
type: kubernetes.io/basic-auth
stringData:
  username: username
  password: password
</code></pre>
<h3 id="添加远程写入-和外部标签">添加远程写入 和外部标签</h3>
<p>修改prometheus-prometheus.yaml文件，添加</p>
<pre><code>prometheusExternalLabelName: &quot;&quot;
  replicaExternalLabelName: &quot;&quot;
  externalLabels:
    tenant: &quot;租户名称&quot;
  remoteWrite:
    - url: http://VictoriaMetrics连接地址/api/v1/write
      basicAuth:
       username:
          key: username
          name: vmuser
       password:
          key: password
          name: vmuser
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[VictoriaMetrics 汇总多个Prometheus指标（二）Grafana安装]]></title>
        <id>https://oldcamel.run/post/grafana-an-zhuang/</id>
        <link href="https://oldcamel.run/post/grafana-an-zhuang/">
        </link>
        <updated>2020-12-25T06:50:09.000Z</updated>
        <content type="html"><![CDATA[<p>安装grafana 从victoriametrics中读取汇总的指标</p>
<h2 id="安装mysql">安装mysql</h2>
<h3 id="下载离线包">下载离线包</h3>
<p><a href="https://dev.mysql.com/downloads/mysql/5.7.html#downloads">下载地址</a><br>
<img src="https://oldcamel.run/post-images/1608879483467.png" alt="" loading="lazy"></p>
<h3 id="解压安装">解压安装</h3>
<figure data-type="image" tabindex="1"><img src="https://oldcamel.run/post-images/1608879544128.png" alt="" loading="lazy"></figure>
<pre><code>rpm -Uvh *.rpm --nodeps --force
</code></pre>
<h3 id="修改配置文件">修改配置文件</h3>
<p>修改 /etc/my.cnf 添加</p>
<pre><code>lower_case_table_names=1
default-storage-engine=INNODB
</code></pre>
<h3 id="生成用户不带密码">生成用户不带密码</h3>
<pre><code>mysqld --initialize-insecure
</code></pre>
<p>如果要重新初始化，必须先清空data文件夹</p>
<h3 id="启动mysql">启动mysql</h3>
<pre><code>systemctl start mysqld
</code></pre>
<h3 id="进入mysql">进入mysql</h3>
<pre><code>mysql -u root
</code></pre>
<h3 id="修改密码开启远程访问">修改密码+开启远程访问</h3>
<pre><code>set password for root@localhost = password('123456');
GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456' WITH GRANT OPTION;
FLUSH PRIVILEGES;
</code></pre>
<h3 id="创建grafana数据库">创建grafana数据库</h3>
<pre><code>CREATE DATABASE  `grafana` DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
</code></pre>
<h2 id="安装grafana">安装grafana</h2>
<h3 id="下载安装">下载安装</h3>
<p><a href="https://grafana.com/grafana/download">参考文档</a></p>
<pre><code>wget https://dl.grafana.com/oss/release/grafana-7.3.6-1.x86_64.rpm
sudo yum install grafana-7.3.6-1.x86_64.rpm
</code></pre>
<h3 id="修改配置文件-2">修改配置文件</h3>
<pre><code class="language-shell">vim /etc/grafana/grafana.ini
</code></pre>
<h4 id="domain修改">domain修改</h4>
<pre><code>domain = 修改成主机IP
</code></pre>
<h4 id="默认用户名密码修改">默认用户名密码修改</h4>
<pre><code>admin_user = admin
admin_password = 123456
</code></pre>
<h4 id="数据库修改">数据库修改</h4>
<pre><code>type = mysql
host = 127.0.0.1:3306
name = grafana
user = root
password =123456

</code></pre>
<h3 id="启动grafana服务">启动grafana服务</h3>
<pre><code>service grafana-server start
</code></pre>
<h3 id="配置-victoriametrics连接">配置 victoriametrics连接</h3>
<p>访问 ip:3000<br>
添加prometheus数据源 设置为 victoriametrics主机ip  端口 8428<br>
<img src="https://oldcamel.run/post-images/1608881141442.png" alt="" loading="lazy"></p>
<p>配置victoriametrics官方的监控图表<br>
<a href="https://grafana.com/grafana/dashboards/10229">grafana lab地址</a><br>
<img src="https://oldcamel.run/post-images/1608881288591.png" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[VictoriaMetrics 汇总多个Prometheus指标（一）安装VictoriaMetrics]]></title>
        <id>https://oldcamel.run/post/victoriametrics-an-zhuang/</id>
        <link href="https://oldcamel.run/post/victoriametrics-an-zhuang/">
        </link>
        <updated>2020-12-25T06:33:44.000Z</updated>
        <content type="html"><![CDATA[<h2 id="victoriametrics-用来收集k8s集群中的prometheus指标信息">victoriametrics 用来收集k8s集群中的prometheus指标信息 。</h2>
<p><a href="https://github.com/VictoriaMetrics/VictoriaMetrics">github地址</a><br>
<a href="https://victoriametrics.github.io/#retention">官方文档地址</a><br>
<a href="https://github.com/VictoriaMetrics/VictoriaMetrics/releases">下载二级制安装包</a></p>
<figure data-type="image" tabindex="1"><img src="https://oldcamel.run/post-images/1608878459634.png" alt="" loading="lazy"></figure>
<h2 id="启动主进程">启动主进程</h2>
<p>运行端口8428</p>
<pre><code class="language-shell">./victoria-metrics-prod -retentionPeriod=3 -selfScrapeInterval=10s &amp;
</code></pre>
<h2 id="启动认证代理">启动认证代理</h2>
<p>运行端口8427</p>
<pre><code class="language-shell">./vmauth-prod -auth.config=/opt/vm/config/config.yml &amp;
</code></pre>
<h2 id="basic认证配置文件">basic认证配置文件</h2>
<pre><code>users:
- username: &quot;username&quot;
  password: &quot;password&quot;
  url_prefix: &quot;http://localhost:8428&quot;
</code></pre>
<h2 id="运行端口">运行端口</h2>
<figure data-type="image" tabindex="2"><img src="https://oldcamel.run/post-images/1608878759387.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[flowable修改流程缓存实现自由流程]]></title>
        <id>https://oldcamel.run/post/flowable-xiu-gai-liu-cheng-huan-cun-shi-xian-zi-you-liu-cheng/</id>
        <link href="https://oldcamel.run/post/flowable-xiu-gai-liu-cheng-huan-cun-shi-xian-zi-you-liu-cheng/">
        </link>
        <updated>2020-12-11T08:32:34.000Z</updated>
        <content type="html"><![CDATA[<p><br>
flowable中会对流程定义做缓存处理，在实现自由流程的时候需要动态给流程定义添加节点，默认的是对流程定义做的缓存，要想在每个流程实例中动态添加节点，可以通过修改流程缓存，添加每个流程实例的缓存</p>
<h2 id="涉及到的类">涉及到的类</h2>
<figure data-type="image" tabindex="1"><img src="https://img-blog.csdnimg.cn/20200410174158592.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5MTM4NjE0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<h2 id="主要步骤">主要步骤</h2>
<ul>
<li>步骤1：修改BpmnJsonConverter,添加自定义属性，区分流程是否为自由流程</li>
<li>步骤2：设置全局的FlowableEventListener( FlowableEngineEventType. ACTIVITY_COMPLETED ),在流程启动的时候判断流程是否为自由流程，如果是的话，把流程的bpmnModel转成xml设置到自己定义的缓存对象中，然后保存到redis里。<br>
Redis采用了Hash方式存储，key为流程实例ID,value为自定义缓存类。</li>
<li>步骤3：自由流程添加节点的时候提前把流程实例id放在request作用域中</li>
<li>步骤4：定义 DefaultDeploymentCache 子类重写get方法，如果是自定义流程并且request作用域中有流程实例id就中redis中获取流程定义json转换成 ProcessDefinitionCacheEntry</li>
<li>步骤5：定义自由流程命令类，获取流程定义缓存，给里面添加连线和节点</li>
<li>步骤6：往新加的节点上跳转</li>
</ul>
<h2 id="放入缓存主要代码">放入缓存主要代码</h2>
<p>if (FlowUtils.isFreeProcess(entity.getProcessDefinitionId())) {</p>
<pre><code class="language-java"> //放入缓存
    ProcessDefinitionCacheEntry processDefinitionCacheEntry = managementService.executeCommand(new GetProcessDefinitionCacheEntryCmd(entity.getProcessDefinitionId()));
    CustomProcessDefinitionCacheEntry customProcessDefinitionCacheEntry = FlowUtils.parseCustomProcessDefinitionCacheEntry(processDefinitionCacheEntry);
    FreeProcessCaChe cache = SpringUtil.getBean(FreeProcessCaChe.class);
    cache.add(entity.getProcessInstanceId(), customProcessDefinitionCacheEntry);
}
</code></pre>
<h2 id="下方为以上用到的类以及方法">下方为以上用到的类以及方法</h2>
<ul>
<li>获取缓存命令类</li>
</ul>
<pre><code class="language-java">public class GetProcessDefinitionCacheEntryCmd implements Command&lt;ProcessDefinitionCacheEntry&gt; {
	protected String processDefinitionId;

	public GetProcessDefinitionCacheEntryCmd(String processDefinitionId) {
		this.processDefinitionId = processDefinitionId;
	}

	@Override
	public ProcessDefinitionCacheEntry execute(CommandContext commandContext) {
		DeploymentManager deploymentManager = CommandContextUtil.getProcessEngineConfiguration().getDeploymentManager();
		ProcessDefinitionCacheEntry processDefinitionCacheEntry = deploymentManager.getProcessDefinitionCache()
				.get(processDefinitionId);
		return processDefinitionCacheEntry;
	}

}
</code></pre>
<ul>
<li>序列化缓存对象方法</li>
</ul>
<pre><code class="language-java"> public static CustomProcessDefinitionCacheEntry parseCustomProcessDefinitionCacheEntry(ProcessDefinitionCacheEntry processDefinitionCacheEntry) {
    CustomProcessDefinitionCacheEntry customProcessDefinitionCacheEntry = new CustomProcessDefinitionCacheEntry();
    ProcessDefinition processDefinition = processDefinitionCacheEntry.getProcessDefinition();
    String resourceName = processDefinition.getResourceName();
    String deploymentId = processDefinition.getDeploymentId();
    BpmnModel bpmnModel = processDefinitionCacheEntry.getBpmnModel();
    BpmnXMLConverter bpmnXMLConverter = new BpmnXMLConverter();
    byte[] bytes = bpmnXMLConverter.convertToXML(bpmnModel);
    customProcessDefinitionCacheEntry.setBpmnModel(bytes);
    customProcessDefinitionCacheEntry.setDeploymentId(deploymentId);
    customProcessDefinitionCacheEntry.setResourceName(resourceName);
    return customProcessDefinitionCacheEntry;
}
</code></pre>
<ul>
<li>Redis存储的对象</li>
</ul>
<pre><code class="language-java">@Data
@AllArgsConstructor
@NoArgsConstructor
public class CustomProcessDefinitionCacheEntry implements Serializable {
    private static final long serialVersionUID = 6833801933658529071L;
    protected  String deploymentId;
    protected  String resourceName;
    protected byte[] bpmnModel;
}
</code></pre>
<ul>
<li>添加到redis接口</li>
</ul>
<pre><code class="language-java">public interface FreeProcessCaChe {
    CustomProcessDefinitionCacheEntry get(String key);
    void set(String key,CustomProcessDefinitionCacheEntry process);

    boolean contains(String key);

    void add(String key, CustomProcessDefinitionCacheEntry process);

    void remove(String key);

    void clear();
}
</code></pre>
<ul>
<li>添加到redis接口实现</li>
</ul>
<pre><code class="language-java">@Service
public class FreeProcessCaCheImpl implements FreeProcessCaChe {
    private String hashkey = &quot;freeprocesscache&quot;;

    @PostConstruct
    public void init() {
        boolean b = redisUtil.hasKey(hashkey);
        if (!b) {
            redisUtil.hset(hashkey, &quot;init&quot;, &quot;init&quot;);
        }
    }


    @Autowired
    private RedisUtil redisUtil;

    @Override
    public CustomProcessDefinitionCacheEntry get(String key) {
        if (StringUtils.isBlank(key)) {
            return null;
        }
        Object value = redisUtil.hget(hashkey, key);
        if (value != null) {
            JSONObject jb = (JSONObject) value;
            CustomProcessDefinitionCacheEntry customProcessDefinitionCacheEntry = JSON.toJavaObject(jb, CustomProcessDefinitionCacheEntry.class);
            return customProcessDefinitionCacheEntry;
        }
        return null;
    }

    @Override
    public boolean contains(String key) {
        return redisUtil.hHasKey(hashkey, key);
    }

    @Override
    public void add(String key, CustomProcessDefinitionCacheEntry process) {
        boolean hset = redisUtil.hset(hashkey, key, process);
        if (!hset) {
            throw new CheckErrorException(&quot;缓存自由流程信息失败&quot;);
        }
    }

    @Override
    public void set(String key, CustomProcessDefinitionCacheEntry process) {
        boolean hset = redisUtil.hset(hashkey, key, process);
        if (!hset) {
            throw new CheckErrorException(&quot;更新自由流程信息失败&quot;);
        }
    }

    @Override
    public void remove(String key) {
        redisUtil.hdel(hashkey, key);
    }

    @Override
    public void clear() {
        redisUtil.hmset(hashkey, Maps.newHashMap());
    }
}
</code></pre>
<h2 id="修改流程配置类添加自定义缓存获取">修改流程配置类添加自定义缓存获取</h2>
<pre><code class="language-java">springProcessEngineConfiguration.setProcessDefinitionCache(new CustomDeploymentCache&lt;&gt;());
</code></pre>
<p>设置的自定义缓存获取类（主要是重写获取缓存的get方法，以下仅为参考）</p>
<pre><code class="language-java">public class CustomDeploymentCache&lt;T&gt; extends DefaultDeploymentCache&lt;T&gt;  {
    @Override
    public T get(String id) {
        T t = super.get(id);
        if(t==null){
            return  t;
        }
        String processInstanceId=null;
        try {
             processInstanceId = (String) FlowUtils.getRequest().getAttribute(&quot;processInstanceId&quot;);
            if (StringUtils.isBlank(processInstanceId)) {
                return t;
            }
        }catch (Exception e){
            return t;
        }
        if(t instanceof ProcessDefinitionCacheEntry) {
            JSONObject jsonObject = new JSONObject();
            Process mainProcess = ((ProcessDefinitionCacheEntry) t).getBpmnModel().getMainProcess();
            String processGlobelSettings = &quot;processGlobelSettings&quot;;
            List&lt;ExtensionElement&gt; extensionElements = mainProcess.getExtensionElements().get(processGlobelSettings);
            if (extensionElements != null &amp;&amp; (!extensionElements.isEmpty())) {
                ExtensionElement extensionElement = extensionElements.get(0);
                String elementText = extensionElement.getElementText();
                jsonObject = JSON.parseObject(elementText, Feature.OrderedField);
            }
           boolean freeProcess=jsonObject.getBooleanValue(&quot;freeProcess&quot;);
            if (!freeProcess) {
                return super.get(id);
            } else {
                //从缓存中取值
                FreeProcessCaChe cache = SpringUtil.getBean(FreeProcessCaChe.class);
                CustomProcessDefinitionCacheEntry redisProcessDefinitionCacheEntry = cache.get(processInstanceId);
                BpmnModel bpmnModel=null;
                if(redisProcessDefinitionCacheEntry==null){
                    return  t;
                }else {
                    try {
                        bpmnModel = FlowUtils.parseBpmnModelFromCustomProcessDefinitionCacheEntry(redisProcessDefinitionCacheEntry);
                    } catch (Exception e) {
                        e.printStackTrace();
                    }
                }
                Process process = bpmnModel.getMainProcess();
                RuntimeService  runtimeService = SpringUtil.getBean(RuntimeService.class);
                RepositoryService repositoryService = SpringUtil.getBean(RepositoryService.class);
                ProcessInstance processInstance = runtimeService.createProcessInstanceQuery().processInstanceId(processInstanceId).singleResult();
                String processDefinitionId = processInstance.getProcessDefinitionId();
                ProcessDefinition dataProcessDefinition = null;
                if(t!=null){
                    ProcessDefinitionCacheEntry pdc=(ProcessDefinitionCacheEntry)t;
                    dataProcessDefinition=pdc.getProcessDefinition();
                }else{
                    dataProcessDefinition=repositoryService.createProcessDefinitionQuery().processDefinitionId(processDefinitionId).singleResult();
                }
                ProcessDefinitionCacheEntry customProcessDefinitionCacheEntry = new ProcessDefinitionCacheEntry(dataProcessDefinition,bpmnModel,process);

                return (T) customProcessDefinitionCacheEntry;
            }
        }else{
            return super.get(id);
        }
    }
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[flowable获取当前任务节点下一步会创建的用户任务]]></title>
        <id>https://oldcamel.run/post/flowable-nexttask/</id>
        <link href="https://oldcamel.run/post/flowable-nexttask/">
        </link>
        <updated>2020-12-11T05:58:52.000Z</updated>
        <content type="html"><![CDATA[<h2 id="获取方法">获取方法</h2>
<pre><code class="language-java">package xxxx.util;

import com.alibaba.fastjson.JSON;
import com.alibaba.fastjson.JSONArray;
import com.alibaba.fastjson.JSONObject;
import com.alibaba.fastjson.serializer.SerializerFeature;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.greenpineyu.fel.FelEngine;
import com.greenpineyu.fel.FelEngineImpl;
import com.greenpineyu.fel.context.FelContext;
import org.apache.commons.lang.StringUtils;
import org.apache.commons.lang3.BooleanUtils;
import org.flowable.bpmn.converter.BpmnXMLConverter;
import org.flowable.bpmn.model.*;
import org.flowable.bpmn.model.Process;
import org.flowable.engine.HistoryService;
import org.flowable.engine.RepositoryService;
import org.flowable.engine.RuntimeService;
import org.flowable.engine.TaskService;
import org.flowable.engine.history.HistoricProcessInstance;
import org.flowable.engine.impl.bpmn.parser.BpmnParse;
import org.flowable.engine.impl.bpmn.parser.BpmnParser;
import org.flowable.engine.impl.cfg.ProcessEngineConfigurationImpl;
import org.flowable.engine.impl.persistence.deploy.ProcessDefinitionCacheEntry;
import org.flowable.engine.impl.persistence.entity.DeploymentEntity;
import org.flowable.engine.impl.persistence.entity.DeploymentEntityManager;
import org.flowable.engine.repository.ProcessDefinition;
import org.flowable.bpmn.model.Task;
import org.flowable.ui.modeler.domain.Model;
import org.flowable.ui.modeler.serviceapi.ModelService;
import org.springframework.web.context.request.RequestContextHolder;
import org.springframework.web.context.request.ServletRequestAttributes;

import javax.servlet.http.HttpServletRequest;
import java.io.ByteArrayInputStream;
import java.io.IOException;
import java.sql.ResultSet;
import java.sql.ResultSetMetaData;
import java.util.*;

public class FlowUtils {


    /**
     * 获取下一步骤的用户任务
     *
     * @param repositoryService
     * @param taskService
     * @param map
     * @return
     */
    public static List&lt;UserTask&gt; getNextUserTasks(RepositoryService repositoryService, TaskService taskService, org.flowable.task.api.Task task, Map&lt;String, Object&gt; map) {
        List&lt;UserTask&gt; data = new ArrayList&lt;&gt;();
        ProcessDefinition processDefinition = repositoryService.createProcessDefinitionQuery().processDefinitionId(task.getProcessDefinitionId()).singleResult();
        BpmnModel bpmnModel = repositoryService.getBpmnModel(processDefinition.getId());
        Process mainProcess = bpmnModel.getMainProcess();
        Collection&lt;FlowElement&gt; flowElements = mainProcess.getFlowElements();
        String key = task.getTaskDefinitionKey();
        FlowElement flowElement = bpmnModel.getFlowElement(key);
        next(flowElements, flowElement, map, data);
        return data;
    }

    public static void next(Collection&lt;FlowElement&gt; flowElements, FlowElement flowElement, Map&lt;String, Object&gt; map, List&lt;UserTask&gt; nextUser) {
        //如果是结束节点
        if (flowElement instanceof EndEvent) {
            //如果是子任务的结束节点
            if (getSubProcess(flowElements, flowElement) != null) {
                flowElement = getSubProcess(flowElements, flowElement);
            }
        }
        //获取Task的出线信息--可以拥有多个
        List&lt;SequenceFlow&gt; outGoingFlows = null;
        if (flowElement instanceof Task) {
            outGoingFlows = ((Task) flowElement).getOutgoingFlows();
        } else if (flowElement instanceof Gateway) {
            outGoingFlows = ((Gateway) flowElement).getOutgoingFlows();
        } else if (flowElement instanceof StartEvent) {
            outGoingFlows = ((StartEvent) flowElement).getOutgoingFlows();
        } else if (flowElement instanceof SubProcess) {
            outGoingFlows = ((SubProcess) flowElement).getOutgoingFlows();
        } else if (flowElement instanceof CallActivity) {
            outGoingFlows = ((CallActivity) flowElement).getOutgoingFlows();
        }
        if (outGoingFlows != null &amp;&amp; outGoingFlows.size() &gt; 0) {
            //遍历所有的出线--找到可以正确执行的那一条
            for (SequenceFlow sequenceFlow : outGoingFlows) {
                //1.有表达式，且为true
                //2.无表达式
                String expression = sequenceFlow.getConditionExpression();
                if (expression == null ||
                        Boolean.valueOf(
                                String.valueOf(
                                        result(map, expression.substring(expression.lastIndexOf(&quot;{&quot;) + 1, expression.lastIndexOf(&quot;}&quot;)))))) {
                    //出线的下一节点
                    String nextFlowElementID = sequenceFlow.getTargetRef();
                    if (checkSubProcess(nextFlowElementID, flowElements, nextUser)) {
                        continue;
                    }

                    //查询下一节点的信息
                    FlowElement nextFlowElement = getFlowElementById(nextFlowElementID, flowElements);
                    //调用流程
                    if (nextFlowElement instanceof CallActivity) {
                        CallActivity ca = (CallActivity) nextFlowElement;
                        if (ca.getLoopCharacteristics() != null) {
                            UserTask userTask = new UserTask();
                            userTask.setId(ca.getId());

                            userTask.setId(ca.getId());
                            userTask.setLoopCharacteristics(ca.getLoopCharacteristics());
                            userTask.setName(ca.getName());
                            nextUser.add(userTask);
                        }
                        next(flowElements, nextFlowElement, map, nextUser);
                    }
                    //用户任务
                    if (nextFlowElement instanceof UserTask) {
                        nextUser.add((UserTask) nextFlowElement);
                    }
                    //排他网关
                    else if (nextFlowElement instanceof ExclusiveGateway) {
                        next(flowElements, nextFlowElement, map, nextUser);
                    }
                    //并行网关
                    else if (nextFlowElement instanceof ParallelGateway) {
                        next(flowElements, nextFlowElement, map, nextUser);
                    }
                    //接收任务
                    else if (nextFlowElement instanceof ReceiveTask) {
                        next(flowElements, nextFlowElement, map, nextUser);
                    }
                    //服务任务
                    else if (nextFlowElement instanceof ServiceTask) {
                        next(flowElements, nextFlowElement, map, nextUser);
                    }
                    //子任务的起点
                    else if (nextFlowElement instanceof StartEvent) {
                        next(flowElements, nextFlowElement, map, nextUser);
                    }
                    //结束节点
                    else if (nextFlowElement instanceof EndEvent) {
                        next(flowElements, nextFlowElement, map, nextUser);
                    }
                }
            }
        }
    }

    /**
     * 判断是否是多实例子流程并且需要设置集合类型变量
     */
    public static boolean checkSubProcess(String Id, Collection&lt;FlowElement&gt; flowElements, List&lt;UserTask&gt; nextUser) {
        for (FlowElement flowElement1 : flowElements) {
            if (flowElement1 instanceof SubProcess &amp;&amp; flowElement1.getId().equals(Id)) {

                SubProcess sp = (SubProcess) flowElement1;
                if (sp.getLoopCharacteristics() != null) {
                    String inputDataItem = sp.getLoopCharacteristics().getInputDataItem();
                    UserTask userTask = new UserTask();
                    userTask.setId(sp.getId());
                    userTask.setLoopCharacteristics(sp.getLoopCharacteristics());
                    userTask.setName(sp.getName());
                    nextUser.add(userTask);
                    return true;
                }
            }
        }

        return false;

    }

    /**
     * 查询一个节点的是否子任务中的节点，如果是，返回子任务
     *
     * @param flowElements 全流程的节点集合
     * @param flowElement  当前节点
     * @return
     */
    public static FlowElement getSubProcess(Collection&lt;FlowElement&gt; flowElements, FlowElement flowElement) {
        for (FlowElement flowElement1 : flowElements) {
            if (flowElement1 instanceof SubProcess) {
                for (FlowElement flowElement2 : ((SubProcess) flowElement1).getFlowElements()) {
                    if (flowElement.equals(flowElement2)) {
                        return flowElement1;
                    }
                }
            }
        }
        return null;
    }


    /**
     * 根据ID查询流程节点对象, 如果是子任务，则返回子任务的开始节点
     *
     * @param Id           节点ID
     * @param flowElements 流程节点集合
     * @return
     */
    public static FlowElement getFlowElementById(String Id, Collection&lt;FlowElement&gt; flowElements) {
        for (FlowElement flowElement : flowElements) {
            if (flowElement.getId().equals(Id)) {
                //如果是子任务，则查询出子任务的开始节点
                if (flowElement instanceof SubProcess) {
                    return getStartFlowElement(((SubProcess) flowElement).getFlowElements());
                }
                return flowElement;
            }
            if (flowElement instanceof SubProcess) {
                FlowElement flowElement1 = getFlowElementById(Id, ((SubProcess) flowElement).getFlowElements());
                if (flowElement1 != null) {
                    return flowElement1;
                }
            }
        }
        return null;
    }

    /**
     * 返回流程的开始节点
     *
     * @param flowElements 节点集合
     * @description:
     */
    public static FlowElement getStartFlowElement(Collection&lt;FlowElement&gt; flowElements) {
        for (FlowElement flowElement : flowElements) {
            if (flowElement instanceof StartEvent) {
                return flowElement;
            }
        }
        return null;
    }

    /**
     * 校验el表达示例
     *
     * @param map
     * @param expression
     * @return
     */
    public static Object result(Map&lt;String, Object&gt; map, String expression) {
        FelEngine fel = new FelEngineImpl();
        FelContext ctx = fel.getContext();
        for (Map.Entry&lt;String, Object&gt; entry : map.entrySet()) {
            ctx.set(entry.getKey(), entry.getValue());
        }
        Object result = fel.eval(expression);
        return result;
    }

}


</code></pre>
<h2 id="maven-el表达式校验的依赖">maven  el表达式校验的依赖</h2>
<pre><code class="language-xml">&lt;dependency&gt; 
    &lt;groupId&gt;org.eweb4j&lt;/groupId&gt; 
    &lt;artifactId&gt;fel&lt;/artifactId&gt; 
    &lt;version&gt;0.8&lt;/version&gt; 
&lt;/dependency&gt;

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ Beam+Kafka同步Oracle日志到ClickHouse]]></title>
        <id>https://oldcamel.run/post/apache-beam-tong-bu-oracle-ri-zhi-dao-clickhouse/</id>
        <link href="https://oldcamel.run/post/apache-beam-tong-bu-oracle-ri-zhi-dao-clickhouse/">
        </link>
        <updated>2020-12-10T10:19:02.000Z</updated>
        <content type="html"><![CDATA[<p>用kafka的oracle connect插件存储oracle日志，用beam读取kafka数据插入到clickhouse中</p>
<h2 id="oracle配置">oracle配置</h2>
<h3 id="archivelog模式">archivelog模式</h3>
<p>数据库必须处于archivelog模式，并且必须启用补充日志记录<br>
在数据库服务器上执行</p>
<pre><code>sqlplus / as sysdba    
SQL&gt;shutdown immediate
SQL&gt;startup mount
SQL&gt;alter database archivelog;
SQL&gt;alter database open;
</code></pre>
<h3 id="启用补充日志记录">启用补充日志记录</h3>
<pre><code>sqlplus / as sysdba    
SQL&gt;alter database add supplemental log data (all) columns;
</code></pre>
<h2 id="kafka配置">kafka配置</h2>
<h3 id="插件配置文件">插件配置文件</h3>
<p>在$KAFKA_HOME/config 目录新建OracleSourceConnector.properties配置文件，<br>
文件内示例配置</p>
<pre><code>name=oracle-logminer-connector
connector.class=com.ecer.kafka.connect.oracle.OracleSourceConnector
db.name.alias=test
tasks.max=1
topic=cdctest
db.name=testdb
db.hostname=10.1.X.X
db.port=1521
db.user=kminer
db.user.password=kminerpass
db.fetch.size=1
table.whitelist=TEST.*,TEST2.TABLE2
table.blacklist=TEST2.TABLE3
parse.dml.data=true
reset.offset=false
multitenant=false
</code></pre>
<h3 id="配置说明">配置说明</h3>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>连接器名称</td>
</tr>
<tr>
<td>connector.class</td>
<td>String</td>
<td>此连接器的Java类的名称</td>
</tr>
<tr>
<td>db.name.alias</td>
<td>String</td>
<td>数据库的标识符名称（例如Test，Dev，Prod）或用于标识数据库的特定名称该名称将用作主题和架构名称的标头</td>
</tr>
<tr>
<td>tasks.max</td>
<td>Integer</td>
<td>创建的最大任务数此连接器使用单个任务.</td>
</tr>
<tr>
<td>topic</td>
<td>String</td>
<td>消息将写入的主题的名称如果设置了值，则所有消息都将写入此声明的topic，如果未设置，则将为每个数据库表动态创建一个主题</td>
</tr>
<tr>
<td>db.name</td>
<td>String</td>
<td>要连接的数据库的服务名称或sid通常使用数据库服务名称</td>
</tr>
<tr>
<td>db.hostname</td>
<td>String</td>
<td>Oracle数据库服务器的IP地址或主机名</td>
</tr>
<tr>
<td>db.port</td>
<td>Integer</td>
<td>Oracle数据库服务器的端口号</td>
</tr>
<tr>
<td>db.user</td>
<td>String</td>
<td>数据库的用户名</td>
</tr>
<tr>
<td>db.user.password</td>
<td>String</td>
<td>数据库用户密码</td>
</tr>
<tr>
<td>db.fetch.size</td>
<td>Integer</td>
<td>此配置属性设置Oracle行提取大小值</td>
</tr>
<tr>
<td>table.whitelist</td>
<td>String</td>
<td>白名单格式为用户名.表名用逗号分隔，如果要收集用户下的所有的表，用用户名.*</td>
</tr>
<tr>
<td>parse.dml.data</td>
<td>Boolean</td>
<td>如果为true，则将捕获的sql DML语句解析为字段和值;如果为false，则仅发布sql DML语句</td>
</tr>
<tr>
<td>reset.offset</td>
<td>Boolean</td>
<td>如果为true，则在连接器启动时将偏移值设置为数据库的当前SCN如果为false，则连接器将从上一个偏移值开始</td>
</tr>
<tr>
<td>start.scn</td>
<td>Long</td>
<td>如果设置此属性，则将偏移值设置为该指定值，并且logminer将从此SCN启动如果连接器希望从所需的SCN启动，则可以使用此属性</td>
</tr>
<tr>
<td>multitenant</td>
<td>Boolean</td>
<td>如果为true，则启用多租户支持如果为false，将使用单实例配置</td>
</tr>
<tr>
<td>table.blacklist</td>
<td>String</td>
<td>黑名单格式为用户名.表名用逗号分隔，如果要收集用户下的所有的表，用用户名.*.</td>
</tr>
<tr>
<td>dml.types</td>
<td>String</td>
<td>以逗号分隔的DML操作列表（INSERT，UPDATE，DELETE）如果未指定，则将执行复制所有DML操作的默认行为，如果指定，则仅捕获指定的操作</td>
</tr>
</tbody>
</table>
<p>以上为机翻，原说明文档地址见<br>
<a href="https://github.com/erdemcer/kafka-connect-oracle/blob/master/README.md">https://github.com/erdemcer/kafka-connect-oracle/blob/master/README.md</a></p>
<h3 id="添加kafka插件包">添加kafka插件包</h3>
<p><a href="https://www.jianguoyun.com/p/DSQTC_AQz5aGCRiHodMD"><mark>kafka-connect-oracle-1.0.68.jar</mark></a>和相应版本的oracle的驱动包放入到 $KAFKA_HOME/lib文件夹中</p>
<h3 id="启动插件">启动插件</h3>
<pre><code>cd $KAFKA_HOME
./bin/connect-standalone.sh ./config/connect-standalone.properties ./config/OracleSourceConnector.properties
</code></pre>
<h3 id="kafka消息说明">kafka消息说明</h3>
<table>
<thead>
<tr>
<th>字段</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>SCN</td>
<td>数据库日志时间</td>
</tr>
<tr>
<td>SEG_OWNER</td>
<td>数据库用户名</td>
</tr>
<tr>
<td>TABLE_NAME</td>
<td>数据库表名</td>
</tr>
<tr>
<td>TIMESTAMP</td>
<td>时间戳</td>
</tr>
<tr>
<td>SQL_REDO</td>
<td>执行的sql语句</td>
</tr>
<tr>
<td>OPERATION</td>
<td>操作</td>
</tr>
<tr>
<td>DATA</td>
<td>更新后的数据</td>
</tr>
<tr>
<td>BEFORE</td>
<td>更新前的数据</td>
</tr>
</tbody>
</table>
<h2 id="clickhouse-建表">ClickHouse 建表</h2>
<pre><code class="language-sql">CREATE TABLE default.ODB_LOG
(
SCN        Int64,
SEG_OWNER  String,
TABLE_NAME String,
`TIMESTAMP` DateTime,
SQL_REDO String,
OPERATION String,
 `DATA` String,
`BEFORE` String
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(TIMESTAMP)
ORDER BY  (SEG_OWNER,TABLE_NAME,OPERATION,TIMESTAMP )
SETTINGS index_granularity=8192;
</code></pre>
<h2 id="beam-程序">Beam 程序</h2>
<h3 id="maven添加-依赖包">maven添加 依赖包</h3>
<pre><code class="language-xml">        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.beam&lt;/groupId&gt;
            &lt;artifactId&gt;beam-sdks-java-io-clickhouse&lt;/artifactId&gt;
            &lt;version&gt;${beam.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.beam&lt;/groupId&gt;
            &lt;artifactId&gt;beam-sdks-java-io-kafka&lt;/artifactId&gt;
            &lt;version&gt;${beam.version}&lt;/version&gt;
        &lt;/dependency&gt;
         &lt;dependency&gt;
            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
            &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
            &lt;version&gt;2.4.1&lt;/version&gt;
            &lt;!--        &lt;scope&gt;runtime&lt;/scope&gt;--&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.beam&lt;/groupId&gt;
            &lt;artifactId&gt;beam-runners-flink-1.11&lt;/artifactId&gt;
            &lt;version&gt;${beam.version}&lt;/version&gt;
        &lt;/dependency&gt;

</code></pre>
<h3 id="创建配置参数类">创建配置参数类</h3>
<pre><code class="language-java">package com.yunzainfo.kol.config;

import org.apache.beam.runners.flink.FlinkPipelineOptions;
import org.apache.beam.sdk.options.Default;
import org.apache.beam.sdk.options.Description;
import org.apache.beam.sdk.options.PipelineOptions;
import org.apache.beam.sdk.options.Validation.Required;

/**
 * Created by IntelliJ IDEA
 * TODO: TODO
 *
 * @author: 徐成
 * Date: 2020/12/2
 * Time: 10:08 上午
 * Email: old_camel@163.com
 */
public interface KolOptions extends FlinkPipelineOptions {
//public interface OdcOptions extends PipelineOptions {
    @Description(&quot;kafka链接地址&quot;)
    @Required
    @Default.String(&quot;xxx.xxx.xxx.xxx:9092&quot;)
    String getBootstrapServers();
    void setBootstrapServers(String value);
    @Description(&quot;kafka 消息主题&quot;)
    @Required
    @Default.String(&quot;kol&quot;)
    String getTopic();
    void setTopic(String value);
    @Description(&quot;分组id&quot;)
    @Required
    @Default.String(&quot;bdf_kol&quot;)
    String getGroupid();
    void setGroupid(String value);
    @Description(&quot;kafka用户&quot;)
    @Required
    @Default.String(&quot;xxx&quot;)
    String getKafkaUsername();
    void setKafkaUsername(String kafkaUsername);
    @Description(&quot;kafka密码&quot;)
    @Required
    @Default.String(&quot;xxx&quot;)
    String getKafkaPassword();
    void setKafkaPassword(String kafkaPassword);
    @Description(&quot;ClickHouse连接地址&quot;)
    @Required
    @Default.String(&quot;xxx.xxx.xxx.xxx:xxxx/default&quot;)
    String getClickHouseUrl();
    void setClickHouseUrl(String value);
    @Description(&quot;ClickHouse用户名&quot;)
    @Required
    @Default.String(&quot;root&quot;)
    String getClickHouseUserName();
    void setClickHouseUserName(String value);
    @Description(&quot;ClickHouse密码&quot;)
    @Required
    @Default.String(&quot;xxx&quot;)
    String getClickHousePassword();
    void setClickHousePassword(String value);
    @Description(&quot;ClickHouse表&quot;)
    @Required
    @Default.String(&quot;ODB_LOG&quot;)
    String getClickHouseTableName();
    void setClickHouseTableName(String value);
}


</code></pre>
<h3 id="kafka消息映射类">kafka消息映射类</h3>
<pre><code class="language-java">package com.yunzainfo.kol.model;

import org.codehaus.jackson.annotate.JsonProperty;

import java.io.Serializable;
import java.util.Date;

/**
 * Created by IntelliJ IDEA
 * TODO: TODO
 *
 * @author: 徐成
 * Date: 2020/11/30
 * Time: 2:32 下午
 * Email: old_camel@163.com
 */
public class Payload implements Serializable {
    private static final long serialVersionUID = 1L;
    @JsonProperty(value = &quot;SCN&quot;)
    private Long SCN;
    @JsonProperty(value = &quot;SEG_OWNER&quot;)
    private String SEG_OWNER;
    @JsonProperty(value = &quot;TABLE_NAME&quot;)
    private String TABLE_NAME;
    @JsonProperty(value = &quot;TIMESTAMP&quot;)
    private Long TIMESTAMP;
    @JsonProperty(value = &quot;SQL_REDO&quot;)
    private String SQL_REDO;
    @JsonProperty(value = &quot;OPERATION&quot;)
    private String OPERATION;
    @JsonProperty(value = &quot;data&quot;)
    private Object DATA;
    @JsonProperty(value = &quot;before&quot;)
    private Object BEFORE;

    public static long getSerialVersionUID() {
        return serialVersionUID;
    }

    public Long getSCN() {
        return SCN;
    }

    public void setSCN(Long SCN) {
        this.SCN = SCN;
    }

    public String getSEG_OWNER() {
        return SEG_OWNER;
    }

    public void setSEG_OWNER(String SEG_OWNER) {
        this.SEG_OWNER = SEG_OWNER;
    }

    public String getTABLE_NAME() {
        return TABLE_NAME;
    }

    public void setTABLE_NAME(String TABLE_NAME) {
        this.TABLE_NAME = TABLE_NAME;
    }

    public Long getTIMESTAMP() {
        return TIMESTAMP;
    }

    public void setTIMESTAMP(Long TIMESTAMP) {
        this.TIMESTAMP = TIMESTAMP;
    }

    public String getSQL_REDO() {
        return SQL_REDO;
    }

    public void setSQL_REDO(String SQL_REDO) {
        this.SQL_REDO = SQL_REDO;
    }

    public String getOPERATION() {
        return OPERATION;
    }

    public void setOPERATION(String OPERATION) {
        this.OPERATION = OPERATION;
    }

    public Object getDATA() {
        return DATA;
    }

    public void setDATA(Object DATA) {
        this.DATA = DATA;
    }

    public Object getBEFORE() {
        return BEFORE;
    }

    public void setBEFORE(Object BEFORE) {
        this.BEFORE = BEFORE;
    }

    @Override
    public String toString() {
        return &quot;Payload{&quot; +
                &quot;SCN=&quot; + SCN +
                &quot;, SEG_OWNER='&quot; + SEG_OWNER + '\'' +
                &quot;, TABLE_NAME='&quot; + TABLE_NAME + '\'' +
                &quot;, TIMESTAMP=&quot; + TIMESTAMP +
                &quot;, SQL_REDO='&quot; + SQL_REDO + '\'' +
                &quot;, OPERATION='&quot; + OPERATION + '\'' +
                &quot;, DATA='&quot; + DATA + '\'' +
                &quot;, BEFORE='&quot; + BEFORE + '\'' +
                '}';
    }
}


</code></pre>
<h3 id="主程序">主程序</h3>
<pre><code class="language-java">package com.yunzainfo.kol;

import com.yunzainfo.kol.config.KolOptions;
import com.yunzainfo.kol.model.Payload;
import org.apache.beam.runners.flink.FlinkRunner;
import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.coders.SerializableCoder;
import org.apache.beam.sdk.io.clickhouse.ClickHouseIO;
import org.apache.beam.sdk.io.kafka.KafkaIO;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.schemas.Schema;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.transforms.Values;
import org.apache.beam.sdk.values.Row;
import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;
import org.apache.kafka.clients.CommonClientConfigs;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.config.SaslConfigs;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.codehaus.jackson.map.ObjectMapper;
import org.joda.time.DateTime;
import org.joda.time.Duration;

import java.io.IOException;
import java.util.LinkedHashMap;
import java.util.Map;

/**
 * Created by IntelliJ IDEA
 * TODO: TODO
 *
 * @author: 徐成
 * Date: 2020/11/27
 * Time: 5:49 下午
 * Email: old_camel@163.com
 */
public class KolApp {


    public static void main(String[] args) {
        KolOptions options = PipelineOptionsFactory.fromArgs(args).as(KolOptions.class);
        options.setRunner(FlinkRunner.class);
        runOdc(options);
    }

    public static Object isnull(Object o) {
        if (o == null) {
            return &quot;&quot;;
        } else {
            return o;
        }
    }

    public static void runOdc(KolOptions options) {
        Pipeline p = Pipeline.create(options);
        final Map&lt;String, Object&gt; immutableMap = new ImmutableMap.Builder&lt;String, Object&gt;()
                .put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;)
                .put(ConsumerConfig.GROUP_ID_CONFIG, options.getGroupid())
                .put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true)
                .put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, &quot;SASL_PLAINTEXT&quot;)
                .put(SaslConfigs.SASL_MECHANISM, &quot;PLAIN&quot;)
                .put(SaslConfigs.SASL_JAAS_CONFIG, &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;&quot; + options.getKafkaUsername() + &quot;\&quot; password=\&quot;&quot; + options.getKafkaPassword() + &quot;\&quot;;&quot;)
                .build();
        final Schema ckType = Schema.of(
                Schema.Field.of(&quot;SCN&quot;, Schema.FieldType.INT64.withNullable(true)),
                Schema.Field.of(&quot;SEG_OWNER&quot;, Schema.FieldType.STRING.withNullable(true)),
                Schema.Field.of(&quot;TABLE_NAME&quot;, Schema.FieldType.STRING.withNullable(true)),
                Schema.Field.of(&quot;TIMESTAMP&quot;, Schema.FieldType.DATETIME.withNullable(true)),
                Schema.Field.of(&quot;SQL_REDO&quot;, Schema.FieldType.STRING.withNullable(true)),
                Schema.Field.of(&quot;OPERATION&quot;, Schema.FieldType.STRING.withNullable(true)),
                Schema.Field.of(&quot;DATA&quot;, Schema.FieldType.STRING.withNullable(true)),
                Schema.Field.of(&quot;BEFORE&quot;, Schema.FieldType.STRING.withNullable(true))
        );
        p.apply(&quot;读取消息&quot;, KafkaIO.&lt;String, String&gt;read()
                .withBootstrapServers(options.getBootstrapServers())
                .withTopic(options.getTopic())
                .withKeyDeserializer(StringDeserializer.class)
                .withValueDeserializer(StringDeserializer.class)
                .withOffsetConsumerConfigOverrides(ImmutableMap.&lt;String, Object&gt;of(ConsumerConfig.GROUP_ID_CONFIG, options.getGroupid()))
                .withConsumerConfigUpdates(immutableMap)
                .withReadCommitted()
                .withoutMetadata())
                .apply(Values.create())
                .apply(&quot;转义Payload&quot;, ParDo.of(new DoFn&lt;String, Payload&gt;() {
                    private static final long serialVersionUID = 1L;

                    @ProcessElement
                    public void processElement(ProcessContext ctx) {
                        String element = ctx.element();
                        ObjectMapper mapper = new ObjectMapper();
                        try {
                            LinkedHashMap map = mapper.readValue(element, LinkedHashMap.class);                //Object payload = data.get(&quot;payload&quot;);
                            LinkedHashMap payloadMap = (LinkedHashMap) map.get(&quot;payload&quot;);
                            Payload payload = mapper.convertValue(payloadMap, Payload.class);
                            payload.setDATA(mapper.writeValueAsString(payload.getDATA()));
                            payload.setBEFORE(mapper.writeValueAsString(payload.getBEFORE()));
                            ctx.output(payload);
                        } catch (IOException e) {
                            e.printStackTrace();
                        }
                    }
                }))
                .setCoder(SerializableCoder.of(Payload.class))
                .apply(&quot;转换Row&quot;, ParDo.of(new DoFn&lt;Payload, Row&gt;() {
                    @ProcessElement
                    public void processElement(ProcessContext cxt) {
                        Payload payload = cxt.element();
                        Row alarmRow = Row.withSchema(ckType).addValues(
                                isnull(payload.getSCN()),
                                isnull(payload.getSEG_OWNER()),
                                isnull(payload.getTABLE_NAME()),
                                isnull(new DateTime(payload.getTIMESTAMP())),
                                isnull(payload.getSQL_REDO()),
                                isnull(payload.getOPERATION()),
                                isnull(payload.getDATA()),
                                isnull(payload.getBEFORE())
                        ).build();
                        cxt.output(alarmRow);
                    }
                })).setRowSchema(ckType)
                .apply(&quot;写入ClickHouse数据库&quot;,
                        ClickHouseIO.&lt;Row&gt;write(&quot;jdbc:clickhouse://&quot; + options.getClickHouseUrl() + &quot;?username=&quot; + options.getClickHouseUserName() + &quot;&amp;password=&quot; + options.getClickHousePassword(), options.getClickHouseTableName())
                                .withMaxRetries(3)//重试次数
                                .withInsertDeduplicate(true)//重复数据是否删除
                                .withMaxInsertBlockSize(1)//添加最大块的大小
                                .withInitialBackoff(Duration.standardSeconds(5))//初始退回时间
                                .withInsertDistributedSync(false)
                );
        p.run().waitUntilFinish();

    }
}

</code></pre>
<h2 id="把beam包放在flink上运行">把beam包放在flink上运行</h2>
<figure data-type="image" tabindex="1"><img src="https://oldcamel.run/post-images/1607602332596.png" alt="" loading="lazy"></figure>
<h2 id="clickhouse中的数据">clickhouse中的数据</h2>
<figure data-type="image" tabindex="2"><img src="https://oldcamel.run/post-images/1607602692592.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
</feed>