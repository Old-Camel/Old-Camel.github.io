{"posts":[{"title":"Kafka实现oracle的CDC数据实时变更","content":"使用工具 debezium-oracle-connector,oracle-LogMiner 1、oracle 设置 开启归档模式 alter system set db_recovery_file_dest_size = 10G; alter system set db_recovery_file_dest = '/home/oracle/oradta/recovery_area' scope=spfile; shutdown immediate startup mount alter database archivelog; alter database open; archive log list exit; ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS; 创建用户 CREATE USER myuser IDENTIFIED BY dbz DEFAULT TABLESPACE 命名空间 QUOTA UNLIMITED ON 命名空间 授权 GRANT CREATE SESSION TO myuser; GRANT CREATE TABLE TO myuser; GRANT CREATE SEQUENCE TO myuser; GRANT CREATE TRIGGER TO myuser; GRANT CREATE SESSION TO myuser; GRANT SELECT ON V_$DATABASE to myuser; GRANT FLASHBACK ANY TABLE TO myuser; GRANT SELECT ANY TABLE TO myuser; GRANT SELECT_CATALOG_ROLE TO myuser; GRANT EXECUTE_CATALOG_ROLE TO myuser; GRANT SELECT ANY TRANSACTION TO myuser; GRANT CREATE TABLE TO myuser; GRANT LOCK ANY TABLE TO myuser; GRANT ALTER ANY TABLE TO myuser; GRANT CREATE SEQUENCE TO myuser; GRANT EXECUTE ON DBMS_LOGMNR TO myuser; GRANT EXECUTE ON DBMS_LOGMNR_D TO myuser; GRANT SELECT ON V_$LOG TO myuser; GRANT SELECT ON V_$LOG_HISTORY TO myuser; GRANT SELECT ON V_$LOGMNR_LOGS TO myuser; GRANT SELECT ON V_$LOGMNR_CONTENTS TO myuser; GRANT SELECT ON V_$LOGMNR_PARAMETERS TO myuser; GRANT SELECT ON V_$LOGFILE TO myuser; GRANT SELECT ON V_$ARCHIVED_LOG TO myuser; GRANT SELECT ON V_$ARCHIVE_DEST_STATUS TO myuser; GRANT SELECT ON SYSTEM.LOGMNR_COL$ TO myuser; GRANT SELECT ON SYSTEM.LOGMNR_OBJ$ TO myuser; GRANT SELECT ON SYSTEM.LOGMNR_USER$ TO myuser; GRANT SELECT ON SYSTEM.LOGMNR_UID$ TO myuser; oracle客户端设置 下载 wget &quot;https://download.oracle.com/otn_software/linux/instantclient/19600/instantclient-basiclite-linux.x64-19.6.0.0.0dbru.zip&quot; -O /tmp/ic.zip； unzip /tmp/ic.zip -d 自定义目录 环境变量 设置LD_LIBRARY_PATH 指向 自定义目录 kafaka connect 插件设置 下载 wget &quot;https://oss.sonatype.org/service/local/artifact/maven/redirect?r=snapshots&amp;g=io.debezium&amp;a=debezium-connector-oracle&amp;v=LATEST&amp;c=plugin&amp;e=tar.gz&quot; -O /tmp/dbz-ora.tgz； tar -xvf /tmp/dbz-ora.tgz --directory kafaka 插件目录 添加 ojdbc jar到插件目录 curl https://maven.xwiki.org/externals/com/oracle/jdbc/ojdbc8/12.2.0.1/ojdbc8-12.2.0.1.jar -o ojdbc8-12.2.0.1.jar 创建kafka connect curl --location --request POST 'http://localhost:8083/connectors' \\ --header 'Content-Type: application/json' \\ --data-raw '{ &quot;name&quot;: &quot;name&quot;, &quot;config&quot;: { &quot;connector.class&quot; : &quot;io.debezium.connector.oracle.OracleConnector&quot;, &quot;tasks.max&quot; : &quot;1&quot;, &quot;database.server.name&quot; : &quot;topic&quot;, &quot;database.hostname&quot; : &quot;database_host&quot;, &quot;database.port&quot; : &quot;1521&quot;, &quot;database.user&quot; : &quot;myuser&quot;, &quot;database.password&quot; : &quot;dbz&quot;, &quot;database.dbname&quot; : &quot;orcl&quot;, &quot;database.tablename.case.insensitive&quot;: &quot;true&quot;, &quot;database.history.kafka.bootstrap.servers&quot; : &quot;localhost:9092&quot;, &quot;database.history.kafka.topic&quot;: &quot;name-h&quot;, &quot;table.include.list&quot;:&quot;AAA.BBB,CCC.DDD&quot;, &quot;database.history.producer.sasl.mechanism&quot;: &quot;PLAIN&quot;, &quot;database.history.producer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\&quot;admin\\&quot; password=\\&quot;password\\&quot;;&quot;, &quot;database.history.producer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;, &quot;database.history.consumer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\&quot;admin\\&quot; password=\\&quot;password\\&quot;;&quot;, &quot;database.history.consumer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;, &quot;database.history.consumer.sasl.mechanism&quot;: &quot;PLAIN&quot; } }' ","link":"https://oldcamel.run/post/kafka-shi-xian-oracle-de-cdc-shu-ju-shi-shi-bian-geng/"},{"title":"linux 安装服务器资源监控 prometheus远程写入","content":"1.安装prometheus wget -c https://github.com/prometheus/prometheus/releases/download/v2.23.0/prometheus-2.23.0.linux-amd64.tar.gz tar zxvf prometheus-2.23.0.linux-amd64.tar.gz -C /opt/ cd /opt/ ln -s prometheus-2.23.0.linux-amd64 prometheus cat &gt; /etc/systemd/system/prometheus.service &lt;&lt;EOF [Unit] Description=prometheus After=network.target [Service] Type=simple WorkingDirectory=/opt/prometheus ExecStart=/opt/prometheus/prometheus --config.file=/opt/prometheus/prometheus.yml LimitNOFILE=65536 PrivateTmp=true RestartSec=2 StartLimitInterval=0 Restart=always [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable prometheus systemctl start prometheus 2.配置Prometheus cat &gt; /opt/prometheus/prometheus.yml &lt;&lt;EOF # my global config global: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s). external_labels: tenant: &quot;140test&quot; remote_write: - url: &quot;url&quot; basic_auth: username: 用户名 password: 密码 # Alertmanager configuration alerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093 # Load rules once and periodically evaluate them according to the global 'evaluation_interval'. rule_files: # - &quot;first_rules.yml&quot; # - &quot;second_rules.yml&quot; # A scrape configuration containing exactly one endpoint to scrape: # Here it's Prometheus itself. scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'servers' file_sd_configs: - refresh_interval: 61s files: - /opt/prometheus/servers/*.json EOF 3.创建node配置文件 mkdir -p /opt/prometheus/servers cd /opt/prometheus/servers vim json.json [ { &quot;targets&quot;: [ “xxx.xxx.xxx.xxx:9100&quot; ], &quot;labels&quot;: { &quot;instance&quot;: &quot;xxx.xxx.xxx.xxx&quot;, &quot;job&quot;: &quot;node_exporter&quot; } } ] 安装node_exporter Wget https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-amd64.tar.gz tar zxvf node_exporter-1.0.1.linux-amd64.tar.gz -C /opt/ cd /opt/ ln -s node_exporter-1.0.1.linux-amd64 node_exporter cat &gt; /etc/systemd/system/node_exporter.service &lt;&lt;EOF [Unit] Description=node_exporter After=network.target [Service] Type=simple WorkingDirectory=/opt/node_exporter ExecStart=/opt/node_exporter/node_exporter LimitNOFILE=65536 PrivateTmp=true RestartSec=2 StartLimitInterval=0 Restart=always [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable node_exporter systemctl start node_exporter systemctl restart prometheus 查看服务日志命令 journalctl -u prometheus ","link":"https://oldcamel.run/post/linux-an-zhuang-fu-wu-qi-zi-yuan-jian-kong-prometheus-yuan-cheng-xie-ru/"},{"title":"Kafka实现sqlserver的CDC数据实时变更","content":"安装sqlserver #!/bin/bash -e # Password for the SA user (required) # 设置密码 MSSQL_SA_PASSWORD='test@12345678' # Product ID of the version of SQL server you're installing # Must be evaluation, developer, express, web, standard, enterprise, or your 25 digit product key # Defaults to developer # 选择版本，有多种版本可供原则，具体版本标识符可自行百度 MSSQL_PID='evaluation' # Install SQL Server Agent (recommended) SQL_ENABLE_AGENT='y' # Install SQL Server Full Text Search (optional) # SQL_INSTALL_FULLTEXT='y' # Create an additional user with sysadmin privileges (optional) # 新建一个额外添加的用户，（可选） # SQL_INSTALL_USER='&lt;Username&gt;' # SQL_INSTALL_USER_PASSWORD='&lt;YourStrong!Passw0rd&gt;' if [ -z $MSSQL_SA_PASSWORD ] then echo Environment variable MSSQL_SA_PASSWORD must be set for unattended install exit 1 fi # --------------------------- 远程拉包 并安装的过程 echo Adding Microsoft repositories... sudo curl -o /etc/yum.repos.d/mssql-server.repo https://packages.microsoft.com/config/rhel/7/mssql-server-2017.repo sudo curl -o /etc/yum.repos.d/msprod.repo https://packages.microsoft.com/config/rhel/7/prod.repo echo Installing SQL Server... sudo yum install -y mssql-server # --------------------------- 下面给出 本地离线rpm包安装部分脚本 echo ：Local Installing SQL Server... sudo yum localinstall -y ./sqlserver2017.rpm # 执行sqlserver的配置 echo Running mssql-conf setup... sudo MSSQL_SA_PASSWORD=$MSSQL_SA_PASSWORD \\ MSSQL_PID=$MSSQL_PID \\ /opt/mssql/bin/mssql-conf -n setup accept-eula echo Installing mssql-tools and unixODBC developer... sudo ACCEPT_EULA=Y yum install -y mssql-tools unixODBC-devel # Add SQL Server tools to the path by default: echo Adding SQL Server tools to your path... echo PATH=&quot;$PATH:/opt/mssql-tools/bin&quot; &gt;&gt; ~/.bash_profile echo 'export PATH=&quot;$PATH:/opt/mssql-tools/bin&quot;' &gt;&gt; ~/.bashrc source ~/.bashrc # Optional Enable SQL Server Agent : if [ ! -z $SQL_ENABLE_AGENT ] then echo Enable SQL Server Agent... sudo /opt/mssql/bin/mssql-conf set sqlagent.enabled true sudo systemctl restart mssql-server fi # Optional SQL Server Full Text Search installation: if [ ! -z $SQL_INSTALL_FULLTEXT ] then echo Installing SQL Server Full-Text Search... sudo yum install -y mssql-server-fts fi # Configure firewall to allow TCP port 1433: # 配置防火墙放行1433端口，懒省事直接关掉防火墙亦可 echo Configuring firewall to allow traffic on port 1433... sudo firewall-cmd --zone=public --add-port=1433/tcp --permanent sudo firewall-cmd --reload # Example of setting post-installation configuration options # Set trace flags 1204 and 1222 for deadlock tracing: #echo Setting trace flags... #sudo /opt/mssql/bin/mssql-conf traceflag 1204 1222 on # Restart SQL Server after making configuration changes: # 重启 echo Restarting SQL Server... sudo systemctl restart mssql-server # Connect to server and get the version: # 官方给出的测试链接脚本 counter=1 errstatus=1 while [ $counter -le 5 ] &amp;&amp; [ $errstatus = 1 ] do echo Waiting for SQL Server to start... sleep 5s /opt/mssql-tools/bin/sqlcmd \\ -S localhost \\ -U SA \\ -P $MSSQL_SA_PASSWORD \\ -Q &quot;SELECT @@VERSION&quot; 2&gt;/dev/null errstatus=$? ((counter++)) done # Display error if connection failed: if [ $errstatus = 1 ] then echo Cannot connect to SQL Server, installation aborted exit $errstatus fi # Optional new user creation: if [ ! -z $SQL_INSTALL_USER ] &amp;&amp; [ ! -z $SQL_INSTALL_USER_PASSWORD ] then echo Creating user $SQL_INSTALL_USER /opt/mssql-tools/bin/sqlcmd \\ -S localhost \\ -U SA \\ -P $MSSQL_SA_PASSWORD \\ -Q &quot;CREATE LOGIN [$SQL_INSTALL_USER] WITH PASSWORD=N'$SQL_INSTALL_USER_PASSWORD', DEFAULT_DATABASE=[master], CHECK_EXPIRATION=ON, CHECK_POLICY=ON; ALTER SERVER ROLE [sysadmin] ADD MEMBER [$SQL_INSTALL_USER]&quot; fi echo Done! 运行脚本等着安装完成 开启表的cdc 创建shihu数据库 建测试表 IF EXISTS (SELECT * FROM sys.all_objects WHERE object_id = OBJECT_ID(N'[dbo].[dt_test]') AND type IN ('U')) DROP TABLE [dbo].[dt_test] GO CREATE TABLE [dbo].[dt_test] ( [tid] int NOT NULL, [tname] nvarchar(200) COLLATE SQL_Latin1_General_CP1_CI_AS NULL, [tidcard] nvarchar(200) COLLATE SQL_Latin1_General_CP1_CI_AS NULL, [tbirthday] date NULL, [tmobile] nvarchar(200) COLLATE SQL_Latin1_General_CP1_CI_AS NULL, [temail] nvarchar(200) COLLATE SQL_Latin1_General_CP1_CI_AS NULL, [tgender] bigint NULL, [tcreate_time] datetime NULL ) GO ALTER TABLE [dbo].[dt_test] SET (LOCK_ESCALATION = TABLE) GO -- ---------------------------- -- Primary Key structure for table dt_test -- ---------------------------- ALTER TABLE [dbo].[dt_test] ADD CONSTRAINT [PK__dt_test__DC105B0FA908205A] PRIMARY KEY CLUSTERED ([tid]) WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] GO 开启cdc USE shihu GO EXEC sys.sp_cdc_enable_db GO USE shihu GO EXEC sys.sp_cdc_enable_table @source_schema = 'dbo', @source_name = ‘dt_test’, @role_name = NULL, @filegroup_name = 'PRIMARY', @supports_net_changes = 1 GO 下载kafka插件 官方文档 https://debezium.io/documentation/reference/1.3/connectors/sqlserver.html 下载 https://repo1.maven.org/maven2/io/debezium/debezium-connector-sqlserver/1.2.5.Final/debezium-connector-sqlserver-1.2.5.Final-plugin.tar.gz 创建插件目录 在kafka跟目录创建 kafka_connect_plugins 目录 将下载的包解压到此目录中 修改kafka/config/connect-distribute.properties bootstrap.servers=IP:9092 plugin.path= /kafka路径/kafka_connect_plugins 启动kafka-connect ./bin/connect-distributed.sh config/connect-distributed.properties &gt;logs/ksc.log &amp; 开启成功的标志,postman可以使用访问端口8083（默认的端口号,可以在connect-distributed.properties 更改） rest api GET /connectors – 返回所有正在运行的connector名 POST /connectors – 新建一个connector; 请求体必须是json格式并且需要包含name字段和config字段，name是connector的名字，config是json格式，必须包含你的connector的配置信息。 GET /connectors/{name} – 获取指定connetor的信息 GET /connectors/{name}/config – 获取指定connector的配置信息 PUT /connectors/{name}/config – 更新指定connector的配置信息 GET /connectors/{name}/status – 获取指定connector的状态，包括它是否在运行、停止、或者失败，如果发生错误，还会列出错误的具体信息。 GET /connectors/{name}/tasks – 获取指定connector正在运行的task。 GET /connectors/{name}/tasks/{taskid}/status – 获取指定connector的task的状态信息 PUT /connectors/{name}/pause – 暂停connector和它的task，停止数据处理知道它被恢复。 PUT /connectors/{name}/resume – 恢复一个被暂停的connector POST /connectors/{name}/restart – 重启一个connector，尤其是在一个connector运行失败的情况下比较常用 POST /connectors/{name}/tasks/{taskId}/restart – 重启一个task，一般是因为它运行失败才这样做。 DELETE /connectors/{name} – 删除一个connector，停止它的所有task并删除配置。 添加sqlserver connect curl --location --request POST 'http://localhost:8083/connectors' \\ --header 'Content-Type: application/json' \\ --data-raw '{ &quot;name&quot;: &quot;sqlserver178-connector&quot;, &quot;config&quot;: { &quot;connector.class&quot;: &quot;io.debezium.connector.sqlserver.SqlServerConnector&quot;, &quot;database.hostname&quot;: &quot;sqlserver的ip&quot;, &quot;database.port&quot;: &quot;1433&quot;, &quot;database.user&quot;: &quot;sa&quot;, &quot;database.password&quot;: &quot;test@12345678&quot;, &quot;database.dbname&quot;: &quot;shihu&quot;, &quot;database.server.name&quot;: &quot;fullfillment&quot;, &quot;table.whitelist&quot;: &quot;dbo.dt_test&quot;, &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka的IP:9092&quot;, &quot;database.history.kafka.topic&quot;: &quot;dbhistory.fullfillment&quot;, &quot;database.history.producer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;, &quot;database.history.producer.sasl.mechanism&quot;: &quot;PLAIN&quot;, &quot;database.history.producer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\&quot;admin\\&quot; password=\\&quot;123\\&quot;;&quot;, &quot;database.history.consumer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;, &quot;database.history.consumer.sasl.mechanism&quot;: &quot;PLAIN&quot;, &quot;database.history.consumer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\&quot;admin\\&quot; password=\\&quot;123\\&quot;;&quot; } }' 消费信息 consumer.preperties配置 bootstrap.servers=localhost:9092 group.id=test-consumer-group security.protocol=SASL_PLAINTEXT sasl.mechanism=PLAIN 修改器kafka-console-consumer.sh为kafka-console-consumer-saal.sh倒数第二行添加 export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/kafka_2.13-2.6.0/config/kafka_client_jaas.conf&quot; 启动 ./bin/kafka-console-consumer-saal.sh --bootstrap-server localhost:9092 --topic fullfillment.dbo.dt_test --consumer.config config/consumer.properties 消息示例 { &quot;schema&quot;:{ &quot;type&quot;:&quot;struct&quot;, &quot;fields&quot;:Array[6], &quot;optional&quot;:false, &quot;name&quot;:&quot;fullfillment.dbo.dt_test.Envelope&quot; }, &quot;payload&quot;:{ &quot;before&quot;:{ &quot;tid&quot;:1, &quot;tname&quot;:&quot;222&quot;, &quot;tidcard&quot;:&quot;2&quot;, &quot;tbirthday&quot;:-25567, &quot;tmobile&quot;:&quot;1&quot;, &quot;temail&quot;:&quot;1&quot;, &quot;tgender&quot;:2, &quot;tcreate_time&quot;:-2208988800000 }, &quot;after&quot;:{ &quot;tid&quot;:1, &quot;tname&quot;:&quot;test&quot;, &quot;tidcard&quot;:&quot;2&quot;, &quot;tbirthday&quot;:-25567, &quot;tmobile&quot;:&quot;1&quot;, &quot;temail&quot;:&quot;1&quot;, &quot;tgender&quot;:2, &quot;tcreate_time&quot;:-2208988800000 }, &quot;source&quot;:Object{...}, &quot;op&quot;:&quot;u&quot;, &quot;ts_ms&quot;:1610019731750, &quot;transaction&quot;:null } } oracle示例 curl --location --request POST 'http://localhost:8083/connectors/' \\ --header 'Content-Type: application/json' \\ --data-raw ' { &quot;name&quot;: &quot;oracle-61-connector&quot;, &quot;config&quot;: { &quot;connector.class&quot;: &quot;com.ecer.kafka.connect.oracle.OracleSourceConnector&quot;, &quot;db.name.alias&quot;: &quot;61test&quot;, &quot;tasks.max&quot;: 1, &quot;topic&quot;: &quot;kol&quot;, &quot;db.name&quot;: &quot;portaldb&quot;, &quot;db.hostname&quot;: &quot;oralce连接host地址&quot;, &quot;db.port&quot;: 1521, &quot;db.user&quot;: &quot;info&quot;, &quot;db.user.password&quot;: &quot;111111&quot;, &quot;db.fetch.size&quot;: 1, &quot;table.whitelist&quot;: &quot;INFO.*,SPAUTH.*,WORKFLOW.*,FLOWABLE.*,OA.*&quot;, &quot;table.blacklist&quot;: &quot;&quot;, &quot;parse.dml.data&quot;: true, &quot;reset.offset&quot;: false, &quot;multitenant&quot;: false } } mysql示例 curl --location --request POST 'http://localhost:8083/connectors/' \\ --header 'Content-Type: application/json' \\ --data-raw ' { &quot;name&quot;: &quot;140mysql-connector&quot;, &quot;config&quot;: { &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;, &quot;database.hostname&quot;: &quot;localhost&quot;, &quot;database.port&quot;: &quot;3306&quot;, &quot;database.user&quot;: &quot;root&quot;, &quot;database.password&quot;: &quot;111111&quot;, &quot;database.server.id&quot;: &quot;184054&quot;, &quot;database.server.name&quot;: &quot;fullfillment140ms&quot;, &quot;database.whitelist&quot;:&quot;test&quot;, &quot;database.history.kafka.bootstrap.servers&quot;: &quot;localhost:9092&quot;, &quot;database.history.kafka.topic&quot;: &quot;dbhistory.fullfillment140ms&quot;, &quot;include.schema.changes&quot;: &quot;false&quot;, &quot;database.history.producer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;, &quot;database.history.producer.sasl.mechanism&quot;: &quot;PLAIN&quot;, &quot;database.history.producer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\&quot;admin\\&quot; password=\\&quot;123\\&quot;;&quot;, &quot;database.history.consumer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;, &quot;database.history.consumer.sasl.mechanism&quot;: &quot;PLAIN&quot;, &quot;database.history.consumer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\&quot;admin\\&quot; password=\\&quot;123\\&quot;;&quot; } } 查看topic list bin/kafka-topics.sh --zookeeper localhost --list ","link":"https://oldcamel.run/post/kafka-shi-xian-sqlserver-de-cdc-shu-ju-shi-shi-bian-geng/"},{"title":"Kafka开启SASL用户名密码认证","content":"创建kafka_server_jaas.conf文件 config目录下创建kafka_server_jaas.conf文件 KafkaServer { org.apache.kafka.common.security.plain.PlainLoginModule required username=&quot;admin&quot; password=&quot;123&quot; user_admin=&quot;123&quot; user_yunzai=&quot;123&quot;; }; 创建kafka_client_jaas.conf文件 config目录下创建kafka_client_jaas.conf文件 KafkaClient { org.apache.kafka.common.security.plain.PlainLoginModule required username=&quot;admin&quot; password=&quot;123&quot;; }; 修改server.properties listeners=SASL_PLAINTEXT://IP:9092 advertised.listeners=SASL_PLAINTEXT://IP:9092 security.inter.broker.protocol=SASL_PLAINTEXT sasl.enabled.mechanisms=PLAIN sasl.mechanism.inter.broker.protocol=PLAIN authorizer.class.name = kafka.security.auth.SimpleAclAuthorizer super.users=User:admin; 修改kafaka启动脚本 修改bin目录下kafka_start.sh 在倒数第二行添加kafka_server_jaas.conf的全路径 export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/kafka_2.13-2.6.0/config/kafka_server_jaas.conf&quot; （可选）为特定用户添加特定topic的acl授权 如下表示为用户yunzai添加topic nginx-log的读写权限。 ./bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 -add --allow-principal User:yunzai --operation Read --operation Write --topic testyunzai 验证授权 ./bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --list --topic testyunzai 生产者消费者验证 生产者 1.以用户yunzai为例，创建JAAS认证文件yunzai_jaas.conf放在config目录下，内容如下: KafkaClient { org.apache.kafka.common.security.plain.PlainLoginModule required username=&quot;yunzai&quot; password=&quot;123&quot;; }; 2.拷贝bin/kafka-console-producer.sh为bin/yunzai-kafka-console-producer.sh，并将JAAS文件作为一个JVM参数传给console producer 倒数第二行添加 export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/kafka_2.13-2.6.0/config/yunzai_jaas.conf&quot; 3.创建文件producer.config指定如下属性： security.protocol=SASL_PLAINTEXT sasl.mechanism=PLAIN 4.启动producer ./bin/yunzai-kafka-console-producer.sh --broker-list IP:9092 --topic testyunzai --producer.config producer.config 消费者 1.以用户yunzai为例，创建JAAS认证文件yunzai_jaas.conf放在config目录下，如果用户跟生产者是同一个，可以复用上面生产者的JAAS文件，内容如下: KafkaClient { org.apache.kafka.common.security.plain.PlainLoginModule required username=&quot;yunzai&quot; password=&quot;123&quot;; }; 2.拷贝bin/kafka-console-consumer.sh为bin/yunzai-kafka-console-consumer.sh，并将JAAS文件作为一个JVM参数传给console consumer 倒数第二行添加 export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/kafka_2.13-2.6.0/config/yunzai_jaas.conf&quot; 3.创建文件consumer.config指定如下属性： security.protocol=SASL_PLAINTEXT sasl.mechanism=PLAIN group.id=test 4.启动consumer ./bin/yunzai-kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic testyunzai --from-beginning --consumer.config consumer.config java beam客户端认证 beam设置 final Map&lt;String, Object&gt; immutableMap = new ImmutableMap.Builder&lt;String, Object&gt;() .put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;) .put(ConsumerConfig.GROUP_ID_CONFIG, options.getGroupid()) .put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true) .put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, &quot;SASL_PLAINTEXT&quot;) .put(SaslConfigs.SASL_MECHANISM, &quot;PLAIN&quot;) .put(SaslConfigs.SASL_JAAS_CONFIG, &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\&quot;&quot; + options.getKafkaUsername() + &quot;\\&quot; password=\\&quot;&quot; + options.getKafkaPassword() + &quot;\\&quot;;&quot;) .build(); ","link":"https://oldcamel.run/post/kafka-kai-qi-sasl-yong-hu-ming-mi-ma-ren-zheng/"},{"title":"VictoriaMetrics 汇总多个Prometheus指标（四）读取Istio指标","content":"istio中默认带了prometheus,里面包含istio代理的指标，现需要将istio中的指标配置在kube-prometheus中。 修改prometheus-k8s 角色权限 修改prometheus-clusterRole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: prometheus-k8s rules: - apiGroups: - &quot;&quot; resources: - nodes - services - endpoints - pods - nodes/proxy verbs: - get - watch - list - apiGroups: - &quot;&quot; resources: - configmaps verbs: - get - nonResourceURLs: - /metrics verbs: - get 配置数据平面的服务监控 apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: prometheus-oper-istio-controlplane spec: jobLabel: istio selector: matchExpressions: - {key: istio, operator: In, values: [mixer,pilot,galley,citadel,sidecar-injector]} namespaceSelector: any: true endpoints: - port: http-monitoring interval: 15s - port: http-policy-monitoring interval: 15s 配置数据层的服务监控 apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: prometheus-oper-istio-dataplane labels: monitoring: istio-dataplane spec: selector: matchExpressions: - {key: istio-prometheus-ignore, operator: DoesNotExist} namespaceSelector: any: true jobLabel: envoy-stats endpoints: - path: /stats/prometheus targetPort: http-envoy-prom interval: 15s 查看是否生效 VictoriaMetrics 中不同集群中的指标获取 ","link":"https://oldcamel.run/post/victoriametrics-hui-zong-duo-ge-prometheus-zhi-biao-san-du-qu-istio-zhi-biao/"},{"title":"VictoriaMetrics 汇总多个Prometheus指标（三）kube-prometheus安装","content":"kube-prometheus是Prometheus Operator 针对kubernetes的监控组件。 github地址 https://github.com/prometheus-operator/kube-prometheus 配置远程写入 basic用户名密码配置 secret apiVersion: v1 kind: Secret metadata: name: vmuser namespace: monitoring type: kubernetes.io/basic-auth stringData: username: username password: password 添加远程写入 和外部标签 修改prometheus-prometheus.yaml文件，添加 prometheusExternalLabelName: &quot;&quot; replicaExternalLabelName: &quot;&quot; externalLabels: tenant: &quot;租户名称&quot; remoteWrite: - url: http://VictoriaMetrics连接地址/api/v1/write basicAuth: username: key: username name: vmuser password: key: password name: vmuser ","link":"https://oldcamel.run/post/victoriametrics-hui-zong-duo-ge-prometheus-zhi-biao-san-kube-prometheus-an-zhuang/"},{"title":"VictoriaMetrics 汇总多个Prometheus指标（二）Grafana安装","content":"安装grafana 从victoriametrics中读取汇总的指标 安装mysql 下载离线包 下载地址 解压安装 rpm -Uvh *.rpm --nodeps --force 修改配置文件 修改 /etc/my.cnf 添加 lower_case_table_names=1 default-storage-engine=INNODB 生成用户不带密码 mysqld --initialize-insecure 如果要重新初始化，必须先清空data文件夹 启动mysql systemctl start mysqld 进入mysql mysql -u root 修改密码+开启远程访问 set password for root@localhost = password('123456'); GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456' WITH GRANT OPTION; FLUSH PRIVILEGES; 创建grafana数据库 CREATE DATABASE `grafana` DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; 安装grafana 下载安装 参考文档 wget https://dl.grafana.com/oss/release/grafana-7.3.6-1.x86_64.rpm sudo yum install grafana-7.3.6-1.x86_64.rpm 修改配置文件 vim /etc/grafana/grafana.ini domain修改 domain = 修改成主机IP 默认用户名密码修改 admin_user = admin admin_password = 123456 数据库修改 type = mysql host = 127.0.0.1:3306 name = grafana user = root password =123456 启动grafana服务 service grafana-server start 配置 victoriametrics连接 访问 ip:3000 添加prometheus数据源 设置为 victoriametrics主机ip 端口 8428 配置victoriametrics官方的监控图表 grafana lab地址 ","link":"https://oldcamel.run/post/grafana-an-zhuang/"},{"title":"VictoriaMetrics 汇总多个Prometheus指标（一）安装VictoriaMetrics","content":"victoriametrics 用来收集k8s集群中的prometheus指标信息 。 github地址 官方文档地址 下载二级制安装包 启动主进程 运行端口8428 ./victoria-metrics-prod -retentionPeriod=3 -selfScrapeInterval=10s &amp; 启动认证代理 运行端口8427 ./vmauth-prod -auth.config=/opt/vm/config/config.yml &amp; basic认证配置文件 users: - username: &quot;username&quot; password: &quot;password&quot; url_prefix: &quot;http://localhost:8428&quot; 运行端口 ","link":"https://oldcamel.run/post/victoriametrics-an-zhuang/"},{"title":"flowable修改流程缓存实现自由流程","content":"\b flowable中会对流程定义做缓存处理，在实现自由流程的时候需要动态给流程定义添加节点，默认的是对流程定义做的缓存，要想在每个流程实例中动态添加节点，可以通过修改流程缓存，添加每个流程实例的缓存 涉及到的类 主要步骤 步骤1：修改BpmnJsonConverter,添加自定义属性，区分流程是否为自由流程 步骤2：设置全局的FlowableEventListener( FlowableEngineEventType. ACTIVITY_COMPLETED ),在流程启动的时候判断流程是否为自由流程，如果是的话，把流程的bpmnModel转成xml设置到自己定义的缓存对象中，然后保存到redis里。 Redis采用了Hash方式存储，key为流程实例ID,value为自定义缓存类。 步骤3：自由流程添加节点的时候提前把流程实例id放在request作用域中 步骤4：定义 DefaultDeploymentCache 子类重写get方法，如果是自定义流程并且request作用域中有流程实例id就中redis中获取流程定义json转换成 ProcessDefinitionCacheEntry 步骤5：定义自由流程命令类，获取流程定义缓存，给里面添加连线和节点 步骤6：往新加的节点上跳转 放入缓存主要代码 if (FlowUtils.isFreeProcess(entity.getProcessDefinitionId())) { //放入缓存 ProcessDefinitionCacheEntry processDefinitionCacheEntry = managementService.executeCommand(new GetProcessDefinitionCacheEntryCmd(entity.getProcessDefinitionId())); CustomProcessDefinitionCacheEntry customProcessDefinitionCacheEntry = FlowUtils.parseCustomProcessDefinitionCacheEntry(processDefinitionCacheEntry); FreeProcessCaChe cache = SpringUtil.getBean(FreeProcessCaChe.class); cache.add(entity.getProcessInstanceId(), customProcessDefinitionCacheEntry); } 下方为以上用到的类以及方法 获取缓存命令类 public class GetProcessDefinitionCacheEntryCmd implements Command&lt;ProcessDefinitionCacheEntry&gt; { protected String processDefinitionId; public GetProcessDefinitionCacheEntryCmd(String processDefinitionId) { this.processDefinitionId = processDefinitionId; } @Override public ProcessDefinitionCacheEntry execute(CommandContext commandContext) { DeploymentManager deploymentManager = CommandContextUtil.getProcessEngineConfiguration().getDeploymentManager(); ProcessDefinitionCacheEntry processDefinitionCacheEntry = deploymentManager.getProcessDefinitionCache() .get(processDefinitionId); return processDefinitionCacheEntry; } } 序列化缓存对象方法 public static CustomProcessDefinitionCacheEntry parseCustomProcessDefinitionCacheEntry(ProcessDefinitionCacheEntry processDefinitionCacheEntry) { CustomProcessDefinitionCacheEntry customProcessDefinitionCacheEntry = new CustomProcessDefinitionCacheEntry(); ProcessDefinition processDefinition = processDefinitionCacheEntry.getProcessDefinition(); String resourceName = processDefinition.getResourceName(); String deploymentId = processDefinition.getDeploymentId(); BpmnModel bpmnModel = processDefinitionCacheEntry.getBpmnModel(); BpmnXMLConverter bpmnXMLConverter = new BpmnXMLConverter(); byte[] bytes = bpmnXMLConverter.convertToXML(bpmnModel); customProcessDefinitionCacheEntry.setBpmnModel(bytes); customProcessDefinitionCacheEntry.setDeploymentId(deploymentId); customProcessDefinitionCacheEntry.setResourceName(resourceName); return customProcessDefinitionCacheEntry; } Redis存储的对象 @Data @AllArgsConstructor @NoArgsConstructor public class CustomProcessDefinitionCacheEntry implements Serializable { private static final long serialVersionUID = 6833801933658529071L; protected String deploymentId; protected String resourceName; protected byte[] bpmnModel; } 添加到redis接口 public interface FreeProcessCaChe { CustomProcessDefinitionCacheEntry get(String key); void set(String key,CustomProcessDefinitionCacheEntry process); boolean contains(String key); void add(String key, CustomProcessDefinitionCacheEntry process); void remove(String key); void clear(); } 添加到redis接口实现 @Service public class FreeProcessCaCheImpl implements FreeProcessCaChe { private String hashkey = &quot;freeprocesscache&quot;; @PostConstruct public void init() { boolean b = redisUtil.hasKey(hashkey); if (!b) { redisUtil.hset(hashkey, &quot;init&quot;, &quot;init&quot;); } } @Autowired private RedisUtil redisUtil; @Override public CustomProcessDefinitionCacheEntry get(String key) { if (StringUtils.isBlank(key)) { return null; } Object value = redisUtil.hget(hashkey, key); if (value != null) { JSONObject jb = (JSONObject) value; CustomProcessDefinitionCacheEntry customProcessDefinitionCacheEntry = JSON.toJavaObject(jb, CustomProcessDefinitionCacheEntry.class); return customProcessDefinitionCacheEntry; } return null; } @Override public boolean contains(String key) { return redisUtil.hHasKey(hashkey, key); } @Override public void add(String key, CustomProcessDefinitionCacheEntry process) { boolean hset = redisUtil.hset(hashkey, key, process); if (!hset) { throw new CheckErrorException(&quot;缓存自由流程信息失败&quot;); } } @Override public void set(String key, CustomProcessDefinitionCacheEntry process) { boolean hset = redisUtil.hset(hashkey, key, process); if (!hset) { throw new CheckErrorException(&quot;更新自由流程信息失败&quot;); } } @Override public void remove(String key) { redisUtil.hdel(hashkey, key); } @Override public void clear() { redisUtil.hmset(hashkey, Maps.newHashMap()); } } 修改流程配置类添加自定义缓存获取 springProcessEngineConfiguration.setProcessDefinitionCache(new CustomDeploymentCache&lt;&gt;()); 设置的自定义缓存获取类（主要是重写获取缓存的get方法，以下仅为参考） public class CustomDeploymentCache&lt;T&gt; extends DefaultDeploymentCache&lt;T&gt; { @Override public T get(String id) { T t = super.get(id); if(t==null){ return t; } String processInstanceId=null; try { processInstanceId = (String) FlowUtils.getRequest().getAttribute(&quot;processInstanceId&quot;); if (StringUtils.isBlank(processInstanceId)) { return t; } }catch (Exception e){ return t; } if(t instanceof ProcessDefinitionCacheEntry) { JSONObject jsonObject = new JSONObject(); Process mainProcess = ((ProcessDefinitionCacheEntry) t).getBpmnModel().getMainProcess(); String processGlobelSettings = &quot;processGlobelSettings&quot;; List&lt;ExtensionElement&gt; extensionElements = mainProcess.getExtensionElements().get(processGlobelSettings); if (extensionElements != null &amp;&amp; (!extensionElements.isEmpty())) { ExtensionElement extensionElement = extensionElements.get(0); String elementText = extensionElement.getElementText(); jsonObject = JSON.parseObject(elementText, Feature.OrderedField); } boolean freeProcess=jsonObject.getBooleanValue(&quot;freeProcess&quot;); if (!freeProcess) { return super.get(id); } else { //从缓存中取值 FreeProcessCaChe cache = SpringUtil.getBean(FreeProcessCaChe.class); CustomProcessDefinitionCacheEntry redisProcessDefinitionCacheEntry = cache.get(processInstanceId); BpmnModel bpmnModel=null; if(redisProcessDefinitionCacheEntry==null){ return t; }else { try { bpmnModel = FlowUtils.parseBpmnModelFromCustomProcessDefinitionCacheEntry(redisProcessDefinitionCacheEntry); } catch (Exception e) { e.printStackTrace(); } } Process process = bpmnModel.getMainProcess(); RuntimeService runtimeService = SpringUtil.getBean(RuntimeService.class); RepositoryService repositoryService = SpringUtil.getBean(RepositoryService.class); ProcessInstance processInstance = runtimeService.createProcessInstanceQuery().processInstanceId(processInstanceId).singleResult(); String processDefinitionId = processInstance.getProcessDefinitionId(); ProcessDefinition dataProcessDefinition = null; if(t!=null){ ProcessDefinitionCacheEntry pdc=(ProcessDefinitionCacheEntry)t; dataProcessDefinition=pdc.getProcessDefinition(); }else{ dataProcessDefinition=repositoryService.createProcessDefinitionQuery().processDefinitionId(processDefinitionId).singleResult(); } ProcessDefinitionCacheEntry customProcessDefinitionCacheEntry = new ProcessDefinitionCacheEntry(dataProcessDefinition,bpmnModel,process); return (T) customProcessDefinitionCacheEntry; } }else{ return super.get(id); } } } ","link":"https://oldcamel.run/post/flowable-xiu-gai-liu-cheng-huan-cun-shi-xian-zi-you-liu-cheng/"},{"title":"flowable获取当前任务节点下一步会创建的用户任务","content":"获取方法 package xxxx.util; import com.alibaba.fastjson.JSON; import com.alibaba.fastjson.JSONArray; import com.alibaba.fastjson.JSONObject; import com.alibaba.fastjson.serializer.SerializerFeature; import com.fasterxml.jackson.databind.JsonNode; import com.fasterxml.jackson.databind.ObjectMapper; import com.greenpineyu.fel.FelEngine; import com.greenpineyu.fel.FelEngineImpl; import com.greenpineyu.fel.context.FelContext; import org.apache.commons.lang.StringUtils; import org.apache.commons.lang3.BooleanUtils; import org.flowable.bpmn.converter.BpmnXMLConverter; import org.flowable.bpmn.model.*; import org.flowable.bpmn.model.Process; import org.flowable.engine.HistoryService; import org.flowable.engine.RepositoryService; import org.flowable.engine.RuntimeService; import org.flowable.engine.TaskService; import org.flowable.engine.history.HistoricProcessInstance; import org.flowable.engine.impl.bpmn.parser.BpmnParse; import org.flowable.engine.impl.bpmn.parser.BpmnParser; import org.flowable.engine.impl.cfg.ProcessEngineConfigurationImpl; import org.flowable.engine.impl.persistence.deploy.ProcessDefinitionCacheEntry; import org.flowable.engine.impl.persistence.entity.DeploymentEntity; import org.flowable.engine.impl.persistence.entity.DeploymentEntityManager; import org.flowable.engine.repository.ProcessDefinition; import org.flowable.bpmn.model.Task; import org.flowable.ui.modeler.domain.Model; import org.flowable.ui.modeler.serviceapi.ModelService; import org.springframework.web.context.request.RequestContextHolder; import org.springframework.web.context.request.ServletRequestAttributes; import javax.servlet.http.HttpServletRequest; import java.io.ByteArrayInputStream; import java.io.IOException; import java.sql.ResultSet; import java.sql.ResultSetMetaData; import java.util.*; public class FlowUtils { /** * 获取下一步骤的用户任务 * * @param repositoryService * @param taskService * @param map * @return */ public static List&lt;UserTask&gt; getNextUserTasks(RepositoryService repositoryService, TaskService taskService, org.flowable.task.api.Task task, Map&lt;String, Object&gt; map) { List&lt;UserTask&gt; data = new ArrayList&lt;&gt;(); ProcessDefinition processDefinition = repositoryService.createProcessDefinitionQuery().processDefinitionId(task.getProcessDefinitionId()).singleResult(); BpmnModel bpmnModel = repositoryService.getBpmnModel(processDefinition.getId()); Process mainProcess = bpmnModel.getMainProcess(); Collection&lt;FlowElement&gt; flowElements = mainProcess.getFlowElements(); String key = task.getTaskDefinitionKey(); FlowElement flowElement = bpmnModel.getFlowElement(key); next(flowElements, flowElement, map, data); return data; } public static void next(Collection&lt;FlowElement&gt; flowElements, FlowElement flowElement, Map&lt;String, Object&gt; map, List&lt;UserTask&gt; nextUser) { //如果是结束节点 if (flowElement instanceof EndEvent) { //如果是子任务的结束节点 if (getSubProcess(flowElements, flowElement) != null) { flowElement = getSubProcess(flowElements, flowElement); } } //获取Task的出线信息--可以拥有多个 List&lt;SequenceFlow&gt; outGoingFlows = null; if (flowElement instanceof Task) { outGoingFlows = ((Task) flowElement).getOutgoingFlows(); } else if (flowElement instanceof Gateway) { outGoingFlows = ((Gateway) flowElement).getOutgoingFlows(); } else if (flowElement instanceof StartEvent) { outGoingFlows = ((StartEvent) flowElement).getOutgoingFlows(); } else if (flowElement instanceof SubProcess) { outGoingFlows = ((SubProcess) flowElement).getOutgoingFlows(); } else if (flowElement instanceof CallActivity) { outGoingFlows = ((CallActivity) flowElement).getOutgoingFlows(); } if (outGoingFlows != null &amp;&amp; outGoingFlows.size() &gt; 0) { //遍历所有的出线--找到可以正确执行的那一条 for (SequenceFlow sequenceFlow : outGoingFlows) { //1.有表达式，且为true //2.无表达式 String expression = sequenceFlow.getConditionExpression(); if (expression == null || Boolean.valueOf( String.valueOf( result(map, expression.substring(expression.lastIndexOf(&quot;{&quot;) + 1, expression.lastIndexOf(&quot;}&quot;)))))) { //出线的下一节点 String nextFlowElementID = sequenceFlow.getTargetRef(); if (checkSubProcess(nextFlowElementID, flowElements, nextUser)) { continue; } //查询下一节点的信息 FlowElement nextFlowElement = getFlowElementById(nextFlowElementID, flowElements); //调用流程 if (nextFlowElement instanceof CallActivity) { CallActivity ca = (CallActivity) nextFlowElement; if (ca.getLoopCharacteristics() != null) { UserTask userTask = new UserTask(); userTask.setId(ca.getId()); userTask.setId(ca.getId()); userTask.setLoopCharacteristics(ca.getLoopCharacteristics()); userTask.setName(ca.getName()); nextUser.add(userTask); } next(flowElements, nextFlowElement, map, nextUser); } //用户任务 if (nextFlowElement instanceof UserTask) { nextUser.add((UserTask) nextFlowElement); } //排他网关 else if (nextFlowElement instanceof ExclusiveGateway) { next(flowElements, nextFlowElement, map, nextUser); } //并行网关 else if (nextFlowElement instanceof ParallelGateway) { next(flowElements, nextFlowElement, map, nextUser); } //接收任务 else if (nextFlowElement instanceof ReceiveTask) { next(flowElements, nextFlowElement, map, nextUser); } //服务任务 else if (nextFlowElement instanceof ServiceTask) { next(flowElements, nextFlowElement, map, nextUser); } //子任务的起点 else if (nextFlowElement instanceof StartEvent) { next(flowElements, nextFlowElement, map, nextUser); } //结束节点 else if (nextFlowElement instanceof EndEvent) { next(flowElements, nextFlowElement, map, nextUser); } } } } } /** * 判断是否是多实例子流程并且需要设置集合类型变量 */ public static boolean checkSubProcess(String Id, Collection&lt;FlowElement&gt; flowElements, List&lt;UserTask&gt; nextUser) { for (FlowElement flowElement1 : flowElements) { if (flowElement1 instanceof SubProcess &amp;&amp; flowElement1.getId().equals(Id)) { SubProcess sp = (SubProcess) flowElement1; if (sp.getLoopCharacteristics() != null) { String inputDataItem = sp.getLoopCharacteristics().getInputDataItem(); UserTask userTask = new UserTask(); userTask.setId(sp.getId()); userTask.setLoopCharacteristics(sp.getLoopCharacteristics()); userTask.setName(sp.getName()); nextUser.add(userTask); return true; } } } return false; } /** * 查询一个节点的是否子任务中的节点，如果是，返回子任务 * * @param flowElements 全流程的节点集合 * @param flowElement 当前节点 * @return */ public static FlowElement getSubProcess(Collection&lt;FlowElement&gt; flowElements, FlowElement flowElement) { for (FlowElement flowElement1 : flowElements) { if (flowElement1 instanceof SubProcess) { for (FlowElement flowElement2 : ((SubProcess) flowElement1).getFlowElements()) { if (flowElement.equals(flowElement2)) { return flowElement1; } } } } return null; } /** * 根据ID查询流程节点对象, 如果是子任务，则返回子任务的开始节点 * * @param Id 节点ID * @param flowElements 流程节点集合 * @return */ public static FlowElement getFlowElementById(String Id, Collection&lt;FlowElement&gt; flowElements) { for (FlowElement flowElement : flowElements) { if (flowElement.getId().equals(Id)) { //如果是子任务，则查询出子任务的开始节点 if (flowElement instanceof SubProcess) { return getStartFlowElement(((SubProcess) flowElement).getFlowElements()); } return flowElement; } if (flowElement instanceof SubProcess) { FlowElement flowElement1 = getFlowElementById(Id, ((SubProcess) flowElement).getFlowElements()); if (flowElement1 != null) { return flowElement1; } } } return null; } /** * 返回流程的开始节点 * * @param flowElements 节点集合 * @description: */ public static FlowElement getStartFlowElement(Collection&lt;FlowElement&gt; flowElements) { for (FlowElement flowElement : flowElements) { if (flowElement instanceof StartEvent) { return flowElement; } } return null; } /** * 校验el表达示例 * * @param map * @param expression * @return */ public static Object result(Map&lt;String, Object&gt; map, String expression) { FelEngine fel = new FelEngineImpl(); FelContext ctx = fel.getContext(); for (Map.Entry&lt;String, Object&gt; entry : map.entrySet()) { ctx.set(entry.getKey(), entry.getValue()); } Object result = fel.eval(expression); return result; } } maven el表达式校验的依赖 &lt;dependency&gt; &lt;groupId&gt;org.eweb4j&lt;/groupId&gt; &lt;artifactId&gt;fel&lt;/artifactId&gt; &lt;version&gt;0.8&lt;/version&gt; &lt;/dependency&gt; ","link":"https://oldcamel.run/post/flowable-nexttask/"},{"title":" Beam+Kafka同步Oracle日志到ClickHouse","content":"用kafka的oracle connect插件存储oracle日志，用beam读取kafka数据插入到clickhouse中 oracle配置 archivelog模式 数据库必须处于archivelog模式，并且必须启用补充日志记录 在数据库服务器上执行 sqlplus / as sysdba SQL&gt;shutdown immediate SQL&gt;startup mount SQL&gt;alter database archivelog; SQL&gt;alter database open; 启用补充日志记录 sqlplus / as sysdba SQL&gt;alter database add supplemental log data (all) columns; kafka配置 插件配置文件 在$KAFKA_HOME/config 目录新建OracleSourceConnector.properties配置文件， 文件内示例配置 name=oracle-logminer-connector connector.class=com.ecer.kafka.connect.oracle.OracleSourceConnector db.name.alias=test tasks.max=1 topic=cdctest db.name=testdb db.hostname=10.1.X.X db.port=1521 db.user=kminer db.user.password=kminerpass db.fetch.size=1 table.whitelist=TEST.*,TEST2.TABLE2 table.blacklist=TEST2.TABLE3 parse.dml.data=true reset.offset=false multitenant=false 配置说明 Name Type Description name String 连接器名称 connector.class String 此连接器的Java类的名称 db.name.alias String 数据库的标识符名称（例如Test，Dev，Prod）或用于标识数据库的特定名称该名称将用作主题和架构名称的标头 tasks.max Integer 创建的最大任务数此连接器使用单个任务. topic String 消息将写入的主题的名称如果设置了值，则所有消息都将写入此声明的topic，如果未设置，则将为每个数据库表动态创建一个主题 db.name String 要连接的数据库的服务名称或sid通常使用数据库服务名称 db.hostname String Oracle数据库服务器的IP地址或主机名 db.port Integer Oracle数据库服务器的端口号 db.user String 数据库的用户名 db.user.password String 数据库用户密码 db.fetch.size Integer 此配置属性设置Oracle行提取大小值 table.whitelist String 白名单格式为用户名.表名用逗号分隔，如果要收集用户下的所有的表，用用户名.* parse.dml.data Boolean 如果为true，则将捕获的sql DML语句解析为字段和值;如果为false，则仅发布sql DML语句 reset.offset Boolean 如果为true，则在连接器启动时将偏移值设置为数据库的当前SCN如果为false，则连接器将从上一个偏移值开始 start.scn Long 如果设置此属性，则将偏移值设置为该指定值，并且logminer将从此SCN启动如果连接器希望从所需的SCN启动，则可以使用此属性 multitenant Boolean 如果为true，则启用多租户支持如果为false，将使用单实例配置 table.blacklist String 黑名单格式为用户名.表名用逗号分隔，如果要收集用户下的所有的表，用用户名.*. dml.types String 以逗号分隔的DML操作列表（INSERT，UPDATE，DELETE）如果未指定，则将执行复制所有DML操作的默认行为，如果指定，则仅捕获指定的操作 以上为机翻，原说明文档地址见 https://github.com/erdemcer/kafka-connect-oracle/blob/master/README.md 添加kafka插件包 kafka-connect-oracle-1.0.68.jar和相应版本的oracle的驱动包放入到 $KAFKA_HOME/lib文件夹中 启动插件 cd $KAFKA_HOME ./bin/connect-standalone.sh ./config/connect-standalone.properties ./config/OracleSourceConnector.properties kafka消息说明 字段 说明 SCN 数据库日志时间 SEG_OWNER 数据库用户名 TABLE_NAME 数据库表名 TIMESTAMP 时间戳 SQL_REDO 执行的sql语句 OPERATION 操作 DATA 更新后的数据 BEFORE 更新前的数据 ClickHouse 建表 CREATE TABLE default.ODB_LOG ( SCN Int64, SEG_OWNER String, TABLE_NAME String, `TIMESTAMP` DateTime, SQL_REDO String, OPERATION String, `DATA` String, `BEFORE` String ) ENGINE = MergeTree() PARTITION BY toYYYYMM(TIMESTAMP) ORDER BY (SEG_OWNER,TABLE_NAME,OPERATION,TIMESTAMP ) SETTINGS index_granularity=8192; Beam 程序 maven添加 依赖包 &lt;dependency&gt; &lt;groupId&gt;org.apache.beam&lt;/groupId&gt; &lt;artifactId&gt;beam-sdks-java-io-clickhouse&lt;/artifactId&gt; &lt;version&gt;${beam.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.beam&lt;/groupId&gt; &lt;artifactId&gt;beam-sdks-java-io-kafka&lt;/artifactId&gt; &lt;version&gt;${beam.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.4.1&lt;/version&gt; &lt;!-- &lt;scope&gt;runtime&lt;/scope&gt;--&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.beam&lt;/groupId&gt; &lt;artifactId&gt;beam-runners-flink-1.11&lt;/artifactId&gt; &lt;version&gt;${beam.version}&lt;/version&gt; &lt;/dependency&gt; 创建配置参数类 package com.yunzainfo.kol.config; import org.apache.beam.runners.flink.FlinkPipelineOptions; import org.apache.beam.sdk.options.Default; import org.apache.beam.sdk.options.Description; import org.apache.beam.sdk.options.PipelineOptions; import org.apache.beam.sdk.options.Validation.Required; /** * Created by IntelliJ IDEA * TODO: TODO * * @author: 徐成 * Date: 2020/12/2 * Time: 10:08 上午 * Email: old_camel@163.com */ public interface KolOptions extends FlinkPipelineOptions { //public interface OdcOptions extends PipelineOptions { @Description(&quot;kafka链接地址&quot;) @Required @Default.String(&quot;xxx.xxx.xxx.xxx:9092&quot;) String getBootstrapServers(); void setBootstrapServers(String value); @Description(&quot;kafka 消息主题&quot;) @Required @Default.String(&quot;kol&quot;) String getTopic(); void setTopic(String value); @Description(&quot;分组id&quot;) @Required @Default.String(&quot;bdf_kol&quot;) String getGroupid(); void setGroupid(String value); @Description(&quot;kafka用户&quot;) @Required @Default.String(&quot;xxx&quot;) String getKafkaUsername(); void setKafkaUsername(String kafkaUsername); @Description(&quot;kafka密码&quot;) @Required @Default.String(&quot;xxx&quot;) String getKafkaPassword(); void setKafkaPassword(String kafkaPassword); @Description(&quot;ClickHouse连接地址&quot;) @Required @Default.String(&quot;xxx.xxx.xxx.xxx:xxxx/default&quot;) String getClickHouseUrl(); void setClickHouseUrl(String value); @Description(&quot;ClickHouse用户名&quot;) @Required @Default.String(&quot;root&quot;) String getClickHouseUserName(); void setClickHouseUserName(String value); @Description(&quot;ClickHouse密码&quot;) @Required @Default.String(&quot;xxx&quot;) String getClickHousePassword(); void setClickHousePassword(String value); @Description(&quot;ClickHouse表&quot;) @Required @Default.String(&quot;ODB_LOG&quot;) String getClickHouseTableName(); void setClickHouseTableName(String value); } kafka消息映射类 package com.yunzainfo.kol.model; import org.codehaus.jackson.annotate.JsonProperty; import java.io.Serializable; import java.util.Date; /** * Created by IntelliJ IDEA * TODO: TODO * * @author: 徐成 * Date: 2020/11/30 * Time: 2:32 下午 * Email: old_camel@163.com */ public class Payload implements Serializable { private static final long serialVersionUID = 1L; @JsonProperty(value = &quot;SCN&quot;) private Long SCN; @JsonProperty(value = &quot;SEG_OWNER&quot;) private String SEG_OWNER; @JsonProperty(value = &quot;TABLE_NAME&quot;) private String TABLE_NAME; @JsonProperty(value = &quot;TIMESTAMP&quot;) private Long TIMESTAMP; @JsonProperty(value = &quot;SQL_REDO&quot;) private String SQL_REDO; @JsonProperty(value = &quot;OPERATION&quot;) private String OPERATION; @JsonProperty(value = &quot;data&quot;) private Object DATA; @JsonProperty(value = &quot;before&quot;) private Object BEFORE; public static long getSerialVersionUID() { return serialVersionUID; } public Long getSCN() { return SCN; } public void setSCN(Long SCN) { this.SCN = SCN; } public String getSEG_OWNER() { return SEG_OWNER; } public void setSEG_OWNER(String SEG_OWNER) { this.SEG_OWNER = SEG_OWNER; } public String getTABLE_NAME() { return TABLE_NAME; } public void setTABLE_NAME(String TABLE_NAME) { this.TABLE_NAME = TABLE_NAME; } public Long getTIMESTAMP() { return TIMESTAMP; } public void setTIMESTAMP(Long TIMESTAMP) { this.TIMESTAMP = TIMESTAMP; } public String getSQL_REDO() { return SQL_REDO; } public void setSQL_REDO(String SQL_REDO) { this.SQL_REDO = SQL_REDO; } public String getOPERATION() { return OPERATION; } public void setOPERATION(String OPERATION) { this.OPERATION = OPERATION; } public Object getDATA() { return DATA; } public void setDATA(Object DATA) { this.DATA = DATA; } public Object getBEFORE() { return BEFORE; } public void setBEFORE(Object BEFORE) { this.BEFORE = BEFORE; } @Override public String toString() { return &quot;Payload{&quot; + &quot;SCN=&quot; + SCN + &quot;, SEG_OWNER='&quot; + SEG_OWNER + '\\'' + &quot;, TABLE_NAME='&quot; + TABLE_NAME + '\\'' + &quot;, TIMESTAMP=&quot; + TIMESTAMP + &quot;, SQL_REDO='&quot; + SQL_REDO + '\\'' + &quot;, OPERATION='&quot; + OPERATION + '\\'' + &quot;, DATA='&quot; + DATA + '\\'' + &quot;, BEFORE='&quot; + BEFORE + '\\'' + '}'; } } 主程序 package com.yunzainfo.kol; import com.yunzainfo.kol.config.KolOptions; import com.yunzainfo.kol.model.Payload; import org.apache.beam.runners.flink.FlinkRunner; import org.apache.beam.sdk.Pipeline; import org.apache.beam.sdk.coders.SerializableCoder; import org.apache.beam.sdk.io.clickhouse.ClickHouseIO; import org.apache.beam.sdk.io.kafka.KafkaIO; import org.apache.beam.sdk.options.PipelineOptionsFactory; import org.apache.beam.sdk.schemas.Schema; import org.apache.beam.sdk.transforms.DoFn; import org.apache.beam.sdk.transforms.ParDo; import org.apache.beam.sdk.transforms.Values; import org.apache.beam.sdk.values.Row; import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap; import org.apache.kafka.clients.CommonClientConfigs; import org.apache.kafka.clients.consumer.ConsumerConfig; import org.apache.kafka.common.config.SaslConfigs; import org.apache.kafka.common.serialization.StringDeserializer; import org.codehaus.jackson.map.ObjectMapper; import org.joda.time.DateTime; import org.joda.time.Duration; import java.io.IOException; import java.util.LinkedHashMap; import java.util.Map; /** * Created by IntelliJ IDEA * TODO: TODO * * @author: 徐成 * Date: 2020/11/27 * Time: 5:49 下午 * Email: old_camel@163.com */ public class KolApp { public static void main(String[] args) { KolOptions options = PipelineOptionsFactory.fromArgs(args).as(KolOptions.class); options.setRunner(FlinkRunner.class); runOdc(options); } public static Object isnull(Object o) { if (o == null) { return &quot;&quot;; } else { return o; } } public static void runOdc(KolOptions options) { Pipeline p = Pipeline.create(options); final Map&lt;String, Object&gt; immutableMap = new ImmutableMap.Builder&lt;String, Object&gt;() .put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;) .put(ConsumerConfig.GROUP_ID_CONFIG, options.getGroupid()) .put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true) .put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, &quot;SASL_PLAINTEXT&quot;) .put(SaslConfigs.SASL_MECHANISM, &quot;PLAIN&quot;) .put(SaslConfigs.SASL_JAAS_CONFIG, &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\&quot;&quot; + options.getKafkaUsername() + &quot;\\&quot; password=\\&quot;&quot; + options.getKafkaPassword() + &quot;\\&quot;;&quot;) .build(); final Schema ckType = Schema.of( Schema.Field.of(&quot;SCN&quot;, Schema.FieldType.INT64.withNullable(true)), Schema.Field.of(&quot;SEG_OWNER&quot;, Schema.FieldType.STRING.withNullable(true)), Schema.Field.of(&quot;TABLE_NAME&quot;, Schema.FieldType.STRING.withNullable(true)), Schema.Field.of(&quot;TIMESTAMP&quot;, Schema.FieldType.DATETIME.withNullable(true)), Schema.Field.of(&quot;SQL_REDO&quot;, Schema.FieldType.STRING.withNullable(true)), Schema.Field.of(&quot;OPERATION&quot;, Schema.FieldType.STRING.withNullable(true)), Schema.Field.of(&quot;DATA&quot;, Schema.FieldType.STRING.withNullable(true)), Schema.Field.of(&quot;BEFORE&quot;, Schema.FieldType.STRING.withNullable(true)) ); p.apply(&quot;读取消息&quot;, KafkaIO.&lt;String, String&gt;read() .withBootstrapServers(options.getBootstrapServers()) .withTopic(options.getTopic()) .withKeyDeserializer(StringDeserializer.class) .withValueDeserializer(StringDeserializer.class) .withOffsetConsumerConfigOverrides(ImmutableMap.&lt;String, Object&gt;of(ConsumerConfig.GROUP_ID_CONFIG, options.getGroupid())) .withConsumerConfigUpdates(immutableMap) .withReadCommitted() .withoutMetadata()) .apply(Values.create()) .apply(&quot;转义Payload&quot;, ParDo.of(new DoFn&lt;String, Payload&gt;() { private static final long serialVersionUID = 1L; @ProcessElement public void processElement(ProcessContext ctx) { String element = ctx.element(); ObjectMapper mapper = new ObjectMapper(); try { LinkedHashMap map = mapper.readValue(element, LinkedHashMap.class); //Object payload = data.get(&quot;payload&quot;); LinkedHashMap payloadMap = (LinkedHashMap) map.get(&quot;payload&quot;); Payload payload = mapper.convertValue(payloadMap, Payload.class); payload.setDATA(mapper.writeValueAsString(payload.getDATA())); payload.setBEFORE(mapper.writeValueAsString(payload.getBEFORE())); ctx.output(payload); } catch (IOException e) { e.printStackTrace(); } } })) .setCoder(SerializableCoder.of(Payload.class)) .apply(&quot;转换Row&quot;, ParDo.of(new DoFn&lt;Payload, Row&gt;() { @ProcessElement public void processElement(ProcessContext cxt) { Payload payload = cxt.element(); Row alarmRow = Row.withSchema(ckType).addValues( isnull(payload.getSCN()), isnull(payload.getSEG_OWNER()), isnull(payload.getTABLE_NAME()), isnull(new DateTime(payload.getTIMESTAMP())), isnull(payload.getSQL_REDO()), isnull(payload.getOPERATION()), isnull(payload.getDATA()), isnull(payload.getBEFORE()) ).build(); cxt.output(alarmRow); } })).setRowSchema(ckType) .apply(&quot;写入ClickHouse数据库&quot;, ClickHouseIO.&lt;Row&gt;write(&quot;jdbc:clickhouse://&quot; + options.getClickHouseUrl() + &quot;?username=&quot; + options.getClickHouseUserName() + &quot;&amp;password=&quot; + options.getClickHousePassword(), options.getClickHouseTableName()) .withMaxRetries(3)//重试次数 .withInsertDeduplicate(true)//重复数据是否删除 .withMaxInsertBlockSize(1)//添加最大块的大小 .withInitialBackoff(Duration.standardSeconds(5))//初始退回时间 .withInsertDistributedSync(false) ); p.run().waitUntilFinish(); } } 把beam包放在flink上运行 clickhouse中的数据 ","link":"https://oldcamel.run/post/apache-beam-tong-bu-oracle-ri-zhi-dao-clickhouse/"}]}