{"posts":[{"title":"4.0 æ–°çš„å¼€ç«¯","content":"åˆåˆåˆåˆå¼€å§‹æ­å»ºæ–°æ¡†æ¶äº†,è¿™æ¬¡å¼€æå¤šç§Ÿæˆ·äº‘æœåŠ¡ã€‚ åˆæ­¥è®¡åˆ’åˆ—è¡¨ stars-spring-boot-starter å¤šæ•°æ®æº jpa+mybatis stars-spring-boot-starter mybatis æ¢æˆmybatisplus stars-spring-boot-starter liquibaseåˆæ­¥é›†æˆï¼ˆå¤šæ•°æ®æºåœ¨å¯åŠ¨æ—¶åˆ·æ–°åº“ï¼‰ è®¤è¯ä¸­å¿ƒ å‡çº§ 2.5.0 spring-cloud-oauth2æ¢æˆ spring-security oauth2 è®¤è¯ä¸­å¿ƒ jpaåœ¨pgsqlé‡Œè¿è¡Œ, è°ƒè¯•åŠŸèƒ½ stars-spring-boot-starter æ·»åŠ é€šç”¨è®¾ç½®+å·¥å…·ç±»+pomä¾èµ–ï¼ˆä¾èµ–é¡¹ç›®pomç²¾ç®€ï¼‰ stars-archetype mavenæ¨¡æ¿é¡¹ç›®é‡æ–°ä¿®æ”¹ k8s rookå®‰è£…cephfsï¼Œæµ‹è¯•pvcåŠ¨æ€åŠ¨æ€æŒ‚è½½ï¼Œs3å‚¨å­˜ pgsql helmç‰ˆæœ¬ åŸºäºmasteræ•°æ®æºåšç§Ÿæˆ·ç®¡ç†åŠŸèƒ½ã€‚ç§Ÿæˆ·æ•°æ®è°ƒæ•´-&gt;åŠ¨æ€å¢å‡æ•°æ®æº-&gt;liquibaseåˆå§‹åŒ–åº“ï¼ˆå‘å¸ƒè®¢é˜…ï¼‰ æ–‡ä»¶ä¸­å¿ƒ mongoå­˜å‚¨ä¿®æ”¹ä¸º cephfså­˜å‚¨ ï¼ˆminioï¼‰ æ¶ˆæ¯ä¸­å¿ƒç­‰å…¶ä»–åŸºç¡€ç»„ä»¶ä¿®æ”¹ cephfs å¤‡ä»½è„šæœ¬ stars-spring-boot-starter å¤šæ•°æ®æº jpa+mybatis å¤šç§Ÿæˆ·ç³»ç»Ÿ,æ•°æ®åº“æŒ‰ç§Ÿæˆ·åŒºåˆ†æ•°æ®æºã€‚è®¡åˆ’æ¯ä¸ªç§Ÿæˆ·å•ç‹¬æ•°æ®æºéƒ¨ç½²ï¼Œæ‰€ä»¥å…ˆæä¸ªå¯åŠ¨å™¨é…ç½®å¤šæ•°æ®æºã€‚ å¤šæ•°æ®æºå®ç°:ç»§æ‰¿AbstractRoutingDataSourceï¼Œé‡å†™determineCurrentLookupKeyæ–¹æ³• è¿”å›è‡ªå®šä¹‰çš„keyã€‚è¿™ä¸ªkey ç”¨æ‹¦æˆªå™¨åœ¨è¯·æ±‚å¤´é‡Œè·å–ç”¨æˆ·çš„ç§Ÿæˆ·å€¼æ”¾åˆ°ä¸€ä¸ªçº¿ç¨‹å˜é‡é‡Œã€‚ æ‹¦æˆªå™¨ public class TenantInterceptor implements HandlerInterceptor { final String X_TENANT_ID = &quot;X_TenantID&quot;; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { final String tenantId = request.getHeader(X_TENANT_ID); if (tenantId != null) { DynamicDataSourceContextHolder.setDataSourceKey(tenantId); } return true; } @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { DynamicDataSourceContextHolder.clearDataSourceKey(); } } @Configuration public class TenantConfig implements WebMvcConfigurer { @Override public void addInterceptors(InterceptorRegistry registry) { //æŒ‡å®šæ‹¦æˆªå™¨ï¼ŒæŒ‡å®šæ‹¦æˆªè·¯å¾„ registry.addInterceptor(new TenantInterceptor()).addPathPatterns(&quot;/**&quot;); } } çº¿ç¨‹å˜é‡ public class DynamicDataSourceContextHolder { private static final ThreadLocal&lt;String&gt; contextHolder = new ThreadLocal&lt;String&gt;() { /** * å°† master æ•°æ®æºçš„ keyä½œä¸ºé»˜è®¤æ•°æ®æºçš„ key */ @Override protected String initialValue() { return &quot;master&quot;; } }; /** * æ•°æ®æºçš„ keyé›†åˆï¼Œç”¨äºåˆ‡æ¢æ—¶åˆ¤æ–­æ•°æ®æºæ˜¯å¦å­˜åœ¨ */ public static Set&lt;Object&gt; dataSourceKeys = new HashSet&lt;&gt;(); /** * åˆ‡æ¢æ•°æ®æº * @param key æ•°æ®æº */ public static void setDataSourceKey(String key) { if (StringUtils.isNotBlank(key)) { contextHolder.set(key); } } /** * è·å–æ•°æ®æº * @return */ public static String getDataSourceKey() { return contextHolder.get(); } /** * é‡ç½®æ•°æ®æº */ public static void clearDataSourceKey() { contextHolder.remove(); } /** * åˆ¤æ–­æ˜¯å¦åŒ…å«æ•°æ®æº * @param key æ•°æ®æº * @return */ public static boolean containDataSourceKey(String key) { return dataSourceKeys.contains(key); } /** * æ·»åŠ æ•°æ®æºKeys * @param keys * @return */ public static boolean addDataSourceKeys(Collection&lt;? extends Object&gt; keys) { return dataSourceKeys.addAll(keys); } } é…ç½®æ•°æ®æº @Bean(&quot;master&quot;) @ConfigurationProperties(prefix = &quot;spring.datasource.hikari&quot;) public DataSource master(){ HikariDataSource build = (HikariDataSource)DataSourceBuilder.create().build(); build.setTransactionIsolation(&quot;TRANSACTION_READ_COMMITTED&quot;); return build; } @Bean @Primary public DataSource dataSource(){ DynamicDataSource dynamicDataSource = new DynamicDataSource(); HashMap&lt;Object, Object&gt; dataSourceMap = new HashMap&lt;&gt;(); dataSourceMap.put(&quot;master&quot;,master()); dynamicDataSource.setDefaultDataSource(master()); dynamicDataSource.setDataSources(dataSourceMap); dynamicDataSource.afterPropertiesSet(); return dynamicDataSource; } å¤šæ•°æ®æºè¯»å– @Data public class DynamicDataSource extends AbstractRoutingDataSource { private final Map&lt;Object, Object&gt; tenantDataSources = new ConcurrentHashMap&lt;&gt;(); @Override protected Object determineCurrentLookupKey() { return DynamicDataSourceContextHolder.getDataSourceKey(); } public void setDefaultDataSource(Object defaultDataSource) { super.setDefaultTargetDataSource(defaultDataSource); } public void setDataSources(Map&lt;Object, Object&gt; dataSources) { tenantDataSources.putAll(dataSources); super.setTargetDataSources(tenantDataSources); super.afterPropertiesSet(); DynamicDataSourceContextHolder.addDataSourceKeys(dataSources.keySet()); } } åˆ›å»ºç§Ÿæˆ·è¡¨ å†™ApplicationRunneråœ¨é¡¹ç›®å¯åŠ¨å®Œæˆåæ·»åŠ å¤šç§Ÿæˆ·æ•°æ®æº public class DynamicDataSourceInit implements ApplicationRunner { @Autowired DynamicDataSource dataSource; @Override public void run(ApplicationArguments args) throws Exception { HikariDataSource hikariDataSource = (HikariDataSource) SpringContextUtils.getBean(&quot;master&quot;); Map&lt;Object, Object&gt; resolvedDataSources =new HashMap&lt;&gt;(); Connection connection = hikariDataSource.getConnection(); Statement statement = connection.createStatement(); ResultSet resultSet = statement.executeQuery(&quot;select tenant_id \\&quot;tenantId\\&quot;,tenant_name \\&quot;tenantName\\&quot;,datasource_url \\&quot;datasourceUrl\\&quot;,datasource_username \\&quot;datasourceUsername\\&quot;,datasource_password \\&quot;datasourcePassword\\&quot;,datasource_driver \\&quot;datasourceDriver\\&quot; from tenant where status=1&quot;); List&lt;Tenant&gt; tenants = (ArrayList&lt;Tenant&gt;) this.populate(resultSet, Tenant.class); for (Tenant tenant : tenants) { HikariDataSource dataSource = new HikariDataSource(); dataSource.setDriverClassName(tenant.getDatasourceDriver()); dataSource.setJdbcUrl(tenant.getDatasourceUrl()); dataSource.setUsername(tenant.getDatasourceUsername()); dataSource.setPassword(tenant.getDatasourcePassword()); dataSource.setConnectionTestQuery(&quot;SELECT 1&quot;); dataSource.setTransactionIsolation(&quot;TRANSACTION_READ_COMMITTED&quot;); dataSource.setDataSourceProperties(hikariDataSource.getDataSourceProperties()); resolvedDataSources.put(tenant.getTenantId(), dataSource); } dataSource.setDataSources(resolvedDataSources); dataSource.afterPropertiesSet(); connection.close(); } private List populate(ResultSet rs , Class clazz) throws SQLException, InstantiationException, IllegalAccessException{ //ç»“æœé›†çš„å…ƒç´ å¯¹è±¡ ResultSetMetaData rsmd = rs.getMetaData(); //è·å–ç»“æœé›†çš„å…ƒç´ ä¸ªæ•° int colCount = rsmd.getColumnCount(); //è¿”å›ç»“æœçš„åˆ—è¡¨é›†åˆ List list = new ArrayList(); //ä¸šåŠ¡å¯¹è±¡çš„å±æ€§æ•°ç»„ Field[] fields = clazz.getDeclaredFields(); while(rs.next()){//å¯¹æ¯ä¸€æ¡è®°å½•è¿›è¡Œæ“ä½œ Object obj = clazz.newInstance();//æ„é€ ä¸šåŠ¡å¯¹è±¡å®ä½“ //å°†æ¯ä¸€ä¸ªå­—æ®µå–å‡ºè¿›è¡Œèµ‹å€¼ for(int i = 1;i&lt;=colCount;i++){ Object value = rs.getObject(i); //å¯»æ‰¾è¯¥åˆ—å¯¹åº”çš„å¯¹è±¡å±æ€§ for(int j=0;j&lt;fields.length;j++){ Field f = fields[j]; //å¦‚æœåŒ¹é…è¿›è¡Œèµ‹å€¼ if(f.getName().equalsIgnoreCase(rsmd.getColumnName(i))){ boolean flag = f.isAccessible(); f.setAccessible(true); f.set(obj, value); f.setAccessible(flag); } } } list.add(obj); } return list; } } stars-spring-boot-starter mybatis æ¢æˆmybatisplus å› ä¸ºéƒ¨åˆ†é¡¹ç›®ç»„ç”¨äº†mybatisplus,åœ¨å¯åŠ¨å™¨é‡ŒæŠŠmybatiså‡çº§æˆmybatisplusï¼Œé‡Œé¢åˆ†é¡µæ’ä»¶ é‡‡ç”¨ pagehelper+mybatisplusè‡ªå¸¦åˆ†é¡µ å¹¶å­˜çš„æ–¹å¼ã€‚ æ·»åŠ ä¾èµ– &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.4.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency mybatisplus+ åˆ†é¡µ é…ç½® @Bean @ConfigurationProperties(prefix = &quot;pagehelper&quot;) public Properties pageHelperProperties() { return new Properties(); } @Bean public MybatisSqlSessionFactoryBean sqlSessionFactoryBean() throws Exception { MybatisSqlSessionFactoryBean sessionFactory = new MybatisSqlSessionFactoryBean(); Interceptor[] plugins = new Interceptor[2]; plugins[0] = mybatisPlusInterceptor(); plugins[1] = pageHelperInterceptor(); sessionFactory.setPlugins(plugins); sessionFactory.setDataSource(dataSource()); PathMatchingResourcePatternResolver resolver = new PathMatchingResourcePatternResolver(); sessionFactory.setMapperLocations(resolver.getResources(&quot;classpath*:**/*Mapper.xml&quot;)); return sessionFactory; } @Bean public PlatformTransactionManager transactionManager() { return new DataSourceTransactionManager(dataSource()); } @Bean public MybatisPlusInterceptor mybatisPlusInterceptor() { MybatisPlusInterceptor interceptor = new MybatisPlusInterceptor(); PaginationInnerInterceptor paginationInnerInterceptor = new PaginationInnerInterceptor(DbType.POSTGRE_SQL); BlockAttackInnerInterceptor blockAttackInnerInterceptor = new BlockAttackInnerInterceptor(); //åˆ†é¡µæ’ä»¶ interceptor.addInnerInterceptor(paginationInnerInterceptor); //é˜²æ­¢å…¨è¡¨æ›´æ–°ä¸åˆ é™¤ interceptor.addInnerInterceptor(blockAttackInnerInterceptor); return interceptor; } @Bean public PageInterceptor pageHelperInterceptor() { PageInterceptor interceptor = new PageInterceptor(); interceptor.setProperties(this.pageHelperProperties()); return interceptor; } ","link":"https://oldcamel.run/post/40-xin-de-kai-duan/"},{"title":"Kubernetes é—®é¢˜å¤„ç†è®°å½•","content":"1ã€å¼ºåˆ¶åˆ é™¤namespace kubectl get namespace rook-ceph -o json &gt; rook-ceph.json vim rook-ceph.json åˆ é™¤specä¸­çš„å†…å®¹ &quot;spec&quot;: { } å¼€å¯kube-proxy kubectl proxy å¦å¼€ä¸€ä¸ªsshç™»å½•masterï¼Œæ‰§è¡Œ curl -k -H &quot;Content-Type: application/json&quot; -X PUT --data-binary @rook-ceph.json http://127.0.0.1:8001/api/v1/namespaces/rook-ceph/finalize æ“ä½œå®ŒæˆğŸ‘Œ #2ã€kubectl get pod è·å–ä¸åˆ°èµ„æº å¤§æ¦‚ç‡æ˜¯èŠ‚ç‚¹ç½‘ç»œæ’ä»¶å‡ºç°äº†é—®é¢˜ã€‚é‡å¯istiod æœåŠ¡å°è¯•å°†å…¶è¿è¡Œåœ¨ä¸åŒçš„èŠ‚ç‚¹ä¸Šè¯•ä¸‹ ","link":"https://oldcamel.run/post/kubernetes-wen-ti-chu-li-ji-lu/"},{"title":"Flink Sql å®ç°CDCæ•°æ®å®æ—¶åŒæ­¥ç‰¹æ®Šå¤„ç†","content":"1ã€decimal ç±»å‹åœ¨kafkaä¸­è§£æä¸ºä¹±ç  kafka connect æ’ä»¶è®¾ç½® &quot;decimal.handling.mode&quot;: &quot;string&quot; å±æ€§ã€‚å°†decimalè§£æä¸ºå­—ç¬¦ä¸² sqlè¯­å¥ä¸­å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°å­— ç”¨CAST(å­—æ®µåç§°) AS INT) å‡½æ•° INTå¯æ ¹æ®ä¸åŒç±»å‹çš„æ•°å­—æ›¿æ¢ä¸ºä¸åŒçš„ç›®æ ‡ç±»å‹ï¼ˆDECIMAL(10,4) BIGINT ç­‰ï¼‰ 2ã€æ—¥æœŸå¤„ç† kafkaä¸­äººæ—¥æœŸç±»å‹ä¼šè§£ææˆæ—¶é—´æˆ³ï¼Œå¯ç”¨TO_TIMESTAMP + FROM_UNIXTIMEåšè½¬æ¢ï¼Œæ³¨æ„å¤„ç†æ—¶åŒºé—®é¢˜ oracleæ—¥æœŸç±»å‹å¤„ç†ç²¾åº¦+æ—¶åŒºï¼š CREATED_DATETS AS TO_TIMESTAMP(FROM_UNIXTIME(CREATED_DATE/1000/1000-8 * 60 * 60, 'yyyy-MM-dd HH:mm:ss')), å…¶ä»–æ•°æ®åº“æ—¥æœŸç±»å‹åªå¤„ç†æ—¥æœŸ FDateTS AS TO_TIMESTAMP(FROM_UNIXTIME((FDate-8 * 60 * 60 * 1000) / 1000, 'yyyy-MM-dd HH:mm:ss')) 3ã€cdcæ•°æ®æ’å…¥è®¾ç½®ä¸»é”® PRIMARY KEY (AAA,BBB) NOT ENFORCED 4ã€kafka sink sasl connectæ’ä»¶è®¾ç½®è®¤è¯ &quot;database.history.producer.sasl.mechanism&quot;: &quot;PLAIN&quot;, &quot;database.history.producer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\&quot;admin\\&quot; password=\\&quot;å¯†ç \\&quot;;&quot;, &quot;database.history.producer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;, &quot;database.history.consumer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\&quot;admin\\&quot; password=\\&quot;å¯†ç \\&quot;;&quot;, &quot;database.history.consumer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;, &quot;database.history.consumer.sasl.mechanism&quot;: &quot;PLAIN&quot; ","link":"https://oldcamel.run/post/flink-sql-shi-xian-cdc-shu-ju-shi-shi-tong-bu-te-shu-chu-li/"},{"title":"Kafkaå®ç°oracleçš„CDCæ•°æ®å®æ—¶å˜æ›´","content":"ä½¿ç”¨å·¥å…· debezium-oracle-connector,oracle-LogMiner 1ã€oracle è®¾ç½® å¼€å¯å½’æ¡£æ¨¡å¼ alter system set db_recovery_file_dest_size = 10G; alter system set db_recovery_file_dest = '/home/oracle/oradta/recovery_area' scope=spfile; shutdown immediate startup mount alter database archivelog; alter database open; archive log list exit; ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS; åˆ›å»ºç”¨æˆ· CREATE USER myuser IDENTIFIED BY dbz DEFAULT TABLESPACE å‘½åç©ºé—´ QUOTA UNLIMITED ON å‘½åç©ºé—´ æˆæƒ GRANT CREATE SESSION TO myuser; GRANT CREATE TABLE TO myuser; GRANT CREATE SEQUENCE TO myuser; GRANT CREATE TRIGGER TO myuser; GRANT CREATE SESSION TO myuser; GRANT SELECT ON V_$DATABASE to myuser; GRANT FLASHBACK ANY TABLE TO myuser; GRANT SELECT ANY TABLE TO myuser; GRANT SELECT_CATALOG_ROLE TO myuser; GRANT EXECUTE_CATALOG_ROLE TO myuser; GRANT SELECT ANY TRANSACTION TO myuser; GRANT CREATE TABLE TO myuser; GRANT LOCK ANY TABLE TO myuser; GRANT ALTER ANY TABLE TO myuser; GRANT CREATE SEQUENCE TO myuser; GRANT EXECUTE ON DBMS_LOGMNR TO myuser; GRANT EXECUTE ON DBMS_LOGMNR_D TO myuser; GRANT SELECT ON V_$LOG TO myuser; GRANT SELECT ON V_$LOG_HISTORY TO myuser; GRANT SELECT ON V_$LOGMNR_LOGS TO myuser; GRANT SELECT ON V_$LOGMNR_CONTENTS TO myuser; GRANT SELECT ON V_$LOGMNR_PARAMETERS TO myuser; GRANT SELECT ON V_$LOGFILE TO myuser; GRANT SELECT ON V_$ARCHIVED_LOG TO myuser; GRANT SELECT ON V_$ARCHIVE_DEST_STATUS TO myuser; GRANT SELECT ON SYSTEM.LOGMNR_COL$ TO myuser; GRANT SELECT ON SYSTEM.LOGMNR_OBJ$ TO myuser; GRANT SELECT ON SYSTEM.LOGMNR_USER$ TO myuser; GRANT SELECT ON SYSTEM.LOGMNR_UID$ TO myuser; oracleå®¢æˆ·ç«¯è®¾ç½® ä¸‹è½½ wget &quot;https://download.oracle.com/otn_software/linux/instantclient/19600/instantclient-basiclite-linux.x64-19.6.0.0.0dbru.zip&quot; -O /tmp/ic.zipï¼› unzip /tmp/ic.zip -d è‡ªå®šä¹‰ç›®å½• ç¯å¢ƒå˜é‡ è®¾ç½®LD_LIBRARY_PATH æŒ‡å‘ è‡ªå®šä¹‰ç›®å½• kafaka connect æ’ä»¶è®¾ç½® ä¸‹è½½ wget &quot;https://oss.sonatype.org/service/local/artifact/maven/redirect?r=snapshots&amp;g=io.debezium&amp;a=debezium-connector-oracle&amp;v=LATEST&amp;c=plugin&amp;e=tar.gz&quot; -O /tmp/dbz-ora.tgzï¼› tar -xvf /tmp/dbz-ora.tgz --directory kafaka æ’ä»¶ç›®å½• æ·»åŠ  ojdbc jaråˆ°æ’ä»¶ç›®å½• curl https://maven.xwiki.org/externals/com/oracle/jdbc/ojdbc8/12.2.0.1/ojdbc8-12.2.0.1.jar -o ojdbc8-12.2.0.1.jar åˆ›å»ºkafka connect curl --location --request POST 'http://localhost:8083/connectors' \\ --header 'Content-Type: application/json' \\ --data-raw '{ &quot;name&quot;: &quot;name&quot;, &quot;config&quot;: { &quot;connector.class&quot; : &quot;io.debezium.connector.oracle.OracleConnector&quot;, &quot;tasks.max&quot; : &quot;1&quot;, &quot;database.server.name&quot; : &quot;topic&quot;, &quot;database.hostname&quot; : &quot;database_host&quot;, &quot;database.port&quot; : &quot;1521&quot;, &quot;database.user&quot; : &quot;myuser&quot;, &quot;database.password&quot; : &quot;dbz&quot;, &quot;database.dbname&quot; : &quot;orcl&quot;, &quot;database.tablename.case.insensitive&quot;: &quot;true&quot;, &quot;database.history.kafka.bootstrap.servers&quot; : &quot;localhost:9092&quot;, &quot;database.history.kafka.topic&quot;: &quot;name-h&quot;, &quot;table.include.list&quot;:&quot;AAA.BBB,CCC.DDD&quot;, &quot;database.history.producer.sasl.mechanism&quot;: &quot;PLAIN&quot;, &quot;database.history.producer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\&quot;admin\\&quot; password=\\&quot;password\\&quot;;&quot;, &quot;database.history.producer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;, &quot;database.history.consumer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\&quot;admin\\&quot; password=\\&quot;password\\&quot;;&quot;, &quot;database.history.consumer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;, &quot;database.history.consumer.sasl.mechanism&quot;: &quot;PLAIN&quot; } }' ","link":"https://oldcamel.run/post/kafka-shi-xian-oracle-de-cdc-shu-ju-shi-shi-bian-geng/"},{"title":"linux å®‰è£…æœåŠ¡å™¨èµ„æºç›‘æ§ prometheusè¿œç¨‹å†™å…¥","content":"1.å®‰è£…prometheus wget -c https://github.com/prometheus/prometheus/releases/download/v2.23.0/prometheus-2.23.0.linux-amd64.tar.gz tar zxvf prometheus-2.23.0.linux-amd64.tar.gz -C /opt/ cd /opt/ ln -s prometheus-2.23.0.linux-amd64 prometheus cat &gt; /etc/systemd/system/prometheus.service &lt;&lt;EOF [Unit] Description=prometheus After=network.target [Service] Type=simple WorkingDirectory=/opt/prometheus ExecStart=/opt/prometheus/prometheus --config.file=/opt/prometheus/prometheus.yml LimitNOFILE=65536 PrivateTmp=true RestartSec=2 StartLimitInterval=0 Restart=always [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable prometheus systemctl start prometheus 2.é…ç½®Prometheus cat &gt; /opt/prometheus/prometheus.yml &lt;&lt;EOF # my global config global: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s). external_labels: tenant: &quot;140test&quot; remote_write: - url: &quot;url&quot; basic_auth: username: ç”¨æˆ·å password: å¯†ç  # Alertmanager configuration alerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093 # Load rules once and periodically evaluate them according to the global 'evaluation_interval'. rule_files: # - &quot;first_rules.yml&quot; # - &quot;second_rules.yml&quot; # A scrape configuration containing exactly one endpoint to scrape: # Here it's Prometheus itself. scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'servers' file_sd_configs: - refresh_interval: 61s files: - /opt/prometheus/servers/*.json EOF 3.åˆ›å»ºnodeé…ç½®æ–‡ä»¶ mkdir -p /opt/prometheus/servers cd /opt/prometheus/servers vim json.json [ { &quot;targets&quot;: [ â€œxxx.xxx.xxx.xxx:9100&quot; ], &quot;labels&quot;: { &quot;instance&quot;: &quot;xxx.xxx.xxx.xxx&quot;, &quot;job&quot;: &quot;node_exporter&quot; } } ] å®‰è£…node_exporter Wget https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-amd64.tar.gz tar zxvf node_exporter-1.0.1.linux-amd64.tar.gz -C /opt/ cd /opt/ ln -s node_exporter-1.0.1.linux-amd64 node_exporter cat &gt; /etc/systemd/system/node_exporter.service &lt;&lt;EOF [Unit] Description=node_exporter After=network.target [Service] Type=simple WorkingDirectory=/opt/node_exporter ExecStart=/opt/node_exporter/node_exporter LimitNOFILE=65536 PrivateTmp=true RestartSec=2 StartLimitInterval=0 Restart=always [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable node_exporter systemctl start node_exporter systemctl restart prometheus æŸ¥çœ‹æœåŠ¡æ—¥å¿—å‘½ä»¤ journalctl -u prometheus ","link":"https://oldcamel.run/post/linux-an-zhuang-fu-wu-qi-zi-yuan-jian-kong-prometheus-yuan-cheng-xie-ru/"},{"title":"Kafkaå®ç°sqlserverçš„CDCæ•°æ®å®æ—¶å˜æ›´","content":"å®‰è£…sqlserver #!/bin/bash -e # Password for the SA user (required) # è®¾ç½®å¯†ç  MSSQL_SA_PASSWORD='test@12345678' # Product ID of the version of SQL server you're installing # Must be evaluation, developer, express, web, standard, enterprise, or your 25 digit product key # Defaults to developer # é€‰æ‹©ç‰ˆæœ¬ï¼Œæœ‰å¤šç§ç‰ˆæœ¬å¯ä¾›åŸåˆ™ï¼Œå…·ä½“ç‰ˆæœ¬æ ‡è¯†ç¬¦å¯è‡ªè¡Œç™¾åº¦ MSSQL_PID='evaluation' # Install SQL Server Agent (recommended) SQL_ENABLE_AGENT='y' # Install SQL Server Full Text Search (optional) # SQL_INSTALL_FULLTEXT='y' # Create an additional user with sysadmin privileges (optional) # æ–°å»ºä¸€ä¸ªé¢å¤–æ·»åŠ çš„ç”¨æˆ·ï¼Œï¼ˆå¯é€‰ï¼‰ # SQL_INSTALL_USER='&lt;Username&gt;' # SQL_INSTALL_USER_PASSWORD='&lt;YourStrong!Passw0rd&gt;' if [ -z $MSSQL_SA_PASSWORD ] then echo Environment variable MSSQL_SA_PASSWORD must be set for unattended install exit 1 fi # --------------------------- è¿œç¨‹æ‹‰åŒ… å¹¶å®‰è£…çš„è¿‡ç¨‹ echo Adding Microsoft repositories... sudo curl -o /etc/yum.repos.d/mssql-server.repo https://packages.microsoft.com/config/rhel/7/mssql-server-2017.repo sudo curl -o /etc/yum.repos.d/msprod.repo https://packages.microsoft.com/config/rhel/7/prod.repo echo Installing SQL Server... sudo yum install -y mssql-server # --------------------------- ä¸‹é¢ç»™å‡º æœ¬åœ°ç¦»çº¿rpmåŒ…å®‰è£…éƒ¨åˆ†è„šæœ¬ echo ï¼šLocal Installing SQL Server... sudo yum localinstall -y ./sqlserver2017.rpm # æ‰§è¡Œsqlserverçš„é…ç½® echo Running mssql-conf setup... sudo MSSQL_SA_PASSWORD=$MSSQL_SA_PASSWORD \\ MSSQL_PID=$MSSQL_PID \\ /opt/mssql/bin/mssql-conf -n setup accept-eula echo Installing mssql-tools and unixODBC developer... sudo ACCEPT_EULA=Y yum install -y mssql-tools unixODBC-devel # Add SQL Server tools to the path by default: echo Adding SQL Server tools to your path... echo PATH=&quot;$PATH:/opt/mssql-tools/bin&quot; &gt;&gt; ~/.bash_profile echo 'export PATH=&quot;$PATH:/opt/mssql-tools/bin&quot;' &gt;&gt; ~/.bashrc source ~/.bashrc # Optional Enable SQL Server Agent : if [ ! -z $SQL_ENABLE_AGENT ] then echo Enable SQL Server Agent... sudo /opt/mssql/bin/mssql-conf set sqlagent.enabled true sudo systemctl restart mssql-server fi # Optional SQL Server Full Text Search installation: if [ ! -z $SQL_INSTALL_FULLTEXT ] then echo Installing SQL Server Full-Text Search... sudo yum install -y mssql-server-fts fi # Configure firewall to allow TCP port 1433: # é…ç½®é˜²ç«å¢™æ”¾è¡Œ1433ç«¯å£ï¼Œæ‡’çœäº‹ç›´æ¥å…³æ‰é˜²ç«å¢™äº¦å¯ echo Configuring firewall to allow traffic on port 1433... sudo firewall-cmd --zone=public --add-port=1433/tcp --permanent sudo firewall-cmd --reload # Example of setting post-installation configuration options # Set trace flags 1204 and 1222 for deadlock tracing: #echo Setting trace flags... #sudo /opt/mssql/bin/mssql-conf traceflag 1204 1222 on # Restart SQL Server after making configuration changes: # é‡å¯ echo Restarting SQL Server... sudo systemctl restart mssql-server # Connect to server and get the version: # å®˜æ–¹ç»™å‡ºçš„æµ‹è¯•é“¾æ¥è„šæœ¬ counter=1 errstatus=1 while [ $counter -le 5 ] &amp;&amp; [ $errstatus = 1 ] do echo Waiting for SQL Server to start... sleep 5s /opt/mssql-tools/bin/sqlcmd \\ -S localhost \\ -U SA \\ -P $MSSQL_SA_PASSWORD \\ -Q &quot;SELECT @@VERSION&quot; 2&gt;/dev/null errstatus=$? ((counter++)) done # Display error if connection failed: if [ $errstatus = 1 ] then echo Cannot connect to SQL Server, installation aborted exit $errstatus fi # Optional new user creation: if [ ! -z $SQL_INSTALL_USER ] &amp;&amp; [ ! -z $SQL_INSTALL_USER_PASSWORD ] then echo Creating user $SQL_INSTALL_USER /opt/mssql-tools/bin/sqlcmd \\ -S localhost \\ -U SA \\ -P $MSSQL_SA_PASSWORD \\ -Q &quot;CREATE LOGIN [$SQL_INSTALL_USER] WITH PASSWORD=N'$SQL_INSTALL_USER_PASSWORD', DEFAULT_DATABASE=[master], CHECK_EXPIRATION=ON, CHECK_POLICY=ON; ALTER SERVER ROLE [sysadmin] ADD MEMBER [$SQL_INSTALL_USER]&quot; fi echo Done! è¿è¡Œè„šæœ¬ç­‰ç€å®‰è£…å®Œæˆ å¼€å¯è¡¨çš„cdc åˆ›å»ºshihuæ•°æ®åº“ å»ºæµ‹è¯•è¡¨ IF EXISTS (SELECT * FROM sys.all_objects WHERE object_id = OBJECT_ID(N'[dbo].[dt_test]') AND type IN ('U')) DROP TABLE [dbo].[dt_test] GO CREATE TABLE [dbo].[dt_test] ( [tid] int NOT NULL, [tname] nvarchar(200) COLLATE SQL_Latin1_General_CP1_CI_AS NULL, [tidcard] nvarchar(200) COLLATE SQL_Latin1_General_CP1_CI_AS NULL, [tbirthday] date NULL, [tmobile] nvarchar(200) COLLATE SQL_Latin1_General_CP1_CI_AS NULL, [temail] nvarchar(200) COLLATE SQL_Latin1_General_CP1_CI_AS NULL, [tgender] bigint NULL, [tcreate_time] datetime NULL ) GO ALTER TABLE [dbo].[dt_test] SET (LOCK_ESCALATION = TABLE) GO -- ---------------------------- -- Primary Key structure for table dt_test -- ---------------------------- ALTER TABLE [dbo].[dt_test] ADD CONSTRAINT [PK__dt_test__DC105B0FA908205A] PRIMARY KEY CLUSTERED ([tid]) WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] GO å¼€å¯cdc USE shihu GO EXEC sys.sp_cdc_enable_db GO USE shihu GO EXEC sys.sp_cdc_enable_table @source_schema = 'dbo', @source_name = â€˜dt_testâ€™, @role_name = NULL, @filegroup_name = 'PRIMARY', @supports_net_changes = 1 GO ä¸‹è½½kafkaæ’ä»¶ å®˜æ–¹æ–‡æ¡£ https://debezium.io/documentation/reference/1.3/connectors/sqlserver.html ä¸‹è½½ https://repo1.maven.org/maven2/io/debezium/debezium-connector-sqlserver/1.2.5.Final/debezium-connector-sqlserver-1.2.5.Final-plugin.tar.gz åˆ›å»ºæ’ä»¶ç›®å½• åœ¨kafkaè·Ÿç›®å½•åˆ›å»º kafka_connect_plugins ç›®å½• å°†ä¸‹è½½çš„åŒ…è§£å‹åˆ°æ­¤ç›®å½•ä¸­ ä¿®æ”¹kafka/config/connect-distribute.properties bootstrap.servers=IP:9092 plugin.path= /kafkaè·¯å¾„/kafka_connect_plugins å¯åŠ¨kafka-connect ./bin/connect-distributed.sh config/connect-distributed.properties &gt;logs/ksc.log &amp; å¼€å¯æˆåŠŸçš„æ ‡å¿—,postmanå¯ä»¥ä½¿ç”¨è®¿é—®ç«¯å£8083ï¼ˆé»˜è®¤çš„ç«¯å£å·,å¯ä»¥åœ¨connect-distributed.properties æ›´æ”¹ï¼‰ rest api GET /connectors â€“ è¿”å›æ‰€æœ‰æ­£åœ¨è¿è¡Œçš„connectorå POST /connectors â€“ æ–°å»ºä¸€ä¸ªconnector; è¯·æ±‚ä½“å¿…é¡»æ˜¯jsonæ ¼å¼å¹¶ä¸”éœ€è¦åŒ…å«nameå­—æ®µå’Œconfigå­—æ®µï¼Œnameæ˜¯connectorçš„åå­—ï¼Œconfigæ˜¯jsonæ ¼å¼ï¼Œå¿…é¡»åŒ…å«ä½ çš„connectorçš„é…ç½®ä¿¡æ¯ã€‚ GET /connectors/{name} â€“ è·å–æŒ‡å®šconnetorçš„ä¿¡æ¯ GET /connectors/{name}/config â€“ è·å–æŒ‡å®šconnectorçš„é…ç½®ä¿¡æ¯ PUT /connectors/{name}/config â€“ æ›´æ–°æŒ‡å®šconnectorçš„é…ç½®ä¿¡æ¯ GET /connectors/{name}/status â€“ è·å–æŒ‡å®šconnectorçš„çŠ¶æ€ï¼ŒåŒ…æ‹¬å®ƒæ˜¯å¦åœ¨è¿è¡Œã€åœæ­¢ã€æˆ–è€…å¤±è´¥ï¼Œå¦‚æœå‘ç”Ÿé”™è¯¯ï¼Œè¿˜ä¼šåˆ—å‡ºé”™è¯¯çš„å…·ä½“ä¿¡æ¯ã€‚ GET /connectors/{name}/tasks â€“ è·å–æŒ‡å®šconnectoræ­£åœ¨è¿è¡Œçš„taskã€‚ GET /connectors/{name}/tasks/{taskid}/status â€“ è·å–æŒ‡å®šconnectorçš„taskçš„çŠ¶æ€ä¿¡æ¯ PUT /connectors/{name}/pause â€“ æš‚åœconnectorå’Œå®ƒçš„taskï¼Œåœæ­¢æ•°æ®å¤„ç†çŸ¥é“å®ƒè¢«æ¢å¤ã€‚ PUT /connectors/{name}/resume â€“ æ¢å¤ä¸€ä¸ªè¢«æš‚åœçš„connector POST /connectors/{name}/restart â€“ é‡å¯ä¸€ä¸ªconnectorï¼Œå°¤å…¶æ˜¯åœ¨ä¸€ä¸ªconnectorè¿è¡Œå¤±è´¥çš„æƒ…å†µä¸‹æ¯”è¾ƒå¸¸ç”¨ POST /connectors/{name}/tasks/{taskId}/restart â€“ é‡å¯ä¸€ä¸ªtaskï¼Œä¸€èˆ¬æ˜¯å› ä¸ºå®ƒè¿è¡Œå¤±è´¥æ‰è¿™æ ·åšã€‚ DELETE /connectors/{name} â€“ åˆ é™¤ä¸€ä¸ªconnectorï¼Œåœæ­¢å®ƒçš„æ‰€æœ‰taskå¹¶åˆ é™¤é…ç½®ã€‚ æ·»åŠ sqlserver connect curl --location --request POST 'http://localhost:8083/connectors' \\ --header 'Content-Type: application/json' \\ --data-raw '{ &quot;name&quot;: &quot;sqlserver178-connector&quot;, &quot;config&quot;: { &quot;connector.class&quot;: &quot;io.debezium.connector.sqlserver.SqlServerConnector&quot;, &quot;database.hostname&quot;: &quot;sqlserverçš„ip&quot;, &quot;database.port&quot;: &quot;1433&quot;, &quot;database.user&quot;: &quot;sa&quot;, &quot;database.password&quot;: &quot;test@12345678&quot;, &quot;database.dbname&quot;: &quot;shihu&quot;, &quot;database.server.name&quot;: &quot;fullfillment&quot;, &quot;table.whitelist&quot;: &quot;dbo.dt_test&quot;, &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafkaçš„IP:9092&quot;, &quot;database.history.kafka.topic&quot;: &quot;dbhistory.fullfillment&quot;, &quot;database.history.producer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;, &quot;database.history.producer.sasl.mechanism&quot;: &quot;PLAIN&quot;, &quot;database.history.producer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\&quot;admin\\&quot; password=\\&quot;123\\&quot;;&quot;, &quot;database.history.consumer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;, &quot;database.history.consumer.sasl.mechanism&quot;: &quot;PLAIN&quot;, &quot;database.history.consumer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\&quot;admin\\&quot; password=\\&quot;123\\&quot;;&quot; } }' æ¶ˆè´¹ä¿¡æ¯ consumer.prepertiesé…ç½® bootstrap.servers=localhost:9092 group.id=test-consumer-group security.protocol=SASL_PLAINTEXT sasl.mechanism=PLAIN ä¿®æ”¹å™¨kafka-console-consumer.shä¸ºkafka-console-consumer-saal.shå€’æ•°ç¬¬äºŒè¡Œæ·»åŠ  export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/kafka_2.13-2.6.0/config/kafka_client_jaas.conf&quot; å¯åŠ¨ ./bin/kafka-console-consumer-saal.sh --bootstrap-server localhost:9092 --topic fullfillment.dbo.dt_test --consumer.config config/consumer.properties æ¶ˆæ¯ç¤ºä¾‹ { &quot;schema&quot;:{ &quot;type&quot;:&quot;struct&quot;, &quot;fields&quot;:Array[6], &quot;optional&quot;:false, &quot;name&quot;:&quot;fullfillment.dbo.dt_test.Envelope&quot; }, &quot;payload&quot;:{ &quot;before&quot;:{ &quot;tid&quot;:1, &quot;tname&quot;:&quot;222&quot;, &quot;tidcard&quot;:&quot;2&quot;, &quot;tbirthday&quot;:-25567, &quot;tmobile&quot;:&quot;1&quot;, &quot;temail&quot;:&quot;1&quot;, &quot;tgender&quot;:2, &quot;tcreate_time&quot;:-2208988800000 }, &quot;after&quot;:{ &quot;tid&quot;:1, &quot;tname&quot;:&quot;test&quot;, &quot;tidcard&quot;:&quot;2&quot;, &quot;tbirthday&quot;:-25567, &quot;tmobile&quot;:&quot;1&quot;, &quot;temail&quot;:&quot;1&quot;, &quot;tgender&quot;:2, &quot;tcreate_time&quot;:-2208988800000 }, &quot;source&quot;:Object{...}, &quot;op&quot;:&quot;u&quot;, &quot;ts_ms&quot;:1610019731750, &quot;transaction&quot;:null } } oracleç¤ºä¾‹ curl --location --request POST 'http://localhost:8083/connectors/' \\ --header 'Content-Type: application/json' \\ --data-raw ' { &quot;name&quot;: &quot;oracle-61-connector&quot;, &quot;config&quot;: { &quot;connector.class&quot;: &quot;com.ecer.kafka.connect.oracle.OracleSourceConnector&quot;, &quot;db.name.alias&quot;: &quot;61test&quot;, &quot;tasks.max&quot;: 1, &quot;topic&quot;: &quot;kol&quot;, &quot;db.name&quot;: &quot;portaldb&quot;, &quot;db.hostname&quot;: &quot;oralceè¿æ¥hoståœ°å€&quot;, &quot;db.port&quot;: 1521, &quot;db.user&quot;: &quot;info&quot;, &quot;db.user.password&quot;: &quot;111111&quot;, &quot;db.fetch.size&quot;: 1, &quot;table.whitelist&quot;: &quot;INFO.*,SPAUTH.*,WORKFLOW.*,FLOWABLE.*,OA.*&quot;, &quot;table.blacklist&quot;: &quot;&quot;, &quot;parse.dml.data&quot;: true, &quot;reset.offset&quot;: false, &quot;multitenant&quot;: false } } mysqlç¤ºä¾‹ curl --location --request POST 'http://localhost:8083/connectors/' \\ --header 'Content-Type: application/json' \\ --data-raw ' { &quot;name&quot;: &quot;140mysql-connector&quot;, &quot;config&quot;: { &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;, &quot;database.hostname&quot;: &quot;localhost&quot;, &quot;database.port&quot;: &quot;3306&quot;, &quot;database.user&quot;: &quot;root&quot;, &quot;database.password&quot;: &quot;111111&quot;, &quot;database.server.id&quot;: &quot;184054&quot;, &quot;database.server.name&quot;: &quot;fullfillment140ms&quot;, &quot;database.whitelist&quot;:&quot;test&quot;, &quot;database.history.kafka.bootstrap.servers&quot;: &quot;localhost:9092&quot;, &quot;database.history.kafka.topic&quot;: &quot;dbhistory.fullfillment140ms&quot;, &quot;include.schema.changes&quot;: &quot;false&quot;, &quot;database.history.producer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;, &quot;database.history.producer.sasl.mechanism&quot;: &quot;PLAIN&quot;, &quot;database.history.producer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\&quot;admin\\&quot; password=\\&quot;123\\&quot;;&quot;, &quot;database.history.consumer.security.protocol&quot;: &quot;SASL_PLAINTEXT&quot;, &quot;database.history.consumer.sasl.mechanism&quot;: &quot;PLAIN&quot;, &quot;database.history.consumer.sasl.jaas.config&quot;: &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\&quot;admin\\&quot; password=\\&quot;123\\&quot;;&quot; } } æŸ¥çœ‹topic list bin/kafka-topics.sh --zookeeper localhost --list ","link":"https://oldcamel.run/post/kafka-shi-xian-sqlserver-de-cdc-shu-ju-shi-shi-bian-geng/"},{"title":"Kafkaå¼€å¯SASLç”¨æˆ·åå¯†ç è®¤è¯","content":"åˆ›å»ºkafka_server_jaas.confæ–‡ä»¶ configç›®å½•ä¸‹åˆ›å»ºkafka_server_jaas.confæ–‡ä»¶ KafkaServer { org.apache.kafka.common.security.plain.PlainLoginModule required username=&quot;admin&quot; password=&quot;123&quot; user_admin=&quot;123&quot; user_yunzai=&quot;123&quot;; }; åˆ›å»ºkafka_client_jaas.confæ–‡ä»¶ configç›®å½•ä¸‹åˆ›å»ºkafka_client_jaas.confæ–‡ä»¶ KafkaClient { org.apache.kafka.common.security.plain.PlainLoginModule required username=&quot;admin&quot; password=&quot;123&quot;; }; ä¿®æ”¹server.properties listeners=SASL_PLAINTEXT://IP:9092 advertised.listeners=SASL_PLAINTEXT://IP:9092 security.inter.broker.protocol=SASL_PLAINTEXT sasl.enabled.mechanisms=PLAIN sasl.mechanism.inter.broker.protocol=PLAIN authorizer.class.name = kafka.security.auth.SimpleAclAuthorizer super.users=User:admin; ä¿®æ”¹kafakaå¯åŠ¨è„šæœ¬ ä¿®æ”¹binç›®å½•ä¸‹kafka_start.sh åœ¨å€’æ•°ç¬¬äºŒè¡Œæ·»åŠ kafka_server_jaas.confçš„å…¨è·¯å¾„ export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/kafka_2.13-2.6.0/config/kafka_server_jaas.conf&quot; ï¼ˆå¯é€‰ï¼‰ä¸ºç‰¹å®šç”¨æˆ·æ·»åŠ ç‰¹å®štopicçš„aclæˆæƒ å¦‚ä¸‹è¡¨ç¤ºä¸ºç”¨æˆ·yunzaiæ·»åŠ topic nginx-logçš„è¯»å†™æƒé™ã€‚ ./bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 -add --allow-principal User:yunzai --operation Read --operation Write --topic testyunzai éªŒè¯æˆæƒ ./bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --list --topic testyunzai ç”Ÿäº§è€…æ¶ˆè´¹è€…éªŒè¯ ç”Ÿäº§è€… 1.ä»¥ç”¨æˆ·yunzaiä¸ºä¾‹ï¼Œåˆ›å»ºJAASè®¤è¯æ–‡ä»¶yunzai_jaas.confæ”¾åœ¨configç›®å½•ä¸‹ï¼Œå†…å®¹å¦‚ä¸‹: KafkaClient { org.apache.kafka.common.security.plain.PlainLoginModule required username=&quot;yunzai&quot; password=&quot;123&quot;; }; 2.æ‹·è´bin/kafka-console-producer.shä¸ºbin/yunzai-kafka-console-producer.shï¼Œå¹¶å°†JAASæ–‡ä»¶ä½œä¸ºä¸€ä¸ªJVMå‚æ•°ä¼ ç»™console producer å€’æ•°ç¬¬äºŒè¡Œæ·»åŠ  export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/kafka_2.13-2.6.0/config/yunzai_jaas.conf&quot; 3.åˆ›å»ºæ–‡ä»¶producer.configæŒ‡å®šå¦‚ä¸‹å±æ€§ï¼š security.protocol=SASL_PLAINTEXT sasl.mechanism=PLAIN 4.å¯åŠ¨producer ./bin/yunzai-kafka-console-producer.sh --broker-list IP:9092 --topic testyunzai --producer.config producer.config æ¶ˆè´¹è€… 1.ä»¥ç”¨æˆ·yunzaiä¸ºä¾‹ï¼Œåˆ›å»ºJAASè®¤è¯æ–‡ä»¶yunzai_jaas.confæ”¾åœ¨configç›®å½•ä¸‹ï¼Œå¦‚æœç”¨æˆ·è·Ÿç”Ÿäº§è€…æ˜¯åŒä¸€ä¸ªï¼Œå¯ä»¥å¤ç”¨ä¸Šé¢ç”Ÿäº§è€…çš„JAASæ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹: KafkaClient { org.apache.kafka.common.security.plain.PlainLoginModule required username=&quot;yunzai&quot; password=&quot;123&quot;; }; 2.æ‹·è´bin/kafka-console-consumer.shä¸ºbin/yunzai-kafka-console-consumer.shï¼Œå¹¶å°†JAASæ–‡ä»¶ä½œä¸ºä¸€ä¸ªJVMå‚æ•°ä¼ ç»™console consumer å€’æ•°ç¬¬äºŒè¡Œæ·»åŠ  export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/kafka_2.13-2.6.0/config/yunzai_jaas.conf&quot; 3.åˆ›å»ºæ–‡ä»¶consumer.configæŒ‡å®šå¦‚ä¸‹å±æ€§ï¼š security.protocol=SASL_PLAINTEXT sasl.mechanism=PLAIN group.id=test 4.å¯åŠ¨consumer ./bin/yunzai-kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic testyunzai --from-beginning --consumer.config consumer.config java beamå®¢æˆ·ç«¯è®¤è¯ beamè®¾ç½® final Map&lt;String, Object&gt; immutableMap = new ImmutableMap.Builder&lt;String, Object&gt;() .put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;) .put(ConsumerConfig.GROUP_ID_CONFIG, options.getGroupid()) .put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true) .put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, &quot;SASL_PLAINTEXT&quot;) .put(SaslConfigs.SASL_MECHANISM, &quot;PLAIN&quot;) .put(SaslConfigs.SASL_JAAS_CONFIG, &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\&quot;&quot; + options.getKafkaUsername() + &quot;\\&quot; password=\\&quot;&quot; + options.getKafkaPassword() + &quot;\\&quot;;&quot;) .build(); ","link":"https://oldcamel.run/post/kafka-kai-qi-sasl-yong-hu-ming-mi-ma-ren-zheng/"},{"title":"VictoriaMetrics æ±‡æ€»å¤šä¸ªPrometheusæŒ‡æ ‡ï¼ˆå››ï¼‰è¯»å–IstioæŒ‡æ ‡","content":"istioä¸­é»˜è®¤å¸¦äº†prometheus,é‡Œé¢åŒ…å«istioä»£ç†çš„æŒ‡æ ‡ï¼Œç°éœ€è¦å°†istioä¸­çš„æŒ‡æ ‡é…ç½®åœ¨kube-prometheusä¸­ã€‚ ä¿®æ”¹prometheus-k8s è§’è‰²æƒé™ ä¿®æ”¹prometheus-clusterRole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: prometheus-k8s rules: - apiGroups: - &quot;&quot; resources: - nodes - services - endpoints - pods - nodes/proxy verbs: - get - watch - list - apiGroups: - &quot;&quot; resources: - configmaps verbs: - get - nonResourceURLs: - /metrics verbs: - get é…ç½®æ•°æ®å¹³é¢çš„æœåŠ¡ç›‘æ§ apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: prometheus-oper-istio-controlplane spec: jobLabel: istio selector: matchExpressions: - {key: istio, operator: In, values: [mixer,pilot,galley,citadel,sidecar-injector]} namespaceSelector: any: true endpoints: - port: http-monitoring interval: 15s - port: http-policy-monitoring interval: 15s é…ç½®æ•°æ®å±‚çš„æœåŠ¡ç›‘æ§ apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: prometheus-oper-istio-dataplane labels: monitoring: istio-dataplane spec: selector: matchExpressions: - {key: istio-prometheus-ignore, operator: DoesNotExist} namespaceSelector: any: true jobLabel: envoy-stats endpoints: - path: /stats/prometheus targetPort: http-envoy-prom interval: 15s æŸ¥çœ‹æ˜¯å¦ç”Ÿæ•ˆ VictoriaMetrics ä¸­ä¸åŒé›†ç¾¤ä¸­çš„æŒ‡æ ‡è·å– ","link":"https://oldcamel.run/post/victoriametrics-hui-zong-duo-ge-prometheus-zhi-biao-san-du-qu-istio-zhi-biao/"},{"title":"VictoriaMetrics æ±‡æ€»å¤šä¸ªPrometheusæŒ‡æ ‡ï¼ˆä¸‰ï¼‰kube-prometheuså®‰è£…","content":"kube-prometheusæ˜¯Prometheus Operator é’ˆå¯¹kubernetesçš„ç›‘æ§ç»„ä»¶ã€‚ githubåœ°å€ https://github.com/prometheus-operator/kube-prometheus é…ç½®è¿œç¨‹å†™å…¥ basicç”¨æˆ·åå¯†ç é…ç½® secret apiVersion: v1 kind: Secret metadata: name: vmuser namespace: monitoring type: kubernetes.io/basic-auth stringData: username: username password: password æ·»åŠ è¿œç¨‹å†™å…¥ å’Œå¤–éƒ¨æ ‡ç­¾ ä¿®æ”¹prometheus-prometheus.yamlæ–‡ä»¶ï¼Œæ·»åŠ  prometheusExternalLabelName: &quot;&quot; replicaExternalLabelName: &quot;&quot; externalLabels: tenant: &quot;ç§Ÿæˆ·åç§°&quot; remoteWrite: - url: http://VictoriaMetricsè¿æ¥åœ°å€/api/v1/write basicAuth: username: key: username name: vmuser password: key: password name: vmuser ","link":"https://oldcamel.run/post/victoriametrics-hui-zong-duo-ge-prometheus-zhi-biao-san-kube-prometheus-an-zhuang/"},{"title":"VictoriaMetrics æ±‡æ€»å¤šä¸ªPrometheusæŒ‡æ ‡ï¼ˆäºŒï¼‰Grafanaå®‰è£…","content":"å®‰è£…grafana ä»victoriametricsä¸­è¯»å–æ±‡æ€»çš„æŒ‡æ ‡ å®‰è£…mysql ä¸‹è½½ç¦»çº¿åŒ… ä¸‹è½½åœ°å€ è§£å‹å®‰è£… rpm -Uvh *.rpm --nodeps --force ä¿®æ”¹é…ç½®æ–‡ä»¶ ä¿®æ”¹ /etc/my.cnf æ·»åŠ  lower_case_table_names=1 default-storage-engine=INNODB ç”Ÿæˆç”¨æˆ·ä¸å¸¦å¯†ç  mysqld --initialize-insecure å¦‚æœè¦é‡æ–°åˆå§‹åŒ–ï¼Œå¿…é¡»å…ˆæ¸…ç©ºdataæ–‡ä»¶å¤¹ å¯åŠ¨mysql systemctl start mysqld è¿›å…¥mysql mysql -u root ä¿®æ”¹å¯†ç +å¼€å¯è¿œç¨‹è®¿é—® set password for root@localhost = password('123456'); GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456' WITH GRANT OPTION; FLUSH PRIVILEGES; åˆ›å»ºgrafanaæ•°æ®åº“ CREATE DATABASE `grafana` DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; å®‰è£…grafana ä¸‹è½½å®‰è£… å‚è€ƒæ–‡æ¡£ wget https://dl.grafana.com/oss/release/grafana-7.3.6-1.x86_64.rpm sudo yum install grafana-7.3.6-1.x86_64.rpm ä¿®æ”¹é…ç½®æ–‡ä»¶ vim /etc/grafana/grafana.ini domainä¿®æ”¹ domain = ä¿®æ”¹æˆä¸»æœºIP é»˜è®¤ç”¨æˆ·åå¯†ç ä¿®æ”¹ admin_user = admin admin_password = 123456 æ•°æ®åº“ä¿®æ”¹ type = mysql host = 127.0.0.1:3306 name = grafana user = root password =123456 å¯åŠ¨grafanaæœåŠ¡ service grafana-server start é…ç½® victoriametricsè¿æ¥ è®¿é—® ip:3000 æ·»åŠ prometheusæ•°æ®æº è®¾ç½®ä¸º victoriametricsä¸»æœºip ç«¯å£ 8428 é…ç½®victoriametricså®˜æ–¹çš„ç›‘æ§å›¾è¡¨ grafana labåœ°å€ ","link":"https://oldcamel.run/post/grafana-an-zhuang/"},{"title":"VictoriaMetrics æ±‡æ€»å¤šä¸ªPrometheusæŒ‡æ ‡ï¼ˆä¸€ï¼‰å®‰è£…VictoriaMetrics","content":"victoriametrics ç”¨æ¥æ”¶é›†k8sé›†ç¾¤ä¸­çš„prometheusæŒ‡æ ‡ä¿¡æ¯ ã€‚ githubåœ°å€ å®˜æ–¹æ–‡æ¡£åœ°å€ ä¸‹è½½äºŒçº§åˆ¶å®‰è£…åŒ… å¯åŠ¨ä¸»è¿›ç¨‹ è¿è¡Œç«¯å£8428 ./victoria-metrics-prod -retentionPeriod=3 -selfScrapeInterval=10s &amp; å¯åŠ¨è®¤è¯ä»£ç† è¿è¡Œç«¯å£8427 ./vmauth-prod -auth.config=/opt/vm/config/config.yml &amp; basicè®¤è¯é…ç½®æ–‡ä»¶ users: - username: &quot;username&quot; password: &quot;password&quot; url_prefix: &quot;http://localhost:8428&quot; è¿è¡Œç«¯å£ ","link":"https://oldcamel.run/post/victoriametrics-an-zhuang/"},{"title":"flowableä¿®æ”¹æµç¨‹ç¼“å­˜å®ç°è‡ªç”±æµç¨‹","content":"\b flowableä¸­ä¼šå¯¹æµç¨‹å®šä¹‰åšç¼“å­˜å¤„ç†ï¼Œåœ¨å®ç°è‡ªç”±æµç¨‹çš„æ—¶å€™éœ€è¦åŠ¨æ€ç»™æµç¨‹å®šä¹‰æ·»åŠ èŠ‚ç‚¹ï¼Œé»˜è®¤çš„æ˜¯å¯¹æµç¨‹å®šä¹‰åšçš„ç¼“å­˜ï¼Œè¦æƒ³åœ¨æ¯ä¸ªæµç¨‹å®ä¾‹ä¸­åŠ¨æ€æ·»åŠ èŠ‚ç‚¹ï¼Œå¯ä»¥é€šè¿‡ä¿®æ”¹æµç¨‹ç¼“å­˜ï¼Œæ·»åŠ æ¯ä¸ªæµç¨‹å®ä¾‹çš„ç¼“å­˜ æ¶‰åŠåˆ°çš„ç±» ä¸»è¦æ­¥éª¤ æ­¥éª¤1ï¼šä¿®æ”¹BpmnJsonConverter,æ·»åŠ è‡ªå®šä¹‰å±æ€§ï¼ŒåŒºåˆ†æµç¨‹æ˜¯å¦ä¸ºè‡ªç”±æµç¨‹ æ­¥éª¤2ï¼šè®¾ç½®å…¨å±€çš„FlowableEventListener( FlowableEngineEventType. ACTIVITY_COMPLETED ),åœ¨æµç¨‹å¯åŠ¨çš„æ—¶å€™åˆ¤æ–­æµç¨‹æ˜¯å¦ä¸ºè‡ªç”±æµç¨‹ï¼Œå¦‚æœæ˜¯çš„è¯ï¼ŒæŠŠæµç¨‹çš„bpmnModelè½¬æˆxmlè®¾ç½®åˆ°è‡ªå·±å®šä¹‰çš„ç¼“å­˜å¯¹è±¡ä¸­ï¼Œç„¶åä¿å­˜åˆ°redisé‡Œã€‚ Redisé‡‡ç”¨äº†Hashæ–¹å¼å­˜å‚¨ï¼Œkeyä¸ºæµç¨‹å®ä¾‹ID,valueä¸ºè‡ªå®šä¹‰ç¼“å­˜ç±»ã€‚ æ­¥éª¤3ï¼šè‡ªç”±æµç¨‹æ·»åŠ èŠ‚ç‚¹çš„æ—¶å€™æå‰æŠŠæµç¨‹å®ä¾‹idæ”¾åœ¨requestä½œç”¨åŸŸä¸­ æ­¥éª¤4ï¼šå®šä¹‰ DefaultDeploymentCache å­ç±»é‡å†™getæ–¹æ³•ï¼Œå¦‚æœæ˜¯è‡ªå®šä¹‰æµç¨‹å¹¶ä¸”requestä½œç”¨åŸŸä¸­æœ‰æµç¨‹å®ä¾‹idå°±ä¸­redisä¸­è·å–æµç¨‹å®šä¹‰jsonè½¬æ¢æˆ ProcessDefinitionCacheEntry æ­¥éª¤5ï¼šå®šä¹‰è‡ªç”±æµç¨‹å‘½ä»¤ç±»ï¼Œè·å–æµç¨‹å®šä¹‰ç¼“å­˜ï¼Œç»™é‡Œé¢æ·»åŠ è¿çº¿å’ŒèŠ‚ç‚¹ æ­¥éª¤6ï¼šå¾€æ–°åŠ çš„èŠ‚ç‚¹ä¸Šè·³è½¬ æ”¾å…¥ç¼“å­˜ä¸»è¦ä»£ç  if (FlowUtils.isFreeProcess(entity.getProcessDefinitionId())) { //æ”¾å…¥ç¼“å­˜ ProcessDefinitionCacheEntry processDefinitionCacheEntry = managementService.executeCommand(new GetProcessDefinitionCacheEntryCmd(entity.getProcessDefinitionId())); CustomProcessDefinitionCacheEntry customProcessDefinitionCacheEntry = FlowUtils.parseCustomProcessDefinitionCacheEntry(processDefinitionCacheEntry); FreeProcessCaChe cache = SpringUtil.getBean(FreeProcessCaChe.class); cache.add(entity.getProcessInstanceId(), customProcessDefinitionCacheEntry); } ä¸‹æ–¹ä¸ºä»¥ä¸Šç”¨åˆ°çš„ç±»ä»¥åŠæ–¹æ³• è·å–ç¼“å­˜å‘½ä»¤ç±» public class GetProcessDefinitionCacheEntryCmd implements Command&lt;ProcessDefinitionCacheEntry&gt; { protected String processDefinitionId; public GetProcessDefinitionCacheEntryCmd(String processDefinitionId) { this.processDefinitionId = processDefinitionId; } @Override public ProcessDefinitionCacheEntry execute(CommandContext commandContext) { DeploymentManager deploymentManager = CommandContextUtil.getProcessEngineConfiguration().getDeploymentManager(); ProcessDefinitionCacheEntry processDefinitionCacheEntry = deploymentManager.getProcessDefinitionCache() .get(processDefinitionId); return processDefinitionCacheEntry; } } åºåˆ—åŒ–ç¼“å­˜å¯¹è±¡æ–¹æ³• public static CustomProcessDefinitionCacheEntry parseCustomProcessDefinitionCacheEntry(ProcessDefinitionCacheEntry processDefinitionCacheEntry) { CustomProcessDefinitionCacheEntry customProcessDefinitionCacheEntry = new CustomProcessDefinitionCacheEntry(); ProcessDefinition processDefinition = processDefinitionCacheEntry.getProcessDefinition(); String resourceName = processDefinition.getResourceName(); String deploymentId = processDefinition.getDeploymentId(); BpmnModel bpmnModel = processDefinitionCacheEntry.getBpmnModel(); BpmnXMLConverter bpmnXMLConverter = new BpmnXMLConverter(); byte[] bytes = bpmnXMLConverter.convertToXML(bpmnModel); customProcessDefinitionCacheEntry.setBpmnModel(bytes); customProcessDefinitionCacheEntry.setDeploymentId(deploymentId); customProcessDefinitionCacheEntry.setResourceName(resourceName); return customProcessDefinitionCacheEntry; } Rediså­˜å‚¨çš„å¯¹è±¡ @Data @AllArgsConstructor @NoArgsConstructor public class CustomProcessDefinitionCacheEntry implements Serializable { private static final long serialVersionUID = 6833801933658529071L; protected String deploymentId; protected String resourceName; protected byte[] bpmnModel; } æ·»åŠ åˆ°redisæ¥å£ public interface FreeProcessCaChe { CustomProcessDefinitionCacheEntry get(String key); void set(String key,CustomProcessDefinitionCacheEntry process); boolean contains(String key); void add(String key, CustomProcessDefinitionCacheEntry process); void remove(String key); void clear(); } æ·»åŠ åˆ°redisæ¥å£å®ç° @Service public class FreeProcessCaCheImpl implements FreeProcessCaChe { private String hashkey = &quot;freeprocesscache&quot;; @PostConstruct public void init() { boolean b = redisUtil.hasKey(hashkey); if (!b) { redisUtil.hset(hashkey, &quot;init&quot;, &quot;init&quot;); } } @Autowired private RedisUtil redisUtil; @Override public CustomProcessDefinitionCacheEntry get(String key) { if (StringUtils.isBlank(key)) { return null; } Object value = redisUtil.hget(hashkey, key); if (value != null) { JSONObject jb = (JSONObject) value; CustomProcessDefinitionCacheEntry customProcessDefinitionCacheEntry = JSON.toJavaObject(jb, CustomProcessDefinitionCacheEntry.class); return customProcessDefinitionCacheEntry; } return null; } @Override public boolean contains(String key) { return redisUtil.hHasKey(hashkey, key); } @Override public void add(String key, CustomProcessDefinitionCacheEntry process) { boolean hset = redisUtil.hset(hashkey, key, process); if (!hset) { throw new CheckErrorException(&quot;ç¼“å­˜è‡ªç”±æµç¨‹ä¿¡æ¯å¤±è´¥&quot;); } } @Override public void set(String key, CustomProcessDefinitionCacheEntry process) { boolean hset = redisUtil.hset(hashkey, key, process); if (!hset) { throw new CheckErrorException(&quot;æ›´æ–°è‡ªç”±æµç¨‹ä¿¡æ¯å¤±è´¥&quot;); } } @Override public void remove(String key) { redisUtil.hdel(hashkey, key); } @Override public void clear() { redisUtil.hmset(hashkey, Maps.newHashMap()); } } ä¿®æ”¹æµç¨‹é…ç½®ç±»æ·»åŠ è‡ªå®šä¹‰ç¼“å­˜è·å– springProcessEngineConfiguration.setProcessDefinitionCache(new CustomDeploymentCache&lt;&gt;()); è®¾ç½®çš„è‡ªå®šä¹‰ç¼“å­˜è·å–ç±»ï¼ˆä¸»è¦æ˜¯é‡å†™è·å–ç¼“å­˜çš„getæ–¹æ³•ï¼Œä»¥ä¸‹ä»…ä¸ºå‚è€ƒï¼‰ public class CustomDeploymentCache&lt;T&gt; extends DefaultDeploymentCache&lt;T&gt; { @Override public T get(String id) { T t = super.get(id); if(t==null){ return t; } String processInstanceId=null; try { processInstanceId = (String) FlowUtils.getRequest().getAttribute(&quot;processInstanceId&quot;); if (StringUtils.isBlank(processInstanceId)) { return t; } }catch (Exception e){ return t; } if(t instanceof ProcessDefinitionCacheEntry) { JSONObject jsonObject = new JSONObject(); Process mainProcess = ((ProcessDefinitionCacheEntry) t).getBpmnModel().getMainProcess(); String processGlobelSettings = &quot;processGlobelSettings&quot;; List&lt;ExtensionElement&gt; extensionElements = mainProcess.getExtensionElements().get(processGlobelSettings); if (extensionElements != null &amp;&amp; (!extensionElements.isEmpty())) { ExtensionElement extensionElement = extensionElements.get(0); String elementText = extensionElement.getElementText(); jsonObject = JSON.parseObject(elementText, Feature.OrderedField); } boolean freeProcess=jsonObject.getBooleanValue(&quot;freeProcess&quot;); if (!freeProcess) { return super.get(id); } else { //ä»ç¼“å­˜ä¸­å–å€¼ FreeProcessCaChe cache = SpringUtil.getBean(FreeProcessCaChe.class); CustomProcessDefinitionCacheEntry redisProcessDefinitionCacheEntry = cache.get(processInstanceId); BpmnModel bpmnModel=null; if(redisProcessDefinitionCacheEntry==null){ return t; }else { try { bpmnModel = FlowUtils.parseBpmnModelFromCustomProcessDefinitionCacheEntry(redisProcessDefinitionCacheEntry); } catch (Exception e) { e.printStackTrace(); } } Process process = bpmnModel.getMainProcess(); RuntimeService runtimeService = SpringUtil.getBean(RuntimeService.class); RepositoryService repositoryService = SpringUtil.getBean(RepositoryService.class); ProcessInstance processInstance = runtimeService.createProcessInstanceQuery().processInstanceId(processInstanceId).singleResult(); String processDefinitionId = processInstance.getProcessDefinitionId(); ProcessDefinition dataProcessDefinition = null; if(t!=null){ ProcessDefinitionCacheEntry pdc=(ProcessDefinitionCacheEntry)t; dataProcessDefinition=pdc.getProcessDefinition(); }else{ dataProcessDefinition=repositoryService.createProcessDefinitionQuery().processDefinitionId(processDefinitionId).singleResult(); } ProcessDefinitionCacheEntry customProcessDefinitionCacheEntry = new ProcessDefinitionCacheEntry(dataProcessDefinition,bpmnModel,process); return (T) customProcessDefinitionCacheEntry; } }else{ return super.get(id); } } } ","link":"https://oldcamel.run/post/flowable-xiu-gai-liu-cheng-huan-cun-shi-xian-zi-you-liu-cheng/"},{"title":"flowableè·å–å½“å‰ä»»åŠ¡èŠ‚ç‚¹ä¸‹ä¸€æ­¥ä¼šåˆ›å»ºçš„ç”¨æˆ·ä»»åŠ¡","content":"è·å–æ–¹æ³• package xxxx.util; import com.alibaba.fastjson.JSON; import com.alibaba.fastjson.JSONArray; import com.alibaba.fastjson.JSONObject; import com.alibaba.fastjson.serializer.SerializerFeature; import com.fasterxml.jackson.databind.JsonNode; import com.fasterxml.jackson.databind.ObjectMapper; import com.greenpineyu.fel.FelEngine; import com.greenpineyu.fel.FelEngineImpl; import com.greenpineyu.fel.context.FelContext; import org.apache.commons.lang.StringUtils; import org.apache.commons.lang3.BooleanUtils; import org.flowable.bpmn.converter.BpmnXMLConverter; import org.flowable.bpmn.model.*; import org.flowable.bpmn.model.Process; import org.flowable.engine.HistoryService; import org.flowable.engine.RepositoryService; import org.flowable.engine.RuntimeService; import org.flowable.engine.TaskService; import org.flowable.engine.history.HistoricProcessInstance; import org.flowable.engine.impl.bpmn.parser.BpmnParse; import org.flowable.engine.impl.bpmn.parser.BpmnParser; import org.flowable.engine.impl.cfg.ProcessEngineConfigurationImpl; import org.flowable.engine.impl.persistence.deploy.ProcessDefinitionCacheEntry; import org.flowable.engine.impl.persistence.entity.DeploymentEntity; import org.flowable.engine.impl.persistence.entity.DeploymentEntityManager; import org.flowable.engine.repository.ProcessDefinition; import org.flowable.bpmn.model.Task; import org.flowable.ui.modeler.domain.Model; import org.flowable.ui.modeler.serviceapi.ModelService; import org.springframework.web.context.request.RequestContextHolder; import org.springframework.web.context.request.ServletRequestAttributes; import javax.servlet.http.HttpServletRequest; import java.io.ByteArrayInputStream; import java.io.IOException; import java.sql.ResultSet; import java.sql.ResultSetMetaData; import java.util.*; public class FlowUtils { /** * è·å–ä¸‹ä¸€æ­¥éª¤çš„ç”¨æˆ·ä»»åŠ¡ * * @param repositoryService * @param taskService * @param map * @return */ public static List&lt;UserTask&gt; getNextUserTasks(RepositoryService repositoryService, TaskService taskService, org.flowable.task.api.Task task, Map&lt;String, Object&gt; map) { List&lt;UserTask&gt; data = new ArrayList&lt;&gt;(); ProcessDefinition processDefinition = repositoryService.createProcessDefinitionQuery().processDefinitionId(task.getProcessDefinitionId()).singleResult(); BpmnModel bpmnModel = repositoryService.getBpmnModel(processDefinition.getId()); Process mainProcess = bpmnModel.getMainProcess(); Collection&lt;FlowElement&gt; flowElements = mainProcess.getFlowElements(); String key = task.getTaskDefinitionKey(); FlowElement flowElement = bpmnModel.getFlowElement(key); next(flowElements, flowElement, map, data); return data; } public static void next(Collection&lt;FlowElement&gt; flowElements, FlowElement flowElement, Map&lt;String, Object&gt; map, List&lt;UserTask&gt; nextUser) { //å¦‚æœæ˜¯ç»“æŸèŠ‚ç‚¹ if (flowElement instanceof EndEvent) { //å¦‚æœæ˜¯å­ä»»åŠ¡çš„ç»“æŸèŠ‚ç‚¹ if (getSubProcess(flowElements, flowElement) != null) { flowElement = getSubProcess(flowElements, flowElement); } } //è·å–Taskçš„å‡ºçº¿ä¿¡æ¯--å¯ä»¥æ‹¥æœ‰å¤šä¸ª List&lt;SequenceFlow&gt; outGoingFlows = null; if (flowElement instanceof Task) { outGoingFlows = ((Task) flowElement).getOutgoingFlows(); } else if (flowElement instanceof Gateway) { outGoingFlows = ((Gateway) flowElement).getOutgoingFlows(); } else if (flowElement instanceof StartEvent) { outGoingFlows = ((StartEvent) flowElement).getOutgoingFlows(); } else if (flowElement instanceof SubProcess) { outGoingFlows = ((SubProcess) flowElement).getOutgoingFlows(); } else if (flowElement instanceof CallActivity) { outGoingFlows = ((CallActivity) flowElement).getOutgoingFlows(); } if (outGoingFlows != null &amp;&amp; outGoingFlows.size() &gt; 0) { //éå†æ‰€æœ‰çš„å‡ºçº¿--æ‰¾åˆ°å¯ä»¥æ­£ç¡®æ‰§è¡Œçš„é‚£ä¸€æ¡ for (SequenceFlow sequenceFlow : outGoingFlows) { //1.æœ‰è¡¨è¾¾å¼ï¼Œä¸”ä¸ºtrue //2.æ— è¡¨è¾¾å¼ String expression = sequenceFlow.getConditionExpression(); if (expression == null || Boolean.valueOf( String.valueOf( result(map, expression.substring(expression.lastIndexOf(&quot;{&quot;) + 1, expression.lastIndexOf(&quot;}&quot;)))))) { //å‡ºçº¿çš„ä¸‹ä¸€èŠ‚ç‚¹ String nextFlowElementID = sequenceFlow.getTargetRef(); if (checkSubProcess(nextFlowElementID, flowElements, nextUser)) { continue; } //æŸ¥è¯¢ä¸‹ä¸€èŠ‚ç‚¹çš„ä¿¡æ¯ FlowElement nextFlowElement = getFlowElementById(nextFlowElementID, flowElements); //è°ƒç”¨æµç¨‹ if (nextFlowElement instanceof CallActivity) { CallActivity ca = (CallActivity) nextFlowElement; if (ca.getLoopCharacteristics() != null) { UserTask userTask = new UserTask(); userTask.setId(ca.getId()); userTask.setId(ca.getId()); userTask.setLoopCharacteristics(ca.getLoopCharacteristics()); userTask.setName(ca.getName()); nextUser.add(userTask); } next(flowElements, nextFlowElement, map, nextUser); } //ç”¨æˆ·ä»»åŠ¡ if (nextFlowElement instanceof UserTask) { nextUser.add((UserTask) nextFlowElement); } //æ’ä»–ç½‘å…³ else if (nextFlowElement instanceof ExclusiveGateway) { next(flowElements, nextFlowElement, map, nextUser); } //å¹¶è¡Œç½‘å…³ else if (nextFlowElement instanceof ParallelGateway) { next(flowElements, nextFlowElement, map, nextUser); } //æ¥æ”¶ä»»åŠ¡ else if (nextFlowElement instanceof ReceiveTask) { next(flowElements, nextFlowElement, map, nextUser); } //æœåŠ¡ä»»åŠ¡ else if (nextFlowElement instanceof ServiceTask) { next(flowElements, nextFlowElement, map, nextUser); } //å­ä»»åŠ¡çš„èµ·ç‚¹ else if (nextFlowElement instanceof StartEvent) { next(flowElements, nextFlowElement, map, nextUser); } //ç»“æŸèŠ‚ç‚¹ else if (nextFlowElement instanceof EndEvent) { next(flowElements, nextFlowElement, map, nextUser); } } } } } /** * åˆ¤æ–­æ˜¯å¦æ˜¯å¤šå®ä¾‹å­æµç¨‹å¹¶ä¸”éœ€è¦è®¾ç½®é›†åˆç±»å‹å˜é‡ */ public static boolean checkSubProcess(String Id, Collection&lt;FlowElement&gt; flowElements, List&lt;UserTask&gt; nextUser) { for (FlowElement flowElement1 : flowElements) { if (flowElement1 instanceof SubProcess &amp;&amp; flowElement1.getId().equals(Id)) { SubProcess sp = (SubProcess) flowElement1; if (sp.getLoopCharacteristics() != null) { String inputDataItem = sp.getLoopCharacteristics().getInputDataItem(); UserTask userTask = new UserTask(); userTask.setId(sp.getId()); userTask.setLoopCharacteristics(sp.getLoopCharacteristics()); userTask.setName(sp.getName()); nextUser.add(userTask); return true; } } } return false; } /** * æŸ¥è¯¢ä¸€ä¸ªèŠ‚ç‚¹çš„æ˜¯å¦å­ä»»åŠ¡ä¸­çš„èŠ‚ç‚¹ï¼Œå¦‚æœæ˜¯ï¼Œè¿”å›å­ä»»åŠ¡ * * @param flowElements å…¨æµç¨‹çš„èŠ‚ç‚¹é›†åˆ * @param flowElement å½“å‰èŠ‚ç‚¹ * @return */ public static FlowElement getSubProcess(Collection&lt;FlowElement&gt; flowElements, FlowElement flowElement) { for (FlowElement flowElement1 : flowElements) { if (flowElement1 instanceof SubProcess) { for (FlowElement flowElement2 : ((SubProcess) flowElement1).getFlowElements()) { if (flowElement.equals(flowElement2)) { return flowElement1; } } } } return null; } /** * æ ¹æ®IDæŸ¥è¯¢æµç¨‹èŠ‚ç‚¹å¯¹è±¡, å¦‚æœæ˜¯å­ä»»åŠ¡ï¼Œåˆ™è¿”å›å­ä»»åŠ¡çš„å¼€å§‹èŠ‚ç‚¹ * * @param Id èŠ‚ç‚¹ID * @param flowElements æµç¨‹èŠ‚ç‚¹é›†åˆ * @return */ public static FlowElement getFlowElementById(String Id, Collection&lt;FlowElement&gt; flowElements) { for (FlowElement flowElement : flowElements) { if (flowElement.getId().equals(Id)) { //å¦‚æœæ˜¯å­ä»»åŠ¡ï¼Œåˆ™æŸ¥è¯¢å‡ºå­ä»»åŠ¡çš„å¼€å§‹èŠ‚ç‚¹ if (flowElement instanceof SubProcess) { return getStartFlowElement(((SubProcess) flowElement).getFlowElements()); } return flowElement; } if (flowElement instanceof SubProcess) { FlowElement flowElement1 = getFlowElementById(Id, ((SubProcess) flowElement).getFlowElements()); if (flowElement1 != null) { return flowElement1; } } } return null; } /** * è¿”å›æµç¨‹çš„å¼€å§‹èŠ‚ç‚¹ * * @param flowElements èŠ‚ç‚¹é›†åˆ * @description: */ public static FlowElement getStartFlowElement(Collection&lt;FlowElement&gt; flowElements) { for (FlowElement flowElement : flowElements) { if (flowElement instanceof StartEvent) { return flowElement; } } return null; } /** * æ ¡éªŒelè¡¨è¾¾ç¤ºä¾‹ * * @param map * @param expression * @return */ public static Object result(Map&lt;String, Object&gt; map, String expression) { FelEngine fel = new FelEngineImpl(); FelContext ctx = fel.getContext(); for (Map.Entry&lt;String, Object&gt; entry : map.entrySet()) { ctx.set(entry.getKey(), entry.getValue()); } Object result = fel.eval(expression); return result; } } maven elè¡¨è¾¾å¼æ ¡éªŒçš„ä¾èµ– &lt;dependency&gt; &lt;groupId&gt;org.eweb4j&lt;/groupId&gt; &lt;artifactId&gt;fel&lt;/artifactId&gt; &lt;version&gt;0.8&lt;/version&gt; &lt;/dependency&gt; ","link":"https://oldcamel.run/post/flowable-nexttask/"},{"title":" Beam+KafkaåŒæ­¥Oracleæ—¥å¿—åˆ°ClickHouse","content":"ç”¨kafkaçš„oracle connectæ’ä»¶å­˜å‚¨oracleæ—¥å¿—ï¼Œç”¨beamè¯»å–kafkaæ•°æ®æ’å…¥åˆ°clickhouseä¸­ oracleé…ç½® archivelogæ¨¡å¼ æ•°æ®åº“å¿…é¡»å¤„äºarchivelogæ¨¡å¼ï¼Œå¹¶ä¸”å¿…é¡»å¯ç”¨è¡¥å……æ—¥å¿—è®°å½• åœ¨æ•°æ®åº“æœåŠ¡å™¨ä¸Šæ‰§è¡Œ sqlplus / as sysdba SQL&gt;shutdown immediate SQL&gt;startup mount SQL&gt;alter database archivelog; SQL&gt;alter database open; å¯ç”¨è¡¥å……æ—¥å¿—è®°å½• sqlplus / as sysdba SQL&gt;alter database add supplemental log data (all) columns; kafkaé…ç½® æ’ä»¶é…ç½®æ–‡ä»¶ åœ¨$KAFKA_HOME/config ç›®å½•æ–°å»ºOracleSourceConnector.propertiesé…ç½®æ–‡ä»¶ï¼Œ æ–‡ä»¶å†…ç¤ºä¾‹é…ç½® name=oracle-logminer-connector connector.class=com.ecer.kafka.connect.oracle.OracleSourceConnector db.name.alias=test tasks.max=1 topic=cdctest db.name=testdb db.hostname=10.1.X.X db.port=1521 db.user=kminer db.user.password=kminerpass db.fetch.size=1 table.whitelist=TEST.*,TEST2.TABLE2 table.blacklist=TEST2.TABLE3 parse.dml.data=true reset.offset=false multitenant=false é…ç½®è¯´æ˜ Name Type Description name String è¿æ¥å™¨åç§° connector.class String æ­¤è¿æ¥å™¨çš„Javaç±»çš„åç§° db.name.alias String æ•°æ®åº“çš„æ ‡è¯†ç¬¦åç§°ï¼ˆä¾‹å¦‚Testï¼ŒDevï¼ŒProdï¼‰æˆ–ç”¨äºæ ‡è¯†æ•°æ®åº“çš„ç‰¹å®šåç§°è¯¥åç§°å°†ç”¨ä½œä¸»é¢˜å’Œæ¶æ„åç§°çš„æ ‡å¤´ tasks.max Integer åˆ›å»ºçš„æœ€å¤§ä»»åŠ¡æ•°æ­¤è¿æ¥å™¨ä½¿ç”¨å•ä¸ªä»»åŠ¡. topic String æ¶ˆæ¯å°†å†™å…¥çš„ä¸»é¢˜çš„åç§°å¦‚æœè®¾ç½®äº†å€¼ï¼Œåˆ™æ‰€æœ‰æ¶ˆæ¯éƒ½å°†å†™å…¥æ­¤å£°æ˜çš„topicï¼Œå¦‚æœæœªè®¾ç½®ï¼Œåˆ™å°†ä¸ºæ¯ä¸ªæ•°æ®åº“è¡¨åŠ¨æ€åˆ›å»ºä¸€ä¸ªä¸»é¢˜ db.name String è¦è¿æ¥çš„æ•°æ®åº“çš„æœåŠ¡åç§°æˆ–sidé€šå¸¸ä½¿ç”¨æ•°æ®åº“æœåŠ¡åç§° db.hostname String Oracleæ•°æ®åº“æœåŠ¡å™¨çš„IPåœ°å€æˆ–ä¸»æœºå db.port Integer Oracleæ•°æ®åº“æœåŠ¡å™¨çš„ç«¯å£å· db.user String æ•°æ®åº“çš„ç”¨æˆ·å db.user.password String æ•°æ®åº“ç”¨æˆ·å¯†ç  db.fetch.size Integer æ­¤é…ç½®å±æ€§è®¾ç½®Oracleè¡Œæå–å¤§å°å€¼ table.whitelist String ç™½åå•æ ¼å¼ä¸ºç”¨æˆ·å.è¡¨åç”¨é€—å·åˆ†éš”ï¼Œå¦‚æœè¦æ”¶é›†ç”¨æˆ·ä¸‹çš„æ‰€æœ‰çš„è¡¨ï¼Œç”¨ç”¨æˆ·å.* parse.dml.data Boolean å¦‚æœä¸ºtrueï¼Œåˆ™å°†æ•è·çš„sql DMLè¯­å¥è§£æä¸ºå­—æ®µå’Œå€¼;å¦‚æœä¸ºfalseï¼Œåˆ™ä»…å‘å¸ƒsql DMLè¯­å¥ reset.offset Boolean å¦‚æœä¸ºtrueï¼Œåˆ™åœ¨è¿æ¥å™¨å¯åŠ¨æ—¶å°†åç§»å€¼è®¾ç½®ä¸ºæ•°æ®åº“çš„å½“å‰SCNå¦‚æœä¸ºfalseï¼Œåˆ™è¿æ¥å™¨å°†ä»ä¸Šä¸€ä¸ªåç§»å€¼å¼€å§‹ start.scn Long å¦‚æœè®¾ç½®æ­¤å±æ€§ï¼Œåˆ™å°†åç§»å€¼è®¾ç½®ä¸ºè¯¥æŒ‡å®šå€¼ï¼Œå¹¶ä¸”logminerå°†ä»æ­¤SCNå¯åŠ¨å¦‚æœè¿æ¥å™¨å¸Œæœ›ä»æ‰€éœ€çš„SCNå¯åŠ¨ï¼Œåˆ™å¯ä»¥ä½¿ç”¨æ­¤å±æ€§ multitenant Boolean å¦‚æœä¸ºtrueï¼Œåˆ™å¯ç”¨å¤šç§Ÿæˆ·æ”¯æŒå¦‚æœä¸ºfalseï¼Œå°†ä½¿ç”¨å•å®ä¾‹é…ç½® table.blacklist String é»‘åå•æ ¼å¼ä¸ºç”¨æˆ·å.è¡¨åç”¨é€—å·åˆ†éš”ï¼Œå¦‚æœè¦æ”¶é›†ç”¨æˆ·ä¸‹çš„æ‰€æœ‰çš„è¡¨ï¼Œç”¨ç”¨æˆ·å.*. dml.types String ä»¥é€—å·åˆ†éš”çš„DMLæ“ä½œåˆ—è¡¨ï¼ˆINSERTï¼ŒUPDATEï¼ŒDELETEï¼‰å¦‚æœæœªæŒ‡å®šï¼Œåˆ™å°†æ‰§è¡Œå¤åˆ¶æ‰€æœ‰DMLæ“ä½œçš„é»˜è®¤è¡Œä¸ºï¼Œå¦‚æœæŒ‡å®šï¼Œåˆ™ä»…æ•è·æŒ‡å®šçš„æ“ä½œ ä»¥ä¸Šä¸ºæœºç¿»ï¼ŒåŸè¯´æ˜æ–‡æ¡£åœ°å€è§ https://github.com/erdemcer/kafka-connect-oracle/blob/master/README.md æ·»åŠ kafkaæ’ä»¶åŒ… kafka-connect-oracle-1.0.68.jarå’Œç›¸åº”ç‰ˆæœ¬çš„oracleçš„é©±åŠ¨åŒ…æ”¾å…¥åˆ° $KAFKA_HOME/libæ–‡ä»¶å¤¹ä¸­ å¯åŠ¨æ’ä»¶ cd $KAFKA_HOME ./bin/connect-standalone.sh ./config/connect-standalone.properties ./config/OracleSourceConnector.properties kafkaæ¶ˆæ¯è¯´æ˜ å­—æ®µ è¯´æ˜ SCN æ•°æ®åº“æ—¥å¿—æ—¶é—´ SEG_OWNER æ•°æ®åº“ç”¨æˆ·å TABLE_NAME æ•°æ®åº“è¡¨å TIMESTAMP æ—¶é—´æˆ³ SQL_REDO æ‰§è¡Œçš„sqlè¯­å¥ OPERATION æ“ä½œ DATA æ›´æ–°åçš„æ•°æ® BEFORE æ›´æ–°å‰çš„æ•°æ® ClickHouse å»ºè¡¨ CREATE TABLE default.ODB_LOG ( SCN Int64, SEG_OWNER String, TABLE_NAME String, `TIMESTAMP` DateTime, SQL_REDO String, OPERATION String, `DATA` String, `BEFORE` String ) ENGINE = MergeTree() PARTITION BY toYYYYMM(TIMESTAMP) ORDER BY (SEG_OWNER,TABLE_NAME,OPERATION,TIMESTAMP ) SETTINGS index_granularity=8192; Beam ç¨‹åº mavenæ·»åŠ  ä¾èµ–åŒ… &lt;dependency&gt; &lt;groupId&gt;org.apache.beam&lt;/groupId&gt; &lt;artifactId&gt;beam-sdks-java-io-clickhouse&lt;/artifactId&gt; &lt;version&gt;${beam.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.beam&lt;/groupId&gt; &lt;artifactId&gt;beam-sdks-java-io-kafka&lt;/artifactId&gt; &lt;version&gt;${beam.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.4.1&lt;/version&gt; &lt;!-- &lt;scope&gt;runtime&lt;/scope&gt;--&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.beam&lt;/groupId&gt; &lt;artifactId&gt;beam-runners-flink-1.11&lt;/artifactId&gt; &lt;version&gt;${beam.version}&lt;/version&gt; &lt;/dependency&gt; åˆ›å»ºé…ç½®å‚æ•°ç±» package com.yunzainfo.kol.config; import org.apache.beam.runners.flink.FlinkPipelineOptions; import org.apache.beam.sdk.options.Default; import org.apache.beam.sdk.options.Description; import org.apache.beam.sdk.options.PipelineOptions; import org.apache.beam.sdk.options.Validation.Required; /** * Created by IntelliJ IDEA * TODO: TODO * * @author: å¾æˆ * Date: 2020/12/2 * Time: 10:08 ä¸Šåˆ * Email: old_camel@163.com */ public interface KolOptions extends FlinkPipelineOptions { //public interface OdcOptions extends PipelineOptions { @Description(&quot;kafkaé“¾æ¥åœ°å€&quot;) @Required @Default.String(&quot;xxx.xxx.xxx.xxx:9092&quot;) String getBootstrapServers(); void setBootstrapServers(String value); @Description(&quot;kafka æ¶ˆæ¯ä¸»é¢˜&quot;) @Required @Default.String(&quot;kol&quot;) String getTopic(); void setTopic(String value); @Description(&quot;åˆ†ç»„id&quot;) @Required @Default.String(&quot;bdf_kol&quot;) String getGroupid(); void setGroupid(String value); @Description(&quot;kafkaç”¨æˆ·&quot;) @Required @Default.String(&quot;xxx&quot;) String getKafkaUsername(); void setKafkaUsername(String kafkaUsername); @Description(&quot;kafkaå¯†ç &quot;) @Required @Default.String(&quot;xxx&quot;) String getKafkaPassword(); void setKafkaPassword(String kafkaPassword); @Description(&quot;ClickHouseè¿æ¥åœ°å€&quot;) @Required @Default.String(&quot;xxx.xxx.xxx.xxx:xxxx/default&quot;) String getClickHouseUrl(); void setClickHouseUrl(String value); @Description(&quot;ClickHouseç”¨æˆ·å&quot;) @Required @Default.String(&quot;root&quot;) String getClickHouseUserName(); void setClickHouseUserName(String value); @Description(&quot;ClickHouseå¯†ç &quot;) @Required @Default.String(&quot;xxx&quot;) String getClickHousePassword(); void setClickHousePassword(String value); @Description(&quot;ClickHouseè¡¨&quot;) @Required @Default.String(&quot;ODB_LOG&quot;) String getClickHouseTableName(); void setClickHouseTableName(String value); } kafkaæ¶ˆæ¯æ˜ å°„ç±» package com.yunzainfo.kol.model; import org.codehaus.jackson.annotate.JsonProperty; import java.io.Serializable; import java.util.Date; /** * Created by IntelliJ IDEA * TODO: TODO * * @author: å¾æˆ * Date: 2020/11/30 * Time: 2:32 ä¸‹åˆ * Email: old_camel@163.com */ public class Payload implements Serializable { private static final long serialVersionUID = 1L; @JsonProperty(value = &quot;SCN&quot;) private Long SCN; @JsonProperty(value = &quot;SEG_OWNER&quot;) private String SEG_OWNER; @JsonProperty(value = &quot;TABLE_NAME&quot;) private String TABLE_NAME; @JsonProperty(value = &quot;TIMESTAMP&quot;) private Long TIMESTAMP; @JsonProperty(value = &quot;SQL_REDO&quot;) private String SQL_REDO; @JsonProperty(value = &quot;OPERATION&quot;) private String OPERATION; @JsonProperty(value = &quot;data&quot;) private Object DATA; @JsonProperty(value = &quot;before&quot;) private Object BEFORE; public static long getSerialVersionUID() { return serialVersionUID; } public Long getSCN() { return SCN; } public void setSCN(Long SCN) { this.SCN = SCN; } public String getSEG_OWNER() { return SEG_OWNER; } public void setSEG_OWNER(String SEG_OWNER) { this.SEG_OWNER = SEG_OWNER; } public String getTABLE_NAME() { return TABLE_NAME; } public void setTABLE_NAME(String TABLE_NAME) { this.TABLE_NAME = TABLE_NAME; } public Long getTIMESTAMP() { return TIMESTAMP; } public void setTIMESTAMP(Long TIMESTAMP) { this.TIMESTAMP = TIMESTAMP; } public String getSQL_REDO() { return SQL_REDO; } public void setSQL_REDO(String SQL_REDO) { this.SQL_REDO = SQL_REDO; } public String getOPERATION() { return OPERATION; } public void setOPERATION(String OPERATION) { this.OPERATION = OPERATION; } public Object getDATA() { return DATA; } public void setDATA(Object DATA) { this.DATA = DATA; } public Object getBEFORE() { return BEFORE; } public void setBEFORE(Object BEFORE) { this.BEFORE = BEFORE; } @Override public String toString() { return &quot;Payload{&quot; + &quot;SCN=&quot; + SCN + &quot;, SEG_OWNER='&quot; + SEG_OWNER + '\\'' + &quot;, TABLE_NAME='&quot; + TABLE_NAME + '\\'' + &quot;, TIMESTAMP=&quot; + TIMESTAMP + &quot;, SQL_REDO='&quot; + SQL_REDO + '\\'' + &quot;, OPERATION='&quot; + OPERATION + '\\'' + &quot;, DATA='&quot; + DATA + '\\'' + &quot;, BEFORE='&quot; + BEFORE + '\\'' + '}'; } } ä¸»ç¨‹åº package com.yunzainfo.kol; import com.yunzainfo.kol.config.KolOptions; import com.yunzainfo.kol.model.Payload; import org.apache.beam.runners.flink.FlinkRunner; import org.apache.beam.sdk.Pipeline; import org.apache.beam.sdk.coders.SerializableCoder; import org.apache.beam.sdk.io.clickhouse.ClickHouseIO; import org.apache.beam.sdk.io.kafka.KafkaIO; import org.apache.beam.sdk.options.PipelineOptionsFactory; import org.apache.beam.sdk.schemas.Schema; import org.apache.beam.sdk.transforms.DoFn; import org.apache.beam.sdk.transforms.ParDo; import org.apache.beam.sdk.transforms.Values; import org.apache.beam.sdk.values.Row; import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap; import org.apache.kafka.clients.CommonClientConfigs; import org.apache.kafka.clients.consumer.ConsumerConfig; import org.apache.kafka.common.config.SaslConfigs; import org.apache.kafka.common.serialization.StringDeserializer; import org.codehaus.jackson.map.ObjectMapper; import org.joda.time.DateTime; import org.joda.time.Duration; import java.io.IOException; import java.util.LinkedHashMap; import java.util.Map; /** * Created by IntelliJ IDEA * TODO: TODO * * @author: å¾æˆ * Date: 2020/11/27 * Time: 5:49 ä¸‹åˆ * Email: old_camel@163.com */ public class KolApp { public static void main(String[] args) { KolOptions options = PipelineOptionsFactory.fromArgs(args).as(KolOptions.class); options.setRunner(FlinkRunner.class); runOdc(options); } public static Object isnull(Object o) { if (o == null) { return &quot;&quot;; } else { return o; } } public static void runOdc(KolOptions options) { Pipeline p = Pipeline.create(options); final Map&lt;String, Object&gt; immutableMap = new ImmutableMap.Builder&lt;String, Object&gt;() .put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;) .put(ConsumerConfig.GROUP_ID_CONFIG, options.getGroupid()) .put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true) .put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, &quot;SASL_PLAINTEXT&quot;) .put(SaslConfigs.SASL_MECHANISM, &quot;PLAIN&quot;) .put(SaslConfigs.SASL_JAAS_CONFIG, &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\&quot;&quot; + options.getKafkaUsername() + &quot;\\&quot; password=\\&quot;&quot; + options.getKafkaPassword() + &quot;\\&quot;;&quot;) .build(); final Schema ckType = Schema.of( Schema.Field.of(&quot;SCN&quot;, Schema.FieldType.INT64.withNullable(true)), Schema.Field.of(&quot;SEG_OWNER&quot;, Schema.FieldType.STRING.withNullable(true)), Schema.Field.of(&quot;TABLE_NAME&quot;, Schema.FieldType.STRING.withNullable(true)), Schema.Field.of(&quot;TIMESTAMP&quot;, Schema.FieldType.DATETIME.withNullable(true)), Schema.Field.of(&quot;SQL_REDO&quot;, Schema.FieldType.STRING.withNullable(true)), Schema.Field.of(&quot;OPERATION&quot;, Schema.FieldType.STRING.withNullable(true)), Schema.Field.of(&quot;DATA&quot;, Schema.FieldType.STRING.withNullable(true)), Schema.Field.of(&quot;BEFORE&quot;, Schema.FieldType.STRING.withNullable(true)) ); p.apply(&quot;è¯»å–æ¶ˆæ¯&quot;, KafkaIO.&lt;String, String&gt;read() .withBootstrapServers(options.getBootstrapServers()) .withTopic(options.getTopic()) .withKeyDeserializer(StringDeserializer.class) .withValueDeserializer(StringDeserializer.class) .withOffsetConsumerConfigOverrides(ImmutableMap.&lt;String, Object&gt;of(ConsumerConfig.GROUP_ID_CONFIG, options.getGroupid())) .withConsumerConfigUpdates(immutableMap) .withReadCommitted() .withoutMetadata()) .apply(Values.create()) .apply(&quot;è½¬ä¹‰Payload&quot;, ParDo.of(new DoFn&lt;String, Payload&gt;() { private static final long serialVersionUID = 1L; @ProcessElement public void processElement(ProcessContext ctx) { String element = ctx.element(); ObjectMapper mapper = new ObjectMapper(); try { LinkedHashMap map = mapper.readValue(element, LinkedHashMap.class); //Object payload = data.get(&quot;payload&quot;); LinkedHashMap payloadMap = (LinkedHashMap) map.get(&quot;payload&quot;); Payload payload = mapper.convertValue(payloadMap, Payload.class); payload.setDATA(mapper.writeValueAsString(payload.getDATA())); payload.setBEFORE(mapper.writeValueAsString(payload.getBEFORE())); ctx.output(payload); } catch (IOException e) { e.printStackTrace(); } } })) .setCoder(SerializableCoder.of(Payload.class)) .apply(&quot;è½¬æ¢Row&quot;, ParDo.of(new DoFn&lt;Payload, Row&gt;() { @ProcessElement public void processElement(ProcessContext cxt) { Payload payload = cxt.element(); Row alarmRow = Row.withSchema(ckType).addValues( isnull(payload.getSCN()), isnull(payload.getSEG_OWNER()), isnull(payload.getTABLE_NAME()), isnull(new DateTime(payload.getTIMESTAMP())), isnull(payload.getSQL_REDO()), isnull(payload.getOPERATION()), isnull(payload.getDATA()), isnull(payload.getBEFORE()) ).build(); cxt.output(alarmRow); } })).setRowSchema(ckType) .apply(&quot;å†™å…¥ClickHouseæ•°æ®åº“&quot;, ClickHouseIO.&lt;Row&gt;write(&quot;jdbc:clickhouse://&quot; + options.getClickHouseUrl() + &quot;?username=&quot; + options.getClickHouseUserName() + &quot;&amp;password=&quot; + options.getClickHousePassword(), options.getClickHouseTableName()) .withMaxRetries(3)//é‡è¯•æ¬¡æ•° .withInsertDeduplicate(true)//é‡å¤æ•°æ®æ˜¯å¦åˆ é™¤ .withMaxInsertBlockSize(1)//æ·»åŠ æœ€å¤§å—çš„å¤§å° .withInitialBackoff(Duration.standardSeconds(5))//åˆå§‹é€€å›æ—¶é—´ .withInsertDistributedSync(false) ); p.run().waitUntilFinish(); } } æŠŠbeamåŒ…æ”¾åœ¨flinkä¸Šè¿è¡Œ clickhouseä¸­çš„æ•°æ® ","link":"https://oldcamel.run/post/apache-beam-tong-bu-oracle-ri-zhi-dao-clickhouse/"}]}