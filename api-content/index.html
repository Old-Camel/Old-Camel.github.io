{"posts":[{"title":"flowable获取当前任务节点下一步会创建的用户任务","content":"获取方法 package xxxx.util; import com.alibaba.fastjson.JSON; import com.alibaba.fastjson.JSONArray; import com.alibaba.fastjson.JSONObject; import com.alibaba.fastjson.serializer.SerializerFeature; import com.fasterxml.jackson.databind.JsonNode; import com.fasterxml.jackson.databind.ObjectMapper; import com.greenpineyu.fel.FelEngine; import com.greenpineyu.fel.FelEngineImpl; import com.greenpineyu.fel.context.FelContext; import org.apache.commons.lang.StringUtils; import org.apache.commons.lang3.BooleanUtils; import org.flowable.bpmn.converter.BpmnXMLConverter; import org.flowable.bpmn.model.*; import org.flowable.bpmn.model.Process; import org.flowable.engine.HistoryService; import org.flowable.engine.RepositoryService; import org.flowable.engine.RuntimeService; import org.flowable.engine.TaskService; import org.flowable.engine.history.HistoricProcessInstance; import org.flowable.engine.impl.bpmn.parser.BpmnParse; import org.flowable.engine.impl.bpmn.parser.BpmnParser; import org.flowable.engine.impl.cfg.ProcessEngineConfigurationImpl; import org.flowable.engine.impl.persistence.deploy.ProcessDefinitionCacheEntry; import org.flowable.engine.impl.persistence.entity.DeploymentEntity; import org.flowable.engine.impl.persistence.entity.DeploymentEntityManager; import org.flowable.engine.repository.ProcessDefinition; import org.flowable.bpmn.model.Task; import org.flowable.ui.modeler.domain.Model; import org.flowable.ui.modeler.serviceapi.ModelService; import org.springframework.web.context.request.RequestContextHolder; import org.springframework.web.context.request.ServletRequestAttributes; import javax.servlet.http.HttpServletRequest; import java.io.ByteArrayInputStream; import java.io.IOException; import java.sql.ResultSet; import java.sql.ResultSetMetaData; import java.util.*; public class FlowUtils { /** * 获取下一步骤的用户任务 * * @param repositoryService * @param taskService * @param map * @return */ public static List&lt;UserTask&gt; getNextUserTasks(RepositoryService repositoryService, TaskService taskService, org.flowable.task.api.Task task, Map&lt;String, Object&gt; map) { List&lt;UserTask&gt; data = new ArrayList&lt;&gt;(); ProcessDefinition processDefinition = repositoryService.createProcessDefinitionQuery().processDefinitionId(task.getProcessDefinitionId()).singleResult(); BpmnModel bpmnModel = repositoryService.getBpmnModel(processDefinition.getId()); Process mainProcess = bpmnModel.getMainProcess(); Collection&lt;FlowElement&gt; flowElements = mainProcess.getFlowElements(); String key = task.getTaskDefinitionKey(); FlowElement flowElement = bpmnModel.getFlowElement(key); next(flowElements, flowElement, map, data); return data; } public static void next(Collection&lt;FlowElement&gt; flowElements, FlowElement flowElement, Map&lt;String, Object&gt; map, List&lt;UserTask&gt; nextUser) { //如果是结束节点 if (flowElement instanceof EndEvent) { //如果是子任务的结束节点 if (getSubProcess(flowElements, flowElement) != null) { flowElement = getSubProcess(flowElements, flowElement); } } //获取Task的出线信息--可以拥有多个 List&lt;SequenceFlow&gt; outGoingFlows = null; if (flowElement instanceof Task) { outGoingFlows = ((Task) flowElement).getOutgoingFlows(); } else if (flowElement instanceof Gateway) { outGoingFlows = ((Gateway) flowElement).getOutgoingFlows(); } else if (flowElement instanceof StartEvent) { outGoingFlows = ((StartEvent) flowElement).getOutgoingFlows(); } else if (flowElement instanceof SubProcess) { outGoingFlows = ((SubProcess) flowElement).getOutgoingFlows(); } else if (flowElement instanceof CallActivity) { outGoingFlows = ((CallActivity) flowElement).getOutgoingFlows(); } if (outGoingFlows != null &amp;&amp; outGoingFlows.size() &gt; 0) { //遍历所有的出线--找到可以正确执行的那一条 for (SequenceFlow sequenceFlow : outGoingFlows) { //1.有表达式，且为true //2.无表达式 String expression = sequenceFlow.getConditionExpression(); if (expression == null || Boolean.valueOf( String.valueOf( result(map, expression.substring(expression.lastIndexOf(&quot;{&quot;) + 1, expression.lastIndexOf(&quot;}&quot;)))))) { //出线的下一节点 String nextFlowElementID = sequenceFlow.getTargetRef(); if (checkSubProcess(nextFlowElementID, flowElements, nextUser)) { continue; } //查询下一节点的信息 FlowElement nextFlowElement = getFlowElementById(nextFlowElementID, flowElements); //调用流程 if (nextFlowElement instanceof CallActivity) { CallActivity ca = (CallActivity) nextFlowElement; if (ca.getLoopCharacteristics() != null) { UserTask userTask = new UserTask(); userTask.setId(ca.getId()); userTask.setId(ca.getId()); userTask.setLoopCharacteristics(ca.getLoopCharacteristics()); userTask.setName(ca.getName()); nextUser.add(userTask); } next(flowElements, nextFlowElement, map, nextUser); } //用户任务 if (nextFlowElement instanceof UserTask) { nextUser.add((UserTask) nextFlowElement); } //排他网关 else if (nextFlowElement instanceof ExclusiveGateway) { next(flowElements, nextFlowElement, map, nextUser); } //并行网关 else if (nextFlowElement instanceof ParallelGateway) { next(flowElements, nextFlowElement, map, nextUser); } //接收任务 else if (nextFlowElement instanceof ReceiveTask) { next(flowElements, nextFlowElement, map, nextUser); } //服务任务 else if (nextFlowElement instanceof ServiceTask) { next(flowElements, nextFlowElement, map, nextUser); } //子任务的起点 else if (nextFlowElement instanceof StartEvent) { next(flowElements, nextFlowElement, map, nextUser); } //结束节点 else if (nextFlowElement instanceof EndEvent) { next(flowElements, nextFlowElement, map, nextUser); } } } } } /** * 判断是否是多实例子流程并且需要设置集合类型变量 */ public static boolean checkSubProcess(String Id, Collection&lt;FlowElement&gt; flowElements, List&lt;UserTask&gt; nextUser) { for (FlowElement flowElement1 : flowElements) { if (flowElement1 instanceof SubProcess &amp;&amp; flowElement1.getId().equals(Id)) { SubProcess sp = (SubProcess) flowElement1; if (sp.getLoopCharacteristics() != null) { String inputDataItem = sp.getLoopCharacteristics().getInputDataItem(); UserTask userTask = new UserTask(); userTask.setId(sp.getId()); userTask.setLoopCharacteristics(sp.getLoopCharacteristics()); userTask.setName(sp.getName()); nextUser.add(userTask); return true; } } } return false; } /** * 查询一个节点的是否子任务中的节点，如果是，返回子任务 * * @param flowElements 全流程的节点集合 * @param flowElement 当前节点 * @return */ public static FlowElement getSubProcess(Collection&lt;FlowElement&gt; flowElements, FlowElement flowElement) { for (FlowElement flowElement1 : flowElements) { if (flowElement1 instanceof SubProcess) { for (FlowElement flowElement2 : ((SubProcess) flowElement1).getFlowElements()) { if (flowElement.equals(flowElement2)) { return flowElement1; } } } } return null; } /** * 根据ID查询流程节点对象, 如果是子任务，则返回子任务的开始节点 * * @param Id 节点ID * @param flowElements 流程节点集合 * @return */ public static FlowElement getFlowElementById(String Id, Collection&lt;FlowElement&gt; flowElements) { for (FlowElement flowElement : flowElements) { if (flowElement.getId().equals(Id)) { //如果是子任务，则查询出子任务的开始节点 if (flowElement instanceof SubProcess) { return getStartFlowElement(((SubProcess) flowElement).getFlowElements()); } return flowElement; } if (flowElement instanceof SubProcess) { FlowElement flowElement1 = getFlowElementById(Id, ((SubProcess) flowElement).getFlowElements()); if (flowElement1 != null) { return flowElement1; } } } return null; } /** * 返回流程的开始节点 * * @param flowElements 节点集合 * @description: */ public static FlowElement getStartFlowElement(Collection&lt;FlowElement&gt; flowElements) { for (FlowElement flowElement : flowElements) { if (flowElement instanceof StartEvent) { return flowElement; } } return null; } /** * 校验el表达示例 * * @param map * @param expression * @return */ public static Object result(Map&lt;String, Object&gt; map, String expression) { FelEngine fel = new FelEngineImpl(); FelContext ctx = fel.getContext(); for (Map.Entry&lt;String, Object&gt; entry : map.entrySet()) { ctx.set(entry.getKey(), entry.getValue()); } Object result = fel.eval(expression); return result; } } maven el表达式校验的依赖 &lt;dependency&gt; &lt;groupId&gt;org.eweb4j&lt;/groupId&gt; &lt;artifactId&gt;fel&lt;/artifactId&gt; &lt;version&gt;0.8&lt;/version&gt; &lt;/dependency&gt; ","link":"https://oldcamel.run/post/flowable-nexttask/"},{"title":" Beam+Kafka同步Oracle日志到ClickHouse","content":"用kafka的oracle connect插件存储oracle日志，用beam读取kafka数据插入到clickhouse中 oracle配置 archivelog模式 数据库必须处于archivelog模式，并且必须启用补充日志记录 在数据库服务器上执行 sqlplus / as sysdba SQL&gt;shutdown immediate SQL&gt;startup mount SQL&gt;alter database archivelog; SQL&gt;alter database open; 启用补充日志记录 sqlplus / as sysdba SQL&gt;alter database add supplemental log data (all) columns; kafka配置 插件配置文件 在$KAFKA_HOME/config 目录新建OracleSourceConnector.properties配置文件， 文件内示例配置 name=oracle-logminer-connector connector.class=com.ecer.kafka.connect.oracle.OracleSourceConnector db.name.alias=test tasks.max=1 topic=cdctest db.name=testdb db.hostname=10.1.X.X db.port=1521 db.user=kminer db.user.password=kminerpass db.fetch.size=1 table.whitelist=TEST.*,TEST2.TABLE2 table.blacklist=TEST2.TABLE3 parse.dml.data=true reset.offset=false multitenant=false 配置说明 Name Type Description name String 连接器名称 connector.class String 此连接器的Java类的名称 db.name.alias String 数据库的标识符名称（例如Test，Dev，Prod）或用于标识数据库的特定名称该名称将用作主题和架构名称的标头 tasks.max Integer 创建的最大任务数此连接器使用单个任务. topic String 消息将写入的主题的名称如果设置了值，则所有消息都将写入此声明的topic，如果未设置，则将为每个数据库表动态创建一个主题 db.name String 要连接的数据库的服务名称或sid通常使用数据库服务名称 db.hostname String Oracle数据库服务器的IP地址或主机名 db.port Integer Oracle数据库服务器的端口号 db.user String 数据库的用户名 db.user.password String 数据库用户密码 db.fetch.size Integer 此配置属性设置Oracle行提取大小值 table.whitelist String 白名单格式为用户名.表名用逗号分隔，如果要收集用户下的所有的表，用用户名.* parse.dml.data Boolean 如果为true，则将捕获的sql DML语句解析为字段和值;如果为false，则仅发布sql DML语句 reset.offset Boolean 如果为true，则在连接器启动时将偏移值设置为数据库的当前SCN如果为false，则连接器将从上一个偏移值开始 start.scn Long 如果设置此属性，则将偏移值设置为该指定值，并且logminer将从此SCN启动如果连接器希望从所需的SCN启动，则可以使用此属性 multitenant Boolean 如果为true，则启用多租户支持如果为false，将使用单实例配置 table.blacklist String 黑名单格式为用户名.表名用逗号分隔，如果要收集用户下的所有的表，用用户名.*. dml.types String 以逗号分隔的DML操作列表（INSERT，UPDATE，DELETE）如果未指定，则将执行复制所有DML操作的默认行为，如果指定，则仅捕获指定的操作 以上为机翻，原说明文档地址见 https://github.com/erdemcer/kafka-connect-oracle/blob/master/README.md 添加kafka插件包 kafka-connect-oracle-1.0.68.jar和相应版本的oracle的驱动包放入到 $KAFKA_HOME/lib文件夹中 启动插件 cd $KAFKA_HOME ./bin/connect-standalone.sh ./config/connect-standalone.properties ./config/OracleSourceConnector.properties kafka消息说明 字段 说明 SCN 数据库日志时间 SEG_OWNER 数据库用户名 TABLE_NAME 数据库表名 TIMESTAMP 时间戳 SQL_REDO 执行的sql语句 OPERATION 操作 DATA 更新后的数据 BEFORE 更新前的数据 ClickHouse 建表 CREATE TABLE default.ODB_LOG ( SCN Int64, SEG_OWNER String, TABLE_NAME String, `TIMESTAMP` DateTime, SQL_REDO String, OPERATION String, `DATA` String, `BEFORE` String ) ENGINE = MergeTree() PARTITION BY toYYYYMM(TIMESTAMP) ORDER BY (SEG_OWNER,TABLE_NAME,OPERATION,TIMESTAMP ) SETTINGS index_granularity=8192; Beam 程序 maven添加 依赖包 &lt;dependency&gt; &lt;groupId&gt;org.apache.beam&lt;/groupId&gt; &lt;artifactId&gt;beam-sdks-java-io-clickhouse&lt;/artifactId&gt; &lt;version&gt;${beam.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.beam&lt;/groupId&gt; &lt;artifactId&gt;beam-sdks-java-io-kafka&lt;/artifactId&gt; &lt;version&gt;${beam.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.4.1&lt;/version&gt; &lt;!-- &lt;scope&gt;runtime&lt;/scope&gt;--&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.beam&lt;/groupId&gt; &lt;artifactId&gt;beam-runners-flink-1.11&lt;/artifactId&gt; &lt;version&gt;${beam.version}&lt;/version&gt; &lt;/dependency&gt; 创建配置参数类 package com.yunzainfo.kol.config; import org.apache.beam.runners.flink.FlinkPipelineOptions; import org.apache.beam.sdk.options.Default; import org.apache.beam.sdk.options.Description; import org.apache.beam.sdk.options.PipelineOptions; import org.apache.beam.sdk.options.Validation.Required; /** * Created by IntelliJ IDEA * TODO: TODO * * @author: 徐成 * Date: 2020/12/2 * Time: 10:08 上午 * Email: old_camel@163.com */ public interface KolOptions extends FlinkPipelineOptions { //public interface OdcOptions extends PipelineOptions { @Description(&quot;kafka链接地址&quot;) @Required @Default.String(&quot;xxx.xxx.xxx.xxx:9092&quot;) String getBootstrapServers(); void setBootstrapServers(String value); @Description(&quot;kafka 消息主题&quot;) @Required @Default.String(&quot;kol&quot;) String getTopic(); void setTopic(String value); @Description(&quot;分组id&quot;) @Required @Default.String(&quot;bdf_kol&quot;) String getGroupid(); void setGroupid(String value); @Description(&quot;kafka用户&quot;) @Required @Default.String(&quot;xxx&quot;) String getKafkaUsername(); void setKafkaUsername(String kafkaUsername); @Description(&quot;kafka密码&quot;) @Required @Default.String(&quot;xxx&quot;) String getKafkaPassword(); void setKafkaPassword(String kafkaPassword); @Description(&quot;ClickHouse连接地址&quot;) @Required @Default.String(&quot;xxx.xxx.xxx.xxx:xxxx/default&quot;) String getClickHouseUrl(); void setClickHouseUrl(String value); @Description(&quot;ClickHouse用户名&quot;) @Required @Default.String(&quot;root&quot;) String getClickHouseUserName(); void setClickHouseUserName(String value); @Description(&quot;ClickHouse密码&quot;) @Required @Default.String(&quot;xxx&quot;) String getClickHousePassword(); void setClickHousePassword(String value); @Description(&quot;ClickHouse表&quot;) @Required @Default.String(&quot;ODB_LOG&quot;) String getClickHouseTableName(); void setClickHouseTableName(String value); } kafka消息映射类 package com.yunzainfo.kol.model; import org.codehaus.jackson.annotate.JsonProperty; import java.io.Serializable; import java.util.Date; /** * Created by IntelliJ IDEA * TODO: TODO * * @author: 徐成 * Date: 2020/11/30 * Time: 2:32 下午 * Email: old_camel@163.com */ public class Payload implements Serializable { private static final long serialVersionUID = 1L; @JsonProperty(value = &quot;SCN&quot;) private Long SCN; @JsonProperty(value = &quot;SEG_OWNER&quot;) private String SEG_OWNER; @JsonProperty(value = &quot;TABLE_NAME&quot;) private String TABLE_NAME; @JsonProperty(value = &quot;TIMESTAMP&quot;) private Long TIMESTAMP; @JsonProperty(value = &quot;SQL_REDO&quot;) private String SQL_REDO; @JsonProperty(value = &quot;OPERATION&quot;) private String OPERATION; @JsonProperty(value = &quot;data&quot;) private Object DATA; @JsonProperty(value = &quot;before&quot;) private Object BEFORE; public static long getSerialVersionUID() { return serialVersionUID; } public Long getSCN() { return SCN; } public void setSCN(Long SCN) { this.SCN = SCN; } public String getSEG_OWNER() { return SEG_OWNER; } public void setSEG_OWNER(String SEG_OWNER) { this.SEG_OWNER = SEG_OWNER; } public String getTABLE_NAME() { return TABLE_NAME; } public void setTABLE_NAME(String TABLE_NAME) { this.TABLE_NAME = TABLE_NAME; } public Long getTIMESTAMP() { return TIMESTAMP; } public void setTIMESTAMP(Long TIMESTAMP) { this.TIMESTAMP = TIMESTAMP; } public String getSQL_REDO() { return SQL_REDO; } public void setSQL_REDO(String SQL_REDO) { this.SQL_REDO = SQL_REDO; } public String getOPERATION() { return OPERATION; } public void setOPERATION(String OPERATION) { this.OPERATION = OPERATION; } public Object getDATA() { return DATA; } public void setDATA(Object DATA) { this.DATA = DATA; } public Object getBEFORE() { return BEFORE; } public void setBEFORE(Object BEFORE) { this.BEFORE = BEFORE; } @Override public String toString() { return &quot;Payload{&quot; + &quot;SCN=&quot; + SCN + &quot;, SEG_OWNER='&quot; + SEG_OWNER + '\\'' + &quot;, TABLE_NAME='&quot; + TABLE_NAME + '\\'' + &quot;, TIMESTAMP=&quot; + TIMESTAMP + &quot;, SQL_REDO='&quot; + SQL_REDO + '\\'' + &quot;, OPERATION='&quot; + OPERATION + '\\'' + &quot;, DATA='&quot; + DATA + '\\'' + &quot;, BEFORE='&quot; + BEFORE + '\\'' + '}'; } } 主程序 package com.yunzainfo.kol; import com.yunzainfo.kol.config.KolOptions; import com.yunzainfo.kol.model.Payload; import org.apache.beam.runners.flink.FlinkRunner; import org.apache.beam.sdk.Pipeline; import org.apache.beam.sdk.coders.SerializableCoder; import org.apache.beam.sdk.io.clickhouse.ClickHouseIO; import org.apache.beam.sdk.io.kafka.KafkaIO; import org.apache.beam.sdk.options.PipelineOptionsFactory; import org.apache.beam.sdk.schemas.Schema; import org.apache.beam.sdk.transforms.DoFn; import org.apache.beam.sdk.transforms.ParDo; import org.apache.beam.sdk.transforms.Values; import org.apache.beam.sdk.values.Row; import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap; import org.apache.kafka.clients.CommonClientConfigs; import org.apache.kafka.clients.consumer.ConsumerConfig; import org.apache.kafka.common.config.SaslConfigs; import org.apache.kafka.common.serialization.StringDeserializer; import org.codehaus.jackson.map.ObjectMapper; import org.joda.time.DateTime; import org.joda.time.Duration; import java.io.IOException; import java.util.LinkedHashMap; import java.util.Map; /** * Created by IntelliJ IDEA * TODO: TODO * * @author: 徐成 * Date: 2020/11/27 * Time: 5:49 下午 * Email: old_camel@163.com */ public class KolApp { public static void main(String[] args) { KolOptions options = PipelineOptionsFactory.fromArgs(args).as(KolOptions.class); options.setRunner(FlinkRunner.class); runOdc(options); } public static Object isnull(Object o) { if (o == null) { return &quot;&quot;; } else { return o; } } public static void runOdc(KolOptions options) { Pipeline p = Pipeline.create(options); final Map&lt;String, Object&gt; immutableMap = new ImmutableMap.Builder&lt;String, Object&gt;() .put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;) .put(ConsumerConfig.GROUP_ID_CONFIG, options.getGroupid()) .put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true) .put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, &quot;SASL_PLAINTEXT&quot;) .put(SaslConfigs.SASL_MECHANISM, &quot;PLAIN&quot;) .put(SaslConfigs.SASL_JAAS_CONFIG, &quot;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\&quot;&quot; + options.getKafkaUsername() + &quot;\\&quot; password=\\&quot;&quot; + options.getKafkaPassword() + &quot;\\&quot;;&quot;) .build(); final Schema ckType = Schema.of( Schema.Field.of(&quot;SCN&quot;, Schema.FieldType.INT64.withNullable(true)), Schema.Field.of(&quot;SEG_OWNER&quot;, Schema.FieldType.STRING.withNullable(true)), Schema.Field.of(&quot;TABLE_NAME&quot;, Schema.FieldType.STRING.withNullable(true)), Schema.Field.of(&quot;TIMESTAMP&quot;, Schema.FieldType.DATETIME.withNullable(true)), Schema.Field.of(&quot;SQL_REDO&quot;, Schema.FieldType.STRING.withNullable(true)), Schema.Field.of(&quot;OPERATION&quot;, Schema.FieldType.STRING.withNullable(true)), Schema.Field.of(&quot;DATA&quot;, Schema.FieldType.STRING.withNullable(true)), Schema.Field.of(&quot;BEFORE&quot;, Schema.FieldType.STRING.withNullable(true)) ); p.apply(&quot;读取消息&quot;, KafkaIO.&lt;String, String&gt;read() .withBootstrapServers(options.getBootstrapServers()) .withTopic(options.getTopic()) .withKeyDeserializer(StringDeserializer.class) .withValueDeserializer(StringDeserializer.class) .withOffsetConsumerConfigOverrides(ImmutableMap.&lt;String, Object&gt;of(ConsumerConfig.GROUP_ID_CONFIG, options.getGroupid())) .withConsumerConfigUpdates(immutableMap) .withReadCommitted() .withoutMetadata()) .apply(Values.create()) .apply(&quot;转义Payload&quot;, ParDo.of(new DoFn&lt;String, Payload&gt;() { private static final long serialVersionUID = 1L; @ProcessElement public void processElement(ProcessContext ctx) { String element = ctx.element(); ObjectMapper mapper = new ObjectMapper(); try { LinkedHashMap map = mapper.readValue(element, LinkedHashMap.class); //Object payload = data.get(&quot;payload&quot;); LinkedHashMap payloadMap = (LinkedHashMap) map.get(&quot;payload&quot;); Payload payload = mapper.convertValue(payloadMap, Payload.class); payload.setDATA(mapper.writeValueAsString(payload.getDATA())); payload.setBEFORE(mapper.writeValueAsString(payload.getBEFORE())); ctx.output(payload); } catch (IOException e) { e.printStackTrace(); } } })) .setCoder(SerializableCoder.of(Payload.class)) .apply(&quot;转换Row&quot;, ParDo.of(new DoFn&lt;Payload, Row&gt;() { @ProcessElement public void processElement(ProcessContext cxt) { Payload payload = cxt.element(); Row alarmRow = Row.withSchema(ckType).addValues( isnull(payload.getSCN()), isnull(payload.getSEG_OWNER()), isnull(payload.getTABLE_NAME()), isnull(new DateTime(payload.getTIMESTAMP())), isnull(payload.getSQL_REDO()), isnull(payload.getOPERATION()), isnull(payload.getDATA()), isnull(payload.getBEFORE()) ).build(); cxt.output(alarmRow); } })).setRowSchema(ckType) .apply(&quot;写入ClickHouse数据库&quot;, ClickHouseIO.&lt;Row&gt;write(&quot;jdbc:clickhouse://&quot; + options.getClickHouseUrl() + &quot;?username=&quot; + options.getClickHouseUserName() + &quot;&amp;password=&quot; + options.getClickHousePassword(), options.getClickHouseTableName()) .withMaxRetries(3)//重试次数 .withInsertDeduplicate(true)//重复数据是否删除 .withMaxInsertBlockSize(1)//添加最大块的大小 .withInitialBackoff(Duration.standardSeconds(5))//初始退回时间 .withInsertDistributedSync(false) ); p.run().waitUntilFinish(); } } 把beam包放在flink上运行 clickhouse中的数据 ","link":"https://oldcamel.run/post/apache-beam-tong-bu-oracle-ri-zhi-dao-clickhouse/"}]}